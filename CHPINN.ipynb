{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# Manually making sure the numpy random seeds are \"the same\" on all devices\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "import scipy.io\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyDOE import lhs\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to load dataset and bring to proper form for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(N_u=None, N_f=None):\n",
    "    \n",
    "    # Loading the dataset from .npy files\n",
    "    t = np.load('time.npy')\n",
    "    conc = np.load('Conc.npy')\n",
    "    x = np.arange(1,129)\n",
    "    y = np.arange(1,129)\n",
    "    \n",
    "    # Creating a meshgrid of all possible tuples of x, y, t\n",
    "    xv,yv,tv = np.meshgrid(x,y,t,indexing='ij')\n",
    "        \n",
    "    # Preparing the inputs x , y and t (meshed as xv, yv and tv) for predictions in one single array, as inp_star\n",
    "    inp_star = np.hstack((xv.flatten('F')[:,None], yv.flatten('F')[:,None],tv.flatten('F')[:,None]))\n",
    "\n",
    "    # Preparing the testing conc_star\n",
    "    conc_star = []\n",
    "    for i in np.arange(inp_star.shape[0]):\n",
    "        conc_star.append(conc[inp_star[i,0]-1,inp_star[i,1]-1,inp_star[i,2]])\n",
    "    conc_star = np.asarray(conc_star)\n",
    "    conc_star = conc_star.reshape((conc_star.shape[0],1))\n",
    "    \n",
    "    idx = np.random.choice(inp_star.shape[0], N_u, replace=False)\n",
    "    inp_train = inp_star[idx,:]\n",
    "    conc_train = []\n",
    "    for i in np.arange(inp_train.shape[0]):\n",
    "        conc_train.append(conc[inp_train[i,0]-1,inp_train[i,1]-1,inp_train[i,2]])\n",
    "    conc_train = np.asarray(conc_train)\n",
    "    conc_train = conc_train.reshape((conc_train.shape[0],1))\n",
    "    \n",
    "    # For the case when N_f is not given as input\n",
    "    if N_f == None:\n",
    "        lb = X_star.min(axis=0)\n",
    "        ub = X_star.max(axis=0) \n",
    "        return x, t, X, T, Exact_u, inp_star, conc_star, inp_train, conc_train, ub, lb\n",
    "\n",
    "    # Domain bounds (lowerbounds upperbounds) [x, y, t], which are here ([1, 1, 0] and [128, 128, 5000])\n",
    "    lb = inp_star.min(axis=0)\n",
    "    ub = inp_star.max(axis=0)\n",
    "    # Getting the initial conditions (t=0)\n",
    "    #initial_cond = inp_star[0:128*128,:]\n",
    "    #conc_initial_cond = conc[:,:,0]\n",
    "    #conc_initial_cond = conc_initial_cond.flatten('F')[:,None]\n",
    "    # Getting the all boundary conditions\n",
    "    #top_bound = inp_star[inp_star[:,0]==1]\n",
    "    #bottom_bound = inp_star[inp_star[:,0]==128]\n",
    "    #left_bound = inp_star[inp_star[:,1]==1]\n",
    "    #right_bound = inp_star[inp_star[:,1]==128]\n",
    "    #boundary_ind = np.vstack((top_bound,bottom_bound,left_bound,right_bound))\n",
    "    \n",
    "    #conc_boundary = []\n",
    "    #for i in np.arange(boundary_ind.shape[0]):\n",
    "    #    conc_boundary.append(conc[boundary_ind[i,0]-1,boundary_ind[i,1]-1,boundary_ind[i,2]])\n",
    "    #conc_boundary = np.asarray(conc_boundary)\n",
    "    #conc_boundary = conc_boundary.flatten('F')[:,None]\n",
    "    \n",
    "    # Stacking them in multidimensional tensors for training (inp_train)\n",
    "    #inp_train = np.vstack([initial_cond,boundary_ind])\n",
    "    #conc_train = np.vstack([conc_initial_cond, conc_boundary])\n",
    "\n",
    "    # Generating the x, y and t collocation points for f, with each having a N_f size\n",
    "    # We pointwise add and multiply to spread the LHS over the 3D domain\n",
    "    inp_f_train = lb + (ub-lb)*lhs(3, N_f)\n",
    "\n",
    "    # Generating a uniform random sample from ints between 0, and the size of inp_train, of size N_u (initial data size) and without replacement (unique)\n",
    "    #idx = np.random.choice(inp_train.shape[0], N_u, replace=False)\n",
    "    # Getting the corresponding inp_train (which is now scarce boundary and initial coordinates)\n",
    "    #inp_train = inp_train[idx,:]\n",
    "    # Getting the corresponding conc_train\n",
    "    #conc_train = conc_train[idx,:]\n",
    "\n",
    "    return x, y, t, xv, tv, yv, conc, inp_star, conc_star, inp_train, conc_train, inp_f_train, ub, lb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger class implemented for error and loss analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, frequency=10):\n",
    "        print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "        print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "        print(\"GPU-accerelated: {}\".format(tf.test.is_gpu_available()))\n",
    "\n",
    "        self.start_time = time.time()\n",
    "        self.frequency = frequency\n",
    "\n",
    "    def __get_elapsed(self):\n",
    "        return datetime.fromtimestamp(time.time() - self.start_time).strftime(\"%M:%S\")\n",
    "\n",
    "    def __get_error_u(self):\n",
    "        return self.error_fn()\n",
    "\n",
    "    def set_error_fn(self, error_fn):\n",
    "        self.error_fn = error_fn\n",
    "  \n",
    "    def log_train_start(self, model):\n",
    "        print(\"\\nTraining started\")\n",
    "        print(\"================\")\n",
    "        self.model = model\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def log_train_epoch(self, epoch, loss, custom=\"\", is_iter=False):\n",
    "        if epoch % self.frequency == 0:\n",
    "            print(f\"{'nt_epoch' if is_iter else 'tf_epoch'} = {epoch:6d}  elapsed = {self.__get_elapsed()}  loss = {loss:.4e}  error = {self.__get_error_u():.4e}  \" + custom)\n",
    "\n",
    "    def log_train_opt(self, name):\n",
    "        # print(f\"tf_epoch =      0  elapsed = 00:00  loss = 2.7391e-01  error = 9.0843e-01\")\n",
    "        print(f\"—— Starting {name} optimization ——\")\n",
    "\n",
    "    def log_train_end(self, epoch, custom=\"\"):\n",
    "        print(\"==================\")\n",
    "        print(f\"Training finished (epoch {epoch}): duration = {self.__get_elapsed()}  error = {self.__get_error_u():.4e}  \" + custom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Functional dependencies like L-BFGS etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Time tracking functions\n",
    "global_time_list = []\n",
    "global_last_time = 0\n",
    "def reset_time():\n",
    "    global global_time_list, global_last_time\n",
    "    global_time_list = []\n",
    "    global_last_time = time.perf_counter()\n",
    "    \n",
    "def record_time():\n",
    "    global global_last_time, global_time_list\n",
    "    new_time = time.perf_counter()\n",
    "    global_time_list.append(new_time - global_last_time)\n",
    "    global_last_time = time.perf_counter()\n",
    "    #print(\"step: %.2f\"%(global_time_list[-1]*1000))\n",
    "\n",
    "def last_time():\n",
    "    \"\"\"Returns last interval records in millis.\"\"\"\n",
    "    global global_last_time, global_time_list\n",
    "    if global_time_list:\n",
    "        return 1000 * global_time_list[-1]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def dot(a, b):\n",
    "    \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
    "    return tf.reduce_sum(a*b)\n",
    "\n",
    "def verbose_func(s):\n",
    "    print(s)\n",
    "\n",
    "final_loss = None\n",
    "times = []\n",
    "def lbfgs(opfunc, x, config, state, do_verbose, log_fn):\n",
    "    \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\"\"\"\n",
    "\n",
    "    if config.maxIter == 0:\n",
    "        return\n",
    "\n",
    "    global final_loss, times\n",
    "\n",
    "    maxIter = config.maxIter\n",
    "    maxEval = config.maxEval or maxIter*1.25\n",
    "    tolFun = config.tolFun or 1e-5\n",
    "    tolX = config.tolX or 1e-19\n",
    "    nCorrection = config.nCorrection or 100\n",
    "    lineSearch = config.lineSearch\n",
    "    lineSearchOpts = config.lineSearchOptions\n",
    "    learningRate = config.learningRate or 1\n",
    "    isverbose = config.verbose or False\n",
    "\n",
    "    # verbose function\n",
    "    if isverbose:\n",
    "        verbose = verbose_func\n",
    "    else:\n",
    "        verbose = lambda x: None\n",
    "\n",
    "    # evaluate initial f(x) and df/dx\n",
    "    f, g = opfunc(x)\n",
    "\n",
    "    f_hist = [f]\n",
    "    currentFuncEval = 1\n",
    "    state.funcEval = state.funcEval + 1\n",
    "    p = g.shape[0]\n",
    "\n",
    "    # check optimality of initial point\n",
    "    tmp1 = tf.abs(g)\n",
    "    if tf.reduce_sum(tmp1) <= tolFun:\n",
    "        verbose(\"optimality condition below tolFun\")\n",
    "        return x, f_hist\n",
    "\n",
    "    # optimize for a max of maxIter iterations\n",
    "    nIter = 0\n",
    "    times = []\n",
    "    while nIter < maxIter:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # keep track of nb of iterations\n",
    "        nIter = nIter + 1\n",
    "        state.nIter = state.nIter + 1\n",
    "\n",
    "    ############################################################\n",
    "    ## compute gradient descent direction\n",
    "    ############################################################\n",
    "        if state.nIter == 1:\n",
    "            d = -g\n",
    "            old_dirs = []\n",
    "            old_stps = []\n",
    "            Hdiag = 1\n",
    "        else:\n",
    "            # do lbfgs update (update memory)\n",
    "            y = g - g_old\n",
    "            s = d*t\n",
    "            ys = dot(y, s)\n",
    "      \n",
    "            if ys > 1e-10:\n",
    "                # updating memory\n",
    "                if len(old_dirs) == nCorrection:\n",
    "                    # shift history by one (limited-memory)\n",
    "                    del old_dirs[0]\n",
    "                    del old_stps[0]\n",
    "\n",
    "                # store new direction/step\n",
    "                old_dirs.append(s)\n",
    "                old_stps.append(y)\n",
    "\n",
    "                # update scale of initial Hessian approximation\n",
    "                Hdiag = ys/dot(y, y)\n",
    "\n",
    "            # compute the approximate (L-BFGS) inverse Hessian \n",
    "            # multiplied by the gradient\n",
    "            k = len(old_dirs)\n",
    "\n",
    "            # need to be accessed element-by-element, so don't re-type tensor:\n",
    "            ro = [0]*nCorrection\n",
    "            for i in range(k):\n",
    "                ro[i] = 1/dot(old_stps[i], old_dirs[i])\n",
    "        \n",
    "\n",
    "            # iteration in L-BFGS loop collapsed to use just one buffer\n",
    "            # need to be accessed element-by-element, so don't re-type tensor:\n",
    "            al = [0]*nCorrection\n",
    "\n",
    "            q = -g\n",
    "            for i in range(k-1, -1, -1):\n",
    "                al[i] = dot(old_dirs[i], q) * ro[i]\n",
    "                q = q - al[i]*old_stps[i]\n",
    "\n",
    "            # multiply by initial Hessian\n",
    "            r = q*Hdiag\n",
    "            for i in range(k):\n",
    "                be_i = dot(old_stps[i], r) * ro[i]\n",
    "                r += (al[i]-be_i)*old_dirs[i]\n",
    "        \n",
    "            d = r\n",
    "        # final direction is in r/d (same object)\n",
    "\n",
    "        g_old = g\n",
    "        f_old = f\n",
    "    \n",
    "        ############################################################\n",
    "        ## compute step length\n",
    "        ############################################################\n",
    "        # directional derivative\n",
    "        gtd = dot(g, d)\n",
    "\n",
    "        # check that progress can be made along that direction\n",
    "        if gtd > -tolX:\n",
    "            verbose(\"Can not make progress along direction.\")\n",
    "            break\n",
    "\n",
    "        # reset initial guess for step size\n",
    "        if state.nIter == 1:\n",
    "            tmp1 = tf.abs(g)\n",
    "            t = min(1, 1/tf.reduce_sum(tmp1))\n",
    "        else:\n",
    "            t = learningRate\n",
    "\n",
    "\n",
    "        # optional line search: user function\n",
    "        lsFuncEval = 0\n",
    "        if lineSearch and isinstance(lineSearch) == types.FunctionType:\n",
    "            # perform line search, using user function\n",
    "            f,g,x,t,lsFuncEval = lineSearch(opfunc,x,t,d,f,g,gtd,lineSearchOpts)\n",
    "            f_hist.append(f)\n",
    "        else:\n",
    "            # no line search, simply move with fixed-step\n",
    "            x += t*d\n",
    "      \n",
    "            if nIter != maxIter:\n",
    "                # re-evaluate function only if not in last iteration\n",
    "                # the reason we do this: in a stochastic setting,\n",
    "                # no use to re-evaluate that function here\n",
    "                f, g = opfunc(x)\n",
    "                lsFuncEval = 1\n",
    "                f_hist.append(f)\n",
    "\n",
    "\n",
    "        # update func eval\n",
    "        currentFuncEval = currentFuncEval + lsFuncEval\n",
    "        state.funcEval = state.funcEval + lsFuncEval\n",
    "\n",
    "        ############################################################\n",
    "        ## check conditions\n",
    "        ############################################################\n",
    "        if nIter == maxIter:\n",
    "            break\n",
    "\n",
    "        if currentFuncEval >= maxEval:\n",
    "            # max nb of function evals\n",
    "            verbose('max nb of function evals')\n",
    "            break\n",
    "\n",
    "        tmp1 = tf.abs(g)\n",
    "        if tf.reduce_sum(tmp1) <=tolFun:\n",
    "            # check optimality\n",
    "            verbose('optimality condition below tolFun')\n",
    "            break\n",
    "    \n",
    "        tmp1 = tf.abs(d*t)\n",
    "        if tf.reduce_sum(tmp1) <= tolX:\n",
    "            # step size below tolX\n",
    "            verbose('step size below tolX')\n",
    "            break\n",
    "\n",
    "        if tf.abs(f-f_old) < tolX:\n",
    "            # function value changing less than tolX\n",
    "            verbose('function value changing less than tolX'+str(tf.abs(f-f_old)))\n",
    "            break\n",
    "\n",
    "        if do_verbose:\n",
    "            log_fn(nIter, f.numpy(), True)\n",
    "            #print(\"Step %3d loss %6.5f msec %6.3f\"%(nIter, f.numpy(), last_time()))\n",
    "            record_time()\n",
    "            times.append(last_time())\n",
    "\n",
    "        if nIter == maxIter - 1:\n",
    "            final_loss = f.numpy()\n",
    "\n",
    "    # save state\n",
    "    state.old_dirs = old_dirs\n",
    "    state.old_stps = old_stps\n",
    "    state.Hdiag = Hdiag\n",
    "    state.g_old = g_old\n",
    "    state.f_old = f_old\n",
    "    state.t = t\n",
    "    state.d = d\n",
    "\n",
    "    return x, f_hist, currentFuncEval\n",
    "\n",
    "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
    "class dummy(object):\n",
    "    pass\n",
    "\n",
    "class Struct(dummy):\n",
    "    def __getattribute__(self, key):\n",
    "        if key == '__dict__':\n",
    "            return super(dummy, self).__getattribute__('__dict__')\n",
    "        return self.__dict__.get(key, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN Architecture defined for 2-D Cahn-Hilliard equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation\n",
    "\n",
    "### 1. Order parameter\n",
    "\n",
    "An order parameter  c  is defined as the concentration of B atom. The unit of  c  is defined as atomic fraction in this code.\n",
    "\n",
    "### 2. Total free energy\n",
    "\n",
    "The total Gibbs free energy of the system is defined by\n",
    "\n",
    "$ G = \\int_{V} \\left( g_{chem}(u) + g_{grad}(\\nabla u) \\right) dV $\n",
    " \n",
    "where  $g_{chem}$  and  $g_{grad}$ are the chemical free energy and the gradient energy densities, respectively. The chemical free energy density is formulated based on the regular solution approximation as:\n",
    "\n",
    "$ g_{chem} = RT\\left[ u\\ln u + (1-u)\\ln(1-u)\\right] + Lu(1-u) $\n",
    " \n",
    "where  $L$  is the atomic interaction parameter. The gradient energy density is expressed by:\n",
    "\n",
    "$ g_{grad} = \\frac{a_{c}}{2} \\left| \\nabla u \\right|^{2} $\n",
    " \n",
    "where  $a_{c}$  is the gradient energy coefficient. In this model,  $a_{c}$  is not related to any physical values.\n",
    "\n",
    "### 3. Time evolution equation\n",
    "\n",
    "The time evolution of the order parameter  $u$  is given by assuming that the total free energy of the system  $G$  decreases monotonically with time. For the conserved order parameter, the time evolution equation is derived from the Cahn-Hilliard equation given as:\n",
    "\n",
    "$ \\frac{\\partial u}{\\partial t} = \\nabla \\cdot \\left( M_{c} \\nabla \\frac{\\delta G}{\\delta u} \\right) =  \\nabla \\cdot \\left( M_{c} \\nabla \\mu \\right) $\n",
    " \n",
    "where  $\\mu$  is the diffusion potential of B atom. According to the total Gibbs free energy,  $\\mu$  is expressed by:\n",
    "\n",
    "$ \\mu = \\frac{\\delta G}{\\delta u} = RT\\left[ \\ln u - \\ln (1-u) \\right] + L(1-2u) - a_{c} \\nabla^{2}u $\n",
    " \n",
    "Here, remind that the functional derivative of $G$  is given by the Euler-Lagrange equation:\n",
    "\n",
    "$ \\frac{\\delta G}{\\delta u}=\\frac{\\partial g}{\\partial u}-\\nabla\\cdot\\frac{\\partial g}{\\partial (\\nabla u)} $\n",
    " \n",
    "$M$  is the diffusion mobility of B atom which is assumed to be given by:\n",
    "\n",
    "$ M_{c} = \\left[ \\frac{D_{A}}{RT}u + \\frac{D_{B}}{RT}(1-u)\\right]u(1-u) = \\frac{D_{A}}{RT}\\left[ u + \\frac{D_{B}}{D_{A}}(1-u)\\right]u(1-u) $\n",
    " \n",
    "Here,  $D_{A}$  and  $D_{B}$  are the diffusion coefficients of A and B atoms, respectively.\n",
    "\n",
    "Since the diffusion mobility  $M_{c}$  depends on  $u$ , in two-dimensional space, the time evolution equation can be written as:\n",
    "\n",
    "$ \\frac{\\partial u}{\\partial t} =  \\nabla \\cdot \\left( M_{c} \\nabla \\mu \\right) = M_{c} \\left(\\frac{\\partial^{2}\\mu}{\\partial x^{2}} +  \\frac{\\partial^{2}\\mu}{\\partial y^{2}}\\right) + \\frac{\\partial M_{c}}{\\partial u} \\left( \\frac{\\partial u}{\\partial x}\\frac{\\partial \\mu}{\\partial x} +  \\frac{\\partial u}{\\partial y}\\frac{\\partial \\mu}{\\partial y} \\right) $\n",
    " \n",
    "where the derivative of  $M_{c}$   with respect to  $ u $  is given by:\n",
    "\n",
    "$ \\frac{\\partial M_{c}}{\\partial u} = \\frac{D_{A}}{RT}\\left[ \\left(1-\\frac{D_{B}}{D_{A}}\\right)u(1-u) + \\left(u+ \\frac{D_{B}}{D_{A}}(1-u)\\right)(1-2u) \\right] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN(object):\n",
    "    def __init__(self, layers, optimizer, logger, inp_f, ub, lb, R, temp, L_a, a_c, D_a, D_b):\n",
    "        # Descriptive Keras model [3, 20, …, 20, 1]\n",
    "        self.u_model = tf.keras.Sequential()\n",
    "        self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "        self.u_model.add(tf.keras.layers.Lambda(lambda X: 2*(X - lb)/(ub - lb) - 1.0))\n",
    "        for width in layers[1:]:\n",
    "            if width != 1:\n",
    "                self.u_model.add(tf.keras.layers.Dense(width, activation=tf.nn.tanh,kernel_initializer='glorot_normal'))\n",
    "            else:\n",
    "                self.u_model.add(tf.keras.layers.Dense(width, activation=tf.nn.sigmoid,kernel_initializer='glorot_normal'))\n",
    "\n",
    "        # Computing the sizes of weights/biases for future decomposition\n",
    "        self.sizes_w = []\n",
    "        self.sizes_b = []\n",
    "        for i, width in enumerate(layers):\n",
    "            if i != 1:\n",
    "                self.sizes_w.append(int(width * layers[1]))\n",
    "                self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "\n",
    "        self.R = R\n",
    "        self.temp = temp\n",
    "        self.D_a = D_a\n",
    "        self.D_b = D_b\n",
    "        self.a_c = a_c\n",
    "        self.L_a = L_a\n",
    "        self.optimizer = optimizer\n",
    "        self.logger = logger\n",
    "\n",
    "        self.dtype = tf.float32\n",
    "\n",
    "        # Separating the collocation coordinates\n",
    "        self.x_f = tf.convert_to_tensor(inp_f[:, 0:1], dtype=self.dtype)\n",
    "        self.y_f = tf.convert_to_tensor(inp_f[:, 1:2], dtype=self.dtype)\n",
    "        self.t_f = tf.convert_to_tensor(inp_f[:, 2:3], dtype=self.dtype)\n",
    "    \n",
    "        # Defining custom loss\n",
    "    def __loss(self, u, u_pred):\n",
    "        f_pred = self.f_model()\n",
    "        return tf.reduce_mean(tf.square(u - u_pred)) + \\\n",
    "            tf.reduce_mean(tf.square(f_pred))\n",
    "\n",
    "    def __grad(self, X, u):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = self.__loss(u, self.u_model(X))\n",
    "        return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "\n",
    "    def __wrap_training_variables(self):\n",
    "        var = self.u_model.trainable_variables\n",
    "        return var\n",
    "\n",
    "    # The actual PINN\n",
    "    def f_model(self):\n",
    "        R = self.R\n",
    "        T = self.temp\n",
    "        D_a = self.D_a\n",
    "        D_b = self.D_b\n",
    "        a_c = self.a_c\n",
    "        L = self.L_a\n",
    "        # Using the new GradientTape paradigm of TF2.0,\n",
    "        # which keeps track of operations to get the gradient at runtime\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Watching the three inputs we’ll need later, x, y and t\n",
    "            tape.watch(self.x_f)\n",
    "            tape.watch(self.y_f)\n",
    "            tape.watch(self.t_f)\n",
    "            tape.watch(self.t_f)\n",
    "            # Packing together the inputs\n",
    "            X_f = tf.stack([self.x_f[:,0], self.y_f[:,0],self.t_f[:,0]], axis=1)\n",
    "            # Getting the prediction\n",
    "            u = self.u_model(X_f)\n",
    "            np.where(u!=0,u,0.5)\n",
    "            # Deriving INSIDE the tape (since we’ll need the x derivative of this later, u_xx)\n",
    "            u_x = tape.gradient(u, self.x_f)\n",
    "            u_y = tape.gradient(u, self.y_f)\n",
    "            u_xx = tape.gradient(u_x, self.x_f)\n",
    "            u_yy = tape.gradient(u_y, self.y_f)\n",
    "            mu = (R*T)*(np.log(u/(1-u))) + (L*(1-2*u)) - (a_c*(u_xx+u_yy))\n",
    "            mu_x = tape.gradient(mu, self.x_f)\n",
    "            mu_y = tape.gradient(mu, self.y_f)\n",
    "            M_c = (D_a/R/T)*(u + (D_b/D_a)*(1-u))*(u*(1-u))\n",
    "            \n",
    "            #tape.watch(mu)\n",
    "        # Getting the other derivatives\n",
    "        Mc_u = tape.gradient(M_c, u)\n",
    "        #Mc_u = (D_a/R/T)*((1-(D_b/D_a))*u*(1-u) + (u + (D_b/D_a)*(1-u))(1-2*u))\n",
    "        u_t = tape.gradient(u, self.t_f)\n",
    "        mu_xx = tape.gradient(mu_x, self.x_f)\n",
    "        mu_yy = tape.gradient(mu_y, self.y_f)\n",
    "\n",
    "        # Letting the tape go\n",
    "        del tape\n",
    "        \n",
    "        f_err = u_t - M_c*(mu_xx+mu_yy) + Mc_u*((u_x*mu_x)+(u_y*mu_y))\n",
    "        \n",
    "        # Buidling the PINNs\n",
    "        return f_err\n",
    "\n",
    "    def get_weights(self):\n",
    "        w = []\n",
    "        for layer in self.u_model.layers[1:]:\n",
    "            weights_biases = layer.get_weights()\n",
    "            weights = weights_biases[0].flatten()\n",
    "            biases = weights_biases[1]\n",
    "            w.extend(weights)\n",
    "            w.extend(biases)\n",
    "        return tf.convert_to_tensor(w, dtype=self.dtype)\n",
    "\n",
    "    def set_weights(self, w):\n",
    "        for i, layer in enumerate(self.u_model.layers[1:]):\n",
    "            start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
    "            end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
    "            weights = w[start_weights:end_weights]\n",
    "            w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
    "            weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
    "            biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
    "            weights_biases = [weights, biases]\n",
    "            layer.set_weights(weights_biases)\n",
    "\n",
    "    def summary(self):\n",
    "        return self.u_model.summary()\n",
    "\n",
    "    # The training function\n",
    "    def fit(self, X_u, u, tf_epochs=5000, nt_config=Struct()):\n",
    "        self.logger.log_train_start(self)\n",
    "\n",
    "        # Creating the tensors\n",
    "        X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
    "        u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
    "\n",
    "        self.logger.log_train_opt(\"Adam\")\n",
    "        for epoch in range(tf_epochs):\n",
    "            # Optimization step\n",
    "            loss_value, grads = self.__grad(X_u, u)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.__wrap_training_variables()))\n",
    "            self.logger.log_train_epoch(epoch, loss_value)\n",
    "    \n",
    "        self.logger.log_train_opt(\"LBFGS\")\n",
    "        def loss_and_flat_grad(w):\n",
    "            with tf.GradientTape() as tape:\n",
    "                self.set_weights(w)\n",
    "                loss_value = self.__loss(u, self.u_model(X_u))\n",
    "            grad = tape.gradient(loss_value, self.u_model.trainable_variables)\n",
    "            grad_flat = []\n",
    "            for g in grad:\n",
    "                grad_flat.append(tf.reshape(g, [-1]))\n",
    "            grad_flat =  tf.concat(grad_flat, 0)\n",
    "            return loss_value, grad_flat\n",
    "        # tfp.optimizer.lbfgs_minimize(\n",
    "        #   loss_and_flat_grad,\n",
    "        #   initial_position=self.get_weights(),\n",
    "        #   num_correction_pairs=nt_config.nCorrection,\n",
    "        #   max_iterations=nt_config.maxIter,\n",
    "        #   f_relative_tolerance=nt_config.tolFun,\n",
    "        #   tolerance=nt_config.tolFun,\n",
    "        #   parallel_iterations=6)\n",
    "        lbfgs(loss_and_flat_grad,self.get_weights(),nt_config, Struct(), True, lambda epoch, loss, is_iter:self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
    "\n",
    "        self.logger.log_train_end(tf_epochs + nt_config.maxIter)\n",
    "\n",
    "    def predict(self, X_star):\n",
    "        u_star = self.u_model(X_star)\n",
    "        f_star = self.f_model()\n",
    "        return u_star, f_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intializing hyperparameters and Cahn-Hilliard parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data size on the solution u\n",
    "N_u = 25000\n",
    "# Collocation points size, where we’ll check for f = 0\n",
    "N_f = 10000\n",
    "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
    "layers = [3, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "tf_epochs = 1000\n",
    "tf_optimizer = tf.keras.optimizers.Adam(\n",
    "  learning_rate=0.01,\n",
    "  beta_1=0.99,\n",
    "  epsilon=1e-3)\n",
    "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "nt_epochs = 2000\n",
    "nt_config = Struct()\n",
    "nt_config.learningRate = 0.8\n",
    "nt_config.maxIter = nt_epochs\n",
    "nt_config.nCorrection = 50\n",
    "nt_config.tolFun = 1.0 * np.finfo(float).eps\n",
    "\n",
    "R = 8.314 # gas constant\n",
    "temp = 673 # temperature [K]\n",
    "L_a = 20000.-9.*temp # Atom intaraction constant [J/mol]\n",
    "a_c = 3.0e-14 # gradient coefficient [Jm2/mol]\n",
    "D_a = 1.0e-04*np.exp(-300000.0/R/temp) # diffusion coefficient of A atom [m2/s]\n",
    "D_b = 2.0e-05*np.exp(-300000.0/R/temp) # diffusion coefficient of B atom [m2/s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running prep_data() to formulate dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, t, xv, tv, yv, conc, inp_star, conc_star, inp_train, conc_train, inp_f_train, ub, lb = prep_data(N_u, N_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conc_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.0.0\n",
      "Eager execution: True\n",
      "GPU-accerelated: True\n",
      "\n",
      "Training started\n",
      "================\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 3,041\n",
      "Trainable params: 3,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "—— Starting Adam optimization ——\n",
      "WARNING:tensorflow:Layer dense_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "tf_epoch =      0  elapsed = 00:01  loss = 1.8473e-02  error = 1.4466e-01  \n",
      "tf_epoch =     10  elapsed = 00:11  loss = 1.5382e-02  error = 7.2288e-02  \n",
      "tf_epoch =     20  elapsed = 00:21  loss = 1.5539e-02  error = 8.4236e-02  \n",
      "tf_epoch =     30  elapsed = 00:32  loss = 1.5654e-02  error = 8.4672e-02  \n",
      "tf_epoch =     40  elapsed = 00:42  loss = 1.5214e-02  error = 6.9859e-02  \n",
      "tf_epoch =     50  elapsed = 00:52  loss = 1.5208e-02  error = 6.8839e-02  \n",
      "tf_epoch =     60  elapsed = 01:02  loss = 1.5299e-02  error = 7.2961e-02  \n",
      "tf_epoch =     70  elapsed = 01:12  loss = 1.5328e-02  error = 7.5424e-02  \n",
      "tf_epoch =     80  elapsed = 01:22  loss = 1.5346e-02  error = 7.3849e-02  \n",
      "tf_epoch =     90  elapsed = 01:32  loss = 1.5229e-02  error = 6.9318e-02  \n",
      "tf_epoch =    100  elapsed = 01:42  loss = 1.5152e-02  error = 6.7268e-02  \n",
      "tf_epoch =    110  elapsed = 01:52  loss = 1.5176e-02  error = 6.8598e-02  \n",
      "tf_epoch =    120  elapsed = 02:02  loss = 1.5175e-02  error = 6.8662e-02  \n",
      "tf_epoch =    130  elapsed = 02:12  loss = 1.5159e-02  error = 6.8164e-02  \n",
      "tf_epoch =    140  elapsed = 02:22  loss = 1.5180e-02  error = 6.8766e-02  \n",
      "tf_epoch =    150  elapsed = 02:31  loss = 1.5194e-02  error = 6.9037e-02  \n",
      "tf_epoch =    160  elapsed = 02:42  loss = 1.5171e-02  error = 6.8363e-02  \n",
      "tf_epoch =    170  elapsed = 02:52  loss = 1.5145e-02  error = 6.7742e-02  \n",
      "tf_epoch =    180  elapsed = 03:02  loss = 1.5142e-02  error = 6.7735e-02  \n",
      "tf_epoch =    190  elapsed = 03:12  loss = 1.5145e-02  error = 6.7617e-02  \n",
      "tf_epoch =    200  elapsed = 03:23  loss = 1.5141e-02  error = 6.7160e-02  \n",
      "tf_epoch =    210  elapsed = 03:34  loss = 1.5138e-02  error = 6.7083e-02  \n",
      "tf_epoch =    220  elapsed = 03:44  loss = 1.5146e-02  error = 6.7463e-02  \n",
      "tf_epoch =    230  elapsed = 03:55  loss = 1.5157e-02  error = 6.7835e-02  \n",
      "tf_epoch =    240  elapsed = 04:06  loss = 1.5157e-02  error = 6.7803e-02  \n",
      "tf_epoch =    250  elapsed = 04:16  loss = 1.5149e-02  error = 6.7413e-02  \n",
      "tf_epoch =    260  elapsed = 04:26  loss = 1.5143e-02  error = 6.7205e-02  \n",
      "tf_epoch =    270  elapsed = 04:37  loss = 1.5142e-02  error = 6.7257e-02  \n",
      "tf_epoch =    280  elapsed = 04:47  loss = 1.5141e-02  error = 6.7237e-02  \n",
      "tf_epoch =    290  elapsed = 04:57  loss = 1.5136e-02  error = 6.7116e-02  \n",
      "tf_epoch =    300  elapsed = 05:07  loss = 1.5131e-02  error = 6.7020e-02  \n",
      "tf_epoch =    310  elapsed = 05:17  loss = 1.5131e-02  error = 6.7041e-02  \n",
      "tf_epoch =    320  elapsed = 05:27  loss = 1.5134e-02  error = 6.7149e-02  \n",
      "tf_epoch =    330  elapsed = 05:38  loss = 1.5136e-02  error = 6.7224e-02  \n",
      "tf_epoch =    340  elapsed = 05:48  loss = 1.5136e-02  error = 6.7283e-02  \n",
      "tf_epoch =    350  elapsed = 05:58  loss = 1.5135e-02  error = 6.7345e-02  \n",
      "tf_epoch =    360  elapsed = 06:09  loss = 1.5135e-02  error = 6.7406e-02  \n",
      "tf_epoch =    370  elapsed = 06:19  loss = 1.5136e-02  error = 6.7440e-02  \n",
      "tf_epoch =    380  elapsed = 06:29  loss = 1.5136e-02  error = 6.7353e-02  \n",
      "tf_epoch =    390  elapsed = 06:40  loss = 1.5134e-02  error = 6.7196e-02  \n",
      "tf_epoch =    400  elapsed = 06:50  loss = 1.5132e-02  error = 6.7109e-02  \n",
      "tf_epoch =    410  elapsed = 07:00  loss = 1.5132e-02  error = 6.7099e-02  \n",
      "tf_epoch =    420  elapsed = 07:10  loss = 1.5132e-02  error = 6.7110e-02  \n",
      "tf_epoch =    430  elapsed = 07:20  loss = 1.5132e-02  error = 6.7107e-02  \n",
      "tf_epoch =    440  elapsed = 07:30  loss = 1.5131e-02  error = 6.7094e-02  \n",
      "tf_epoch =    450  elapsed = 07:40  loss = 1.5130e-02  error = 6.7079e-02  \n",
      "tf_epoch =    460  elapsed = 07:51  loss = 1.5131e-02  error = 6.7055e-02  \n",
      "tf_epoch =    470  elapsed = 08:02  loss = 1.5132e-02  error = 6.7038e-02  \n",
      "tf_epoch =    480  elapsed = 08:12  loss = 1.5132e-02  error = 6.7033e-02  \n",
      "tf_epoch =    490  elapsed = 08:23  loss = 1.5131e-02  error = 6.7028e-02  \n",
      "tf_epoch =    500  elapsed = 08:33  loss = 1.5131e-02  error = 6.7033e-02  \n",
      "tf_epoch =    510  elapsed = 08:43  loss = 1.5131e-02  error = 6.7055e-02  \n",
      "tf_epoch =    520  elapsed = 08:54  loss = 1.5131e-02  error = 6.7071e-02  \n",
      "tf_epoch =    530  elapsed = 09:04  loss = 1.5131e-02  error = 6.7059e-02  \n",
      "tf_epoch =    540  elapsed = 09:14  loss = 1.5131e-02  error = 6.7037e-02  \n",
      "tf_epoch =    550  elapsed = 09:24  loss = 1.5130e-02  error = 6.7030e-02  \n",
      "tf_epoch =    560  elapsed = 09:34  loss = 1.5130e-02  error = 6.7036e-02  \n",
      "tf_epoch =    570  elapsed = 09:45  loss = 1.5130e-02  error = 6.7048e-02  \n",
      "tf_epoch =    580  elapsed = 09:55  loss = 1.5130e-02  error = 6.7067e-02  \n",
      "tf_epoch =    590  elapsed = 10:05  loss = 1.5130e-02  error = 6.7084e-02  \n",
      "tf_epoch =    600  elapsed = 10:14  loss = 1.5130e-02  error = 6.7084e-02  \n",
      "tf_epoch =    610  elapsed = 10:24  loss = 1.5130e-02  error = 6.7078e-02  \n",
      "tf_epoch =    620  elapsed = 10:34  loss = 1.5130e-02  error = 6.7073e-02  \n",
      "tf_epoch =    630  elapsed = 10:44  loss = 1.5130e-02  error = 6.7066e-02  \n",
      "tf_epoch =    640  elapsed = 10:54  loss = 1.5130e-02  error = 6.7067e-02  \n",
      "tf_epoch =    650  elapsed = 11:04  loss = 1.5130e-02  error = 6.7086e-02  \n",
      "tf_epoch =    660  elapsed = 11:14  loss = 1.5130e-02  error = 6.7109e-02  \n",
      "tf_epoch =    670  elapsed = 11:24  loss = 1.5130e-02  error = 6.7111e-02  \n",
      "tf_epoch =    680  elapsed = 11:34  loss = 1.5129e-02  error = 6.7101e-02  \n",
      "tf_epoch =    690  elapsed = 11:44  loss = 1.5129e-02  error = 6.7084e-02  \n",
      "tf_epoch =    700  elapsed = 11:55  loss = 1.5129e-02  error = 6.7064e-02  \n",
      "tf_epoch =    710  elapsed = 12:05  loss = 1.5129e-02  error = 6.7053e-02  \n",
      "tf_epoch =    720  elapsed = 12:15  loss = 1.5129e-02  error = 6.7053e-02  \n",
      "tf_epoch =    730  elapsed = 12:25  loss = 1.5129e-02  error = 6.7059e-02  \n",
      "tf_epoch =    740  elapsed = 12:35  loss = 1.5129e-02  error = 6.7067e-02  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_epoch =    750  elapsed = 12:45  loss = 1.5129e-02  error = 6.7073e-02  \n",
      "tf_epoch =    760  elapsed = 12:55  loss = 1.5129e-02  error = 6.7067e-02  \n",
      "tf_epoch =    770  elapsed = 13:05  loss = 1.5129e-02  error = 6.7051e-02  \n",
      "tf_epoch =    780  elapsed = 13:15  loss = 1.5129e-02  error = 6.7041e-02  \n",
      "tf_epoch =    790  elapsed = 13:25  loss = 1.5129e-02  error = 6.7039e-02  \n",
      "tf_epoch =    800  elapsed = 13:35  loss = 1.5128e-02  error = 6.7041e-02  \n",
      "tf_epoch =    810  elapsed = 13:45  loss = 1.5128e-02  error = 6.7047e-02  \n",
      "tf_epoch =    820  elapsed = 13:55  loss = 1.5128e-02  error = 6.7055e-02  \n",
      "tf_epoch =    830  elapsed = 14:05  loss = 1.5128e-02  error = 6.7057e-02  \n",
      "tf_epoch =    840  elapsed = 14:15  loss = 1.5128e-02  error = 6.7054e-02  \n",
      "tf_epoch =    850  elapsed = 14:25  loss = 1.5128e-02  error = 6.7050e-02  \n",
      "tf_epoch =    860  elapsed = 14:35  loss = 1.5128e-02  error = 6.7046e-02  \n",
      "tf_epoch =    870  elapsed = 14:46  loss = 1.5128e-02  error = 6.7046e-02  \n",
      "tf_epoch =    880  elapsed = 14:55  loss = 1.5128e-02  error = 6.7051e-02  \n",
      "tf_epoch =    890  elapsed = 15:06  loss = 1.5128e-02  error = 6.7056e-02  \n",
      "tf_epoch =    900  elapsed = 15:16  loss = 1.5128e-02  error = 6.7058e-02  \n",
      "tf_epoch =    910  elapsed = 15:27  loss = 1.5128e-02  error = 6.7060e-02  \n",
      "tf_epoch =    920  elapsed = 15:37  loss = 1.5127e-02  error = 6.7059e-02  \n",
      "tf_epoch =    930  elapsed = 15:47  loss = 1.5127e-02  error = 6.7056e-02  \n",
      "tf_epoch =    940  elapsed = 15:57  loss = 1.5127e-02  error = 6.7055e-02  \n",
      "tf_epoch =    950  elapsed = 16:07  loss = 1.5127e-02  error = 6.7055e-02  \n",
      "tf_epoch =    960  elapsed = 16:18  loss = 1.5127e-02  error = 6.7054e-02  \n",
      "tf_epoch =    970  elapsed = 16:28  loss = 1.5127e-02  error = 6.7054e-02  \n",
      "tf_epoch =    980  elapsed = 16:38  loss = 1.5127e-02  error = 6.7053e-02  \n",
      "tf_epoch =    990  elapsed = 16:48  loss = 1.5127e-02  error = 6.7051e-02  \n",
      "—— Starting LBFGS optimization ——\n",
      "nt_epoch =     10  elapsed = 17:08  loss = 1.5151e-02  error = 6.7871e-02  \n",
      "nt_epoch =     20  elapsed = 17:19  loss = 1.5122e-02  error = 6.7021e-02  \n",
      "nt_epoch =     30  elapsed = 17:29  loss = 1.5121e-02  error = 6.7040e-02  \n",
      "nt_epoch =     40  elapsed = 17:39  loss = 1.5119e-02  error = 6.7043e-02  \n",
      "nt_epoch =     50  elapsed = 17:49  loss = 1.5118e-02  error = 6.7004e-02  \n",
      "nt_epoch =     60  elapsed = 17:59  loss = 1.5118e-02  error = 6.7007e-02  \n",
      "nt_epoch =     70  elapsed = 18:10  loss = 1.5117e-02  error = 6.7041e-02  \n",
      "nt_epoch =     80  elapsed = 18:20  loss = 1.5117e-02  error = 6.6991e-02  \n",
      "nt_epoch =     90  elapsed = 18:30  loss = 1.5116e-02  error = 6.7049e-02  \n",
      "nt_epoch =    100  elapsed = 18:41  loss = 1.5115e-02  error = 6.7269e-02  \n",
      "nt_epoch =    110  elapsed = 18:51  loss = 1.5112e-02  error = 6.7224e-02  \n",
      "nt_epoch =    120  elapsed = 19:02  loss = 1.5111e-02  error = 6.7161e-02  \n",
      "nt_epoch =    130  elapsed = 19:13  loss = 1.5109e-02  error = 6.7230e-02  \n",
      "nt_epoch =    140  elapsed = 19:24  loss = 1.5109e-02  error = 6.7135e-02  \n",
      "nt_epoch =    150  elapsed = 19:34  loss = 1.5108e-02  error = 6.7071e-02  \n",
      "nt_epoch =    160  elapsed = 19:45  loss = 1.5106e-02  error = 6.7083e-02  \n",
      "nt_epoch =    170  elapsed = 19:56  loss = 1.5104e-02  error = 6.7077e-02  \n",
      "nt_epoch =    180  elapsed = 20:07  loss = 1.5102e-02  error = 6.7140e-02  \n",
      "nt_epoch =    190  elapsed = 20:17  loss = 1.5099e-02  error = 6.7174e-02  \n",
      "nt_epoch =    200  elapsed = 20:28  loss = 1.5097e-02  error = 6.7148e-02  \n",
      "nt_epoch =    210  elapsed = 20:39  loss = 1.5093e-02  error = 6.7088e-02  \n",
      "nt_epoch =    220  elapsed = 20:49  loss = 1.5091e-02  error = 6.7038e-02  \n",
      "nt_epoch =    230  elapsed = 21:00  loss = 1.5088e-02  error = 6.7132e-02  \n",
      "nt_epoch =    240  elapsed = 21:11  loss = 1.5085e-02  error = 6.7214e-02  \n",
      "nt_epoch =    250  elapsed = 21:21  loss = 1.5081e-02  error = 6.7061e-02  \n",
      "nt_epoch =    260  elapsed = 21:32  loss = 1.5079e-02  error = 6.7019e-02  \n",
      "nt_epoch =    270  elapsed = 21:42  loss = 1.5075e-02  error = 6.6974e-02  \n",
      "nt_epoch =    280  elapsed = 21:53  loss = 1.5071e-02  error = 6.7030e-02  \n",
      "nt_epoch =    290  elapsed = 22:03  loss = 1.5069e-02  error = 6.7080e-02  \n",
      "nt_epoch =    300  elapsed = 22:13  loss = 1.5065e-02  error = 6.7064e-02  \n",
      "nt_epoch =    310  elapsed = 22:24  loss = 1.5062e-02  error = 6.7067e-02  \n",
      "nt_epoch =    320  elapsed = 22:34  loss = 1.5057e-02  error = 6.7067e-02  \n",
      "nt_epoch =    330  elapsed = 22:45  loss = 1.5051e-02  error = 6.7208e-02  \n",
      "nt_epoch =    340  elapsed = 22:55  loss = 1.5045e-02  error = 6.7218e-02  \n",
      "nt_epoch =    350  elapsed = 23:05  loss = 1.5037e-02  error = 6.7172e-02  \n",
      "nt_epoch =    360  elapsed = 23:16  loss = 1.5029e-02  error = 6.7083e-02  \n",
      "nt_epoch =    370  elapsed = 23:27  loss = 1.5022e-02  error = 6.7110e-02  \n",
      "nt_epoch =    380  elapsed = 23:37  loss = 1.5013e-02  error = 6.7087e-02  \n",
      "nt_epoch =    390  elapsed = 23:47  loss = 1.5002e-02  error = 6.7154e-02  \n",
      "nt_epoch =    400  elapsed = 23:57  loss = 1.4991e-02  error = 6.7091e-02  \n",
      "nt_epoch =    410  elapsed = 24:08  loss = 1.4982e-02  error = 6.7083e-02  \n",
      "nt_epoch =    420  elapsed = 24:18  loss = 1.4971e-02  error = 6.7107e-02  \n",
      "nt_epoch =    430  elapsed = 24:29  loss = 1.4958e-02  error = 6.7068e-02  \n",
      "nt_epoch =    440  elapsed = 24:40  loss = 1.4947e-02  error = 6.7117e-02  \n",
      "nt_epoch =    450  elapsed = 24:50  loss = 1.4931e-02  error = 6.7118e-02  \n",
      "nt_epoch =    460  elapsed = 25:01  loss = 1.4915e-02  error = 6.7027e-02  \n",
      "nt_epoch =    470  elapsed = 25:12  loss = 1.4899e-02  error = 6.7034e-02  \n",
      "nt_epoch =    480  elapsed = 25:22  loss = 1.4882e-02  error = 6.7004e-02  \n",
      "nt_epoch =    490  elapsed = 25:33  loss = 1.4862e-02  error = 6.7034e-02  \n",
      "nt_epoch =    500  elapsed = 25:43  loss = 1.4834e-02  error = 6.7069e-02  \n",
      "nt_epoch =    510  elapsed = 25:54  loss = 1.4816e-02  error = 6.7260e-02  \n",
      "nt_epoch =    520  elapsed = 26:04  loss = 1.4795e-02  error = 6.7144e-02  \n",
      "nt_epoch =    530  elapsed = 26:15  loss = 1.4775e-02  error = 6.7110e-02  \n",
      "nt_epoch =    540  elapsed = 26:26  loss = 1.4758e-02  error = 6.7233e-02  \n",
      "nt_epoch =    550  elapsed = 26:36  loss = 1.4734e-02  error = 6.7202e-02  \n",
      "nt_epoch =    560  elapsed = 26:47  loss = 1.4700e-02  error = 6.7256e-02  \n",
      "nt_epoch =    570  elapsed = 26:58  loss = 1.4672e-02  error = 6.7112e-02  \n",
      "nt_epoch =    580  elapsed = 27:09  loss = 1.4640e-02  error = 6.7229e-02  \n",
      "nt_epoch =    590  elapsed = 27:20  loss = 1.4608e-02  error = 6.7028e-02  \n",
      "nt_epoch =    600  elapsed = 27:31  loss = 1.4576e-02  error = 6.7087e-02  \n",
      "nt_epoch =    610  elapsed = 27:41  loss = 1.4538e-02  error = 6.7097e-02  \n",
      "nt_epoch =    620  elapsed = 27:52  loss = 1.4509e-02  error = 6.7042e-02  \n",
      "nt_epoch =    630  elapsed = 28:03  loss = 1.4472e-02  error = 6.7107e-02  \n",
      "nt_epoch =    640  elapsed = 28:14  loss = 1.4429e-02  error = 6.7135e-02  \n",
      "nt_epoch =    650  elapsed = 28:24  loss = 1.4388e-02  error = 6.7110e-02  \n",
      "nt_epoch =    660  elapsed = 28:35  loss = 1.4353e-02  error = 6.7054e-02  \n",
      "nt_epoch =    670  elapsed = 28:45  loss = 1.4317e-02  error = 6.7046e-02  \n",
      "nt_epoch =    680  elapsed = 28:56  loss = 1.4269e-02  error = 6.7108e-02  \n",
      "nt_epoch =    690  elapsed = 29:07  loss = 1.4221e-02  error = 6.7017e-02  \n",
      "nt_epoch =    700  elapsed = 29:17  loss = 1.4183e-02  error = 6.7088e-02  \n",
      "nt_epoch =    710  elapsed = 29:28  loss = 1.4138e-02  error = 6.7155e-02  \n",
      "nt_epoch =    720  elapsed = 29:39  loss = 1.4098e-02  error = 6.7335e-02  \n",
      "nt_epoch =    730  elapsed = 29:49  loss = 1.4055e-02  error = 6.7379e-02  \n",
      "nt_epoch =    740  elapsed = 30:00  loss = 1.4017e-02  error = 6.7230e-02  \n",
      "nt_epoch =    750  elapsed = 30:11  loss = 1.3978e-02  error = 6.7137e-02  \n",
      "nt_epoch =    760  elapsed = 30:22  loss = 1.3936e-02  error = 6.7163e-02  \n",
      "nt_epoch =    770  elapsed = 30:32  loss = 1.3906e-02  error = 6.7186e-02  \n",
      "nt_epoch =    780  elapsed = 30:43  loss = 1.3870e-02  error = 6.7263e-02  \n",
      "nt_epoch =    790  elapsed = 30:53  loss = 1.3827e-02  error = 6.7317e-02  \n",
      "nt_epoch =    800  elapsed = 31:04  loss = 1.3784e-02  error = 6.7324e-02  \n",
      "nt_epoch =    810  elapsed = 31:15  loss = 1.3741e-02  error = 6.7314e-02  \n",
      "nt_epoch =    820  elapsed = 31:26  loss = 1.3701e-02  error = 6.7254e-02  \n",
      "nt_epoch =    830  elapsed = 31:37  loss = 1.3669e-02  error = 6.7132e-02  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt_epoch =    840  elapsed = 31:47  loss = 1.3626e-02  error = 6.7129e-02  \n",
      "nt_epoch =    850  elapsed = 31:58  loss = 1.3587e-02  error = 6.7098e-02  \n",
      "nt_epoch =    860  elapsed = 32:09  loss = 1.3548e-02  error = 6.7108e-02  \n",
      "nt_epoch =    870  elapsed = 32:19  loss = 1.3514e-02  error = 6.7189e-02  \n",
      "nt_epoch =    880  elapsed = 32:30  loss = 1.3477e-02  error = 6.7186e-02  \n",
      "nt_epoch =    890  elapsed = 32:40  loss = 1.3440e-02  error = 6.7270e-02  \n",
      "nt_epoch =    900  elapsed = 32:51  loss = 1.3409e-02  error = 6.7363e-02  \n",
      "nt_epoch =    910  elapsed = 33:02  loss = 1.3381e-02  error = 6.7440e-02  \n",
      "nt_epoch =    920  elapsed = 33:13  loss = 1.3348e-02  error = 6.7482e-02  \n",
      "nt_epoch =    930  elapsed = 33:23  loss = 1.3313e-02  error = 6.7440e-02  \n",
      "nt_epoch =    940  elapsed = 33:34  loss = 1.3277e-02  error = 6.7340e-02  \n",
      "nt_epoch =    950  elapsed = 33:45  loss = 1.3244e-02  error = 6.7277e-02  \n",
      "nt_epoch =    960  elapsed = 33:56  loss = 1.3210e-02  error = 6.7195e-02  \n",
      "nt_epoch =    970  elapsed = 34:06  loss = 1.3167e-02  error = 6.7229e-02  \n",
      "nt_epoch =    980  elapsed = 34:17  loss = 1.3138e-02  error = 6.7193e-02  \n",
      "nt_epoch =    990  elapsed = 34:28  loss = 1.3097e-02  error = 6.7229e-02  \n",
      "nt_epoch =   1000  elapsed = 34:38  loss = 1.3063e-02  error = 6.7186e-02  \n",
      "nt_epoch =   1010  elapsed = 34:49  loss = 1.3027e-02  error = 6.7169e-02  \n",
      "nt_epoch =   1020  elapsed = 35:00  loss = 1.2995e-02  error = 6.7239e-02  \n",
      "nt_epoch =   1030  elapsed = 35:10  loss = 1.2962e-02  error = 6.7253e-02  \n",
      "nt_epoch =   1040  elapsed = 35:21  loss = 1.2929e-02  error = 6.7266e-02  \n",
      "nt_epoch =   1050  elapsed = 35:32  loss = 1.2899e-02  error = 6.7305e-02  \n",
      "nt_epoch =   1060  elapsed = 35:42  loss = 1.2871e-02  error = 6.7302e-02  \n",
      "nt_epoch =   1070  elapsed = 35:53  loss = 1.2844e-02  error = 6.7239e-02  \n",
      "nt_epoch =   1080  elapsed = 36:04  loss = 1.2826e-02  error = 6.7234e-02  \n",
      "nt_epoch =   1090  elapsed = 36:14  loss = 1.2800e-02  error = 6.7086e-02  \n",
      "nt_epoch =   1100  elapsed = 36:25  loss = 1.2773e-02  error = 6.7174e-02  \n",
      "nt_epoch =   1110  elapsed = 36:35  loss = 1.2746e-02  error = 6.7147e-02  \n",
      "nt_epoch =   1120  elapsed = 36:46  loss = 1.2717e-02  error = 6.7137e-02  \n",
      "nt_epoch =   1130  elapsed = 36:57  loss = 1.2686e-02  error = 6.7239e-02  \n",
      "nt_epoch =   1140  elapsed = 37:07  loss = 1.2663e-02  error = 6.7174e-02  \n",
      "nt_epoch =   1150  elapsed = 37:18  loss = 1.2637e-02  error = 6.7137e-02  \n",
      "nt_epoch =   1160  elapsed = 37:29  loss = 1.2610e-02  error = 6.7097e-02  \n",
      "nt_epoch =   1170  elapsed = 37:39  loss = 1.2585e-02  error = 6.7098e-02  \n",
      "nt_epoch =   1180  elapsed = 37:50  loss = 1.2556e-02  error = 6.7071e-02  \n",
      "nt_epoch =   1190  elapsed = 38:01  loss = 1.2535e-02  error = 6.7090e-02  \n",
      "nt_epoch =   1200  elapsed = 38:12  loss = 1.2515e-02  error = 6.7129e-02  \n",
      "nt_epoch =   1210  elapsed = 38:22  loss = 1.2495e-02  error = 6.7141e-02  \n",
      "nt_epoch =   1220  elapsed = 38:34  loss = 1.2473e-02  error = 6.7088e-02  \n",
      "nt_epoch =   1230  elapsed = 38:45  loss = 1.2452e-02  error = 6.7073e-02  \n",
      "nt_epoch =   1240  elapsed = 38:56  loss = 1.2436e-02  error = 6.7122e-02  \n",
      "nt_epoch =   1250  elapsed = 39:06  loss = 1.2416e-02  error = 6.7079e-02  \n",
      "nt_epoch =   1260  elapsed = 39:17  loss = 1.2400e-02  error = 6.7088e-02  \n",
      "nt_epoch =   1270  elapsed = 39:28  loss = 1.2382e-02  error = 6.7140e-02  \n",
      "nt_epoch =   1280  elapsed = 39:39  loss = 1.2364e-02  error = 6.7128e-02  \n",
      "nt_epoch =   1290  elapsed = 39:49  loss = 1.2345e-02  error = 6.7138e-02  \n",
      "nt_epoch =   1300  elapsed = 40:00  loss = 1.2327e-02  error = 6.7144e-02  \n",
      "nt_epoch =   1310  elapsed = 40:11  loss = 1.2306e-02  error = 6.7138e-02  \n",
      "nt_epoch =   1320  elapsed = 40:21  loss = 1.2283e-02  error = 6.7167e-02  \n",
      "nt_epoch =   1330  elapsed = 40:32  loss = 1.2266e-02  error = 6.7187e-02  \n",
      "nt_epoch =   1340  elapsed = 40:42  loss = 1.2252e-02  error = 6.7206e-02  \n",
      "nt_epoch =   1350  elapsed = 40:53  loss = 1.2238e-02  error = 6.7192e-02  \n",
      "nt_epoch =   1360  elapsed = 41:03  loss = 1.2220e-02  error = 6.7236e-02  \n",
      "nt_epoch =   1370  elapsed = 41:14  loss = 1.2205e-02  error = 6.7227e-02  \n",
      "nt_epoch =   1380  elapsed = 41:24  loss = 1.2190e-02  error = 6.7329e-02  \n",
      "nt_epoch =   1390  elapsed = 41:35  loss = 1.2171e-02  error = 6.7397e-02  \n",
      "nt_epoch =   1400  elapsed = 41:45  loss = 1.2150e-02  error = 6.7507e-02  \n",
      "nt_epoch =   1410  elapsed = 41:56  loss = 1.2131e-02  error = 6.7500e-02  \n",
      "nt_epoch =   1420  elapsed = 42:07  loss = 1.2111e-02  error = 6.7493e-02  \n",
      "nt_epoch =   1430  elapsed = 42:18  loss = 1.2093e-02  error = 6.7529e-02  \n",
      "nt_epoch =   1440  elapsed = 42:28  loss = 1.2076e-02  error = 6.7669e-02  \n",
      "nt_epoch =   1450  elapsed = 42:39  loss = 1.2058e-02  error = 6.7827e-02  \n",
      "nt_epoch =   1460  elapsed = 42:50  loss = 1.2041e-02  error = 6.7819e-02  \n",
      "nt_epoch =   1470  elapsed = 43:01  loss = 1.2019e-02  error = 6.7944e-02  \n",
      "nt_epoch =   1480  elapsed = 43:12  loss = 1.1998e-02  error = 6.8002e-02  \n",
      "nt_epoch =   1490  elapsed = 43:23  loss = 1.1977e-02  error = 6.7766e-02  \n",
      "nt_epoch =   1500  elapsed = 43:33  loss = 1.1955e-02  error = 6.7793e-02  \n",
      "nt_epoch =   1510  elapsed = 43:44  loss = 1.1937e-02  error = 6.7766e-02  \n",
      "nt_epoch =   1520  elapsed = 43:55  loss = 1.1924e-02  error = 6.7677e-02  \n",
      "nt_epoch =   1530  elapsed = 44:05  loss = 1.1908e-02  error = 6.7805e-02  \n",
      "nt_epoch =   1540  elapsed = 44:16  loss = 1.1892e-02  error = 6.7863e-02  \n",
      "nt_epoch =   1550  elapsed = 44:27  loss = 1.1876e-02  error = 6.7745e-02  \n",
      "nt_epoch =   1560  elapsed = 44:38  loss = 1.1860e-02  error = 6.7688e-02  \n",
      "nt_epoch =   1570  elapsed = 44:48  loss = 1.1842e-02  error = 6.7636e-02  \n",
      "nt_epoch =   1580  elapsed = 44:59  loss = 1.1827e-02  error = 6.7537e-02  \n",
      "nt_epoch =   1590  elapsed = 45:10  loss = 1.1810e-02  error = 6.7550e-02  \n",
      "nt_epoch =   1600  elapsed = 45:20  loss = 1.1790e-02  error = 6.7602e-02  \n",
      "nt_epoch =   1610  elapsed = 45:31  loss = 1.1771e-02  error = 6.7563e-02  \n",
      "nt_epoch =   1620  elapsed = 45:41  loss = 1.1758e-02  error = 6.7381e-02  \n",
      "nt_epoch =   1630  elapsed = 45:52  loss = 1.1739e-02  error = 6.7374e-02  \n",
      "nt_epoch =   1640  elapsed = 46:03  loss = 1.1719e-02  error = 6.7430e-02  \n",
      "nt_epoch =   1650  elapsed = 46:14  loss = 1.1700e-02  error = 6.7396e-02  \n",
      "nt_epoch =   1660  elapsed = 46:24  loss = 1.1684e-02  error = 6.7326e-02  \n",
      "nt_epoch =   1670  elapsed = 46:35  loss = 1.1671e-02  error = 6.7376e-02  \n",
      "nt_epoch =   1680  elapsed = 46:46  loss = 1.1658e-02  error = 6.7383e-02  \n",
      "nt_epoch =   1690  elapsed = 46:57  loss = 1.1642e-02  error = 6.7348e-02  \n",
      "nt_epoch =   1700  elapsed = 47:08  loss = 1.1625e-02  error = 6.7352e-02  \n",
      "nt_epoch =   1710  elapsed = 47:18  loss = 1.1610e-02  error = 6.7342e-02  \n",
      "nt_epoch =   1720  elapsed = 47:29  loss = 1.1591e-02  error = 6.7298e-02  \n",
      "nt_epoch =   1730  elapsed = 47:40  loss = 1.1575e-02  error = 6.7375e-02  \n",
      "nt_epoch =   1740  elapsed = 47:51  loss = 1.1558e-02  error = 6.7456e-02  \n",
      "nt_epoch =   1750  elapsed = 48:02  loss = 1.1539e-02  error = 6.7461e-02  \n",
      "nt_epoch =   1760  elapsed = 48:14  loss = 1.1523e-02  error = 6.7428e-02  \n",
      "nt_epoch =   1770  elapsed = 48:24  loss = 1.1509e-02  error = 6.7471e-02  \n",
      "nt_epoch =   1780  elapsed = 48:35  loss = 1.1493e-02  error = 6.7431e-02  \n",
      "nt_epoch =   1790  elapsed = 48:46  loss = 1.1476e-02  error = 6.7414e-02  \n",
      "nt_epoch =   1800  elapsed = 48:56  loss = 1.1459e-02  error = 6.7429e-02  \n",
      "nt_epoch =   1810  elapsed = 49:07  loss = 1.1444e-02  error = 6.7377e-02  \n",
      "nt_epoch =   1820  elapsed = 49:18  loss = 1.1428e-02  error = 6.7419e-02  \n",
      "nt_epoch =   1830  elapsed = 49:28  loss = 1.1416e-02  error = 6.7471e-02  \n",
      "nt_epoch =   1840  elapsed = 49:40  loss = 1.1401e-02  error = 6.7493e-02  \n",
      "nt_epoch =   1850  elapsed = 49:50  loss = 1.1384e-02  error = 6.7596e-02  \n",
      "nt_epoch =   1860  elapsed = 50:01  loss = 1.1370e-02  error = 6.7642e-02  \n",
      "nt_epoch =   1870  elapsed = 50:12  loss = 1.1357e-02  error = 6.7659e-02  \n",
      "nt_epoch =   1880  elapsed = 50:23  loss = 1.1344e-02  error = 6.7662e-02  \n",
      "nt_epoch =   1890  elapsed = 50:33  loss = 1.1335e-02  error = 6.7664e-02  \n",
      "nt_epoch =   1900  elapsed = 50:44  loss = 1.1322e-02  error = 6.7611e-02  \n",
      "nt_epoch =   1910  elapsed = 50:55  loss = 1.1308e-02  error = 6.7613e-02  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt_epoch =   1920  elapsed = 51:06  loss = 1.1295e-02  error = 6.7540e-02  \n",
      "nt_epoch =   1930  elapsed = 51:16  loss = 1.1280e-02  error = 6.7501e-02  \n",
      "nt_epoch =   1940  elapsed = 51:27  loss = 1.1268e-02  error = 6.7416e-02  \n",
      "nt_epoch =   1950  elapsed = 51:38  loss = 1.1253e-02  error = 6.7398e-02  \n",
      "nt_epoch =   1960  elapsed = 51:49  loss = 1.1238e-02  error = 6.7497e-02  \n",
      "nt_epoch =   1970  elapsed = 51:59  loss = 1.1224e-02  error = 6.7495e-02  \n",
      "nt_epoch =   1980  elapsed = 52:10  loss = 1.1210e-02  error = 6.7480e-02  \n",
      "nt_epoch =   1990  elapsed = 52:21  loss = 1.1198e-02  error = 6.7498e-02  \n",
      "==================\n",
      "Training finished (epoch 3000): duration = 52:31  error = 6.7481e-02  \n"
     ]
    }
   ],
   "source": [
    "# Creating the model and training\n",
    "logger = Logger(frequency=10)\n",
    "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, inp_f_train, ub, lb,  R, temp, L_a, a_c, D_a, D_b)\n",
    "def error():\n",
    "    u_pred, _ = pinn.predict(inp_star[0:100000,:])\n",
    "    return np.linalg.norm(conc_star[0:100000,:] - u_pred, 2) / np.linalg.norm(conc_star[0:100000,:], 2)\n",
    "logger.set_error_fn(error)\n",
    "pinn.fit(inp_train, conc_train, tf_epochs, nt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred, f_pred = pinn.predict(inp_star[32768000:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   2,   3, ..., 126, 127, 128],\n",
       "       [  1,   2,   3, ..., 126, 127, 128],\n",
       "       [  1,   2,   3, ..., 126, 127, 128],\n",
       "       ...,\n",
       "       [  1,   2,   3, ..., 126, 127, 128],\n",
       "       [  1,   2,   3, ..., 126, 127, 128],\n",
       "       [  1,   2,   3, ..., 126, 127, 128]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_star[32768000:,0].reshape((128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "32784384-(128*128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred, f_pred = pinn.predict(inp_star[0:128*128,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = np.asarray(u_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = u_pred.reshape((128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54734105, 0.5523704 , 0.55647755, ..., 0.48859262, 0.4896335 ,\n",
       "        0.4906696 ],\n",
       "       [0.54608005, 0.5515458 , 0.55611265, ..., 0.48833802, 0.48931965,\n",
       "        0.49031952],\n",
       "       [0.5444604 , 0.5504689 , 0.55557835, ..., 0.4881748 , 0.48906928,\n",
       "        0.49000552],\n",
       "       ...,\n",
       "       [0.5296041 , 0.52979034, 0.52994925, ..., 0.60368514, 0.6433724 ,\n",
       "        0.66754836],\n",
       "       [0.52311546, 0.52317476, 0.5231973 , ..., 0.5092224 , 0.5896044 ,\n",
       "        0.64137447],\n",
       "       [0.5165731 , 0.5165146 , 0.5164135 , ..., 0.39445904, 0.48831165,\n",
       "        0.5728438 ]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATQAAAD7CAYAAADkSGhKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOy9eZwd1XUu+u0zn9Onu0/Pg7ql1jxYSAxikgVmBvNkYzwCsbFjO+TFCe/Z2LnxvZl8/fLynJfEiZ2b6/u4xDG5dgweMCYEAwabAGaSEEIIECBASI3UklqtVqvV4+lT74+1V+11du1TfaRubNHU9/v1b1dX1anaNa39rXErz/MQIUKECHMBsd90ByJEiBBhthAJtAgRIswZRAItQoQIcwaRQIsQIcKcQSTQIkSIMGcQCbQIESLMGbxlAk0pdYVS6iWl1E6l1JffqvNEiBAhAkO9FXFoSqk4gJcBXAqgF8AmANd6nvfCrJ8sQoQIETQSb9FxzwKw0/O81wBAKXUbgKsAOAVac2Oj19PdDUjhWo2gVaq6dccLz6t8fte2SuuOB0qZvnMbi5k2mQQATEzQqrExs1xbS226eAyYnCw/fzxObSpFfwBKmphPTZndXN21uyNvrWudc5t9glLJfTK+Vt3fYtH8jC8hNjkO7vgxL1f2s1gsvL+ME3ks1ayrZtts4ESOz795+umn+z3PaznRc1+hlNdf5b5PA/d5nnfFiZ7rRPFWCbR5APaI/3sBnC13UErdAOAGAJg/bx4233MPveylEu1gtxLyLXati1XQpOV6eVz7HKWS+aLsfhSLwW3FYvkyUC4twsBvWzzuCxy/zWSorasDWlsBAG/spv137AD27qXNF1xA7cIDTwJ9feX9qKujdv58+gMwPJUFABw9aoQi7w6Y25RIBFt7mxQkLHh4WzLhmRPI1r5/8loLBQDAoQHl94svIbv3VVoYGsLjY6cBMMK8psaX+X7reg1crxOvc+0f9qq5fiP/r1b48H2rBtW82pW2JRLqjerPFEQ/gM1V7quA5pmc60TxVgk016Ms+7o9z7sZwM0AsG7t2rcm/yrsbask3Ph/3m4LtFjMfLH8YcZiwa9CCrmpqfLjVxJ0tlDkdmKCKBmAXI6EUSZjTsUybGFtTfCa+RiC0iUz2bKu2l1wffQM16VX/pgUkiycXZCCTfYTQDpNfZyYMLIw29lJCw8/DKwggcaEtFg0fQuD/VjlOtd+lcbMSr91CbHjEViVECawgHChPKuo9gRhL9FbiLdKoPUC6Bb/dwHYG/oLKRSqhRQ8LoYWJtDC1ru+6jDhNd06W1ABQSEn97P3FwItn6cPva7OELl+1gN6moA9e8xv5DHGxvxjJPL11CbC30/ZXRthwqNcXtMXXibYXOybl3W/a+roOkdHzaVM6WuPr1qFLi3bWJi7yP3xftzTMS/Xa2Wz00rHe6sRdq5Zky1KVTdqAOah/ZrxVgm0TQCWKqUWAngTwDUArnuLzhUhQoRfF36dUvoE8JYINM/zikqpPwBwH4A4gG97nvf8tD+slqUd7xBcja7gOr7rXFI9CmNoiURQdZTHcOk+9jklQxsZAQBktUGpUIgjR3ZxHDlCrdfeAZXSfhe9v4vlxTGluxMP+B/CyKkLYQN2+e8EU7NPMDERYGhqghwAmUwaw8Pll1S7aBG677kVALB3xScBkOppa7fV2MYqrQ9jaC5WVq0ycLz4DWlubij1zhRoAOB53j0A7nmrjh8hQoTfAN6pAu24MR07m86KWy1rqzQEu/aRx5Wtza5cDE3ux0M6G5ekwyDMmyuZmmZXGBwEADQ3N6GpqWwVXn4ZWJ7P0z9DQ+XHEAzNdw4ksz7LYOO6Cy4ToET1LA1wOgok62WwEySf9pmZb5Zpbva9oXyoqamgDyaMkFd6nVwmWb/nFjNzOUSqZYPVKiInnfw46TpUjpNHoEWIEOHkxjtZ5TwusPdkOtsSYzq2ZiMsRm26UA5X+IbLnRbG0OxYB6XK2Zp9TS4j1uho2bFq2/OYNy8NANi3jza9/DKwfE07/XPwILXjOhhV2OGY/SRz2bJ4MvtSXI9gNuxpsRRRnDjTK9c91T+IlyaRyVBgGXd/dEwhu2YNAOC0GIVWbTqwwD9HWKSI34cKTMpmZpKVhTnUGQrVRSDFxe88Z5STG79Rm9rxeDl/Qzg5eicFWiVBUglhMWRhON6ISZfAcQXWyjg0l2oKlDsMpDoaJklYALLaODCArq4OACZ0YccO4EhhAQCgPruTVkqBxr/VbapQnQZuXzZfXiXId96+BVNTRnVMScFmOQX8/8fGkMuVC7SRERGT9t3vUrvmM77aHBa+MZ2loZLhX4a4+PvAc79/x4NYLCjO9AniseqF3a9N0EUMLUKECHMGkUA7DjBLA8KN5C6EqYvTYaaOAqmnuQJww8I2pDpayfoumSvTkJERJIcOAQCWLSPvwIEDwEMP0earFrfRAsc8FIuG4uh1amIciQSprTJ8oxpyypiOqfF2vkzPM0Z7/7akklAJS/UWgcFJ0DWnUiafdRzU7/SppwIAWpuBgYHy49qpWDZc1ge+D3b6VCIh1Mkw8wOj0rsa5jVwMHpl7ROLGcZWjeVlVplbZEOLECHCnEIk0KpAqUR2nUTCnSzI+7h+Zy+7bGhhxpRKjoJqAm9dI7W0oVWyB1ayr/GyfQ9caVLFos+0mpopsXvNmho8/DBt7i2sBgB05ffTisHBAEPDyAhSGWI6zEhcp2KEZWtVAl+Ki00w2YzFRGqU695qtsa2tNFRcynpJUsAAAvuvBUHVlGQbTXOAadBX7mZGQCo0lT58+O+HQ9Dm84TYe/n2KbE9kpsbbpTnjAip0B1GDyWxE8ea8PVGw4GP+pq6P10+81A9XQaZWOkyyiXGhqmclaroroEW5jLUQeiLexKYehU+iLvu482febKVbTw1FNGjeMYteFhZAoNAIyAkN9QmLAKE3xhv3M5DIpFh+dT3jPd72wdCe50Oo5jx2hzvo3yUpNNTf7x+DK5gIcc41yvgsuTWSbIuD9hVVaOV6BVE/zmcruK30l1VAq3SqefFUQMLUKECHMCkQ2tOqTTwNKlwE8ebcHVl+mhl1mEZGqu0dBlSHfFjnFb5QNhZhY28MpRMcasLSXOXUltrqSOuvoLuJ0lrhiKgQGsWUPOAL5933+IQjuuXdIJ7N6Nso39/VB+xD0VFuNwt0pwVT6y2ZqLuErYWsvUlDDkp6wibPK+6HCTbLbGj0Bh7blh7VqckaHYu027qIYhM7R4PHhO6QBwRf77DgCZj2vn5lab8WGf2P4/jKG5AgVdbM06bjXq6AkhEmgRIkSYE4gYWnVIpYDubmJqd/+yBgBw6aXUpvt0kU0XS3ElIIYxoxnY18LIoTwMj4yxWLycrQHl/XJVxA2zx1TDBEolqMHDAICzziLbGDOYB4fOxMWN+h8uoNbf7+dE5nuIoY2NGcZVLemwkx5sn0YlSPLB55gq0f2Lh4Sz5Os8HDtG+7EtraGzE/jOd2i3VZ8BYGKKZdxu2GMvs8G77GWycondt2qZ2vHYyeQ7L9dVcQxmbB7U7MqgSKBNj3icChZKhs3eumXLKPJ9wdirwZrQpZL5+lxWald8T5VqaDX+B9e68nfMCDdAqKMuL2eY0KokRUM6ki7Sl75+PQ0MDzwAvLjqQgDAyuLPad+hIaC3l/qmk9qz2RZfEIQhTMjZSRASLlu4fIx83LiUdva9GhtDTQ0VezxEoXg4OhJH7Wry7Ga0X4HV52zWPca5ijIGYs2kemkXzZRlj2yPdNgkDYBbz7XVS1mGV77zLjW0gvfRVkVnhMjLGSFChDmFiKFVAc+DKk4in08GCg7u2kVtX2Yxzm58hf6Ro5Y9Ikqm5qLmYeqceFhhfgWGixTyOlfxv0RCM7ZEMmh0nk6trCYswLoGAKivo/Ns2KBw//20rvGySwEAbdt+TukFgB+0lV+dw2QNsTo7N1KW6JmuG9XApTH5x0uExFkUi8jn6bpGR+mejowAtTom7bQXHwUAPOJt0PuYmDRJdKr6NuWzsFVO14QvrmIDEmHqos28pMop9Xk7rCcsBGS6OuvHg8iGFiFChDmFSKBVD1WcRE1Nsmwd37/eXuBnO5cCAN676CWzkUO7edR0Jgs6RkMZTh5iMXYFmVYziVOxaEwmtn2ITB6GrQFW5QZXWy0VqhBBWlcHrF9Py3feSe11112K2s2/pH+0LQ2JBBq0LapYpCwCjvKQPphqbIyV3n0/NEKw8YqmmUoH0XEbhQLZ0gYG4Ds48CpNdzfRRQxtfNy8Hk5HhCgIyeE6AfOffAZhdjV50OnsaAC9v5UYWjxu3u+w8CVXtLJ0qsymEIoEWoQIEeYEIqdAlRD10DjVpKZGp8LEy3cBgB9vXw4A+NCprwZHqWIxPC/HZjDSkyRGPuMdUmW7S8jThBEomxQq5fKKKiQSVkpVNbY0+0TW9XEYBGAm7D3rLGpvvhm44QbyfNbu2EQrd+/2z9GiK1mUSkm//57lBJyuS5W6KduqCajjpMkJ8ubW1dVgskT3L7l4Ma0jgonRUfgTykhbmv0MZD/irgduP49KoRzVXBQff3JS5FlZpT6kjZg/hHQ6nMHbAmc2BVBkQ6sOngeMTyikUnFfoHGbydCDbGoKloP54ZbF+MgGXa6V/feS0rtC2PnFczkKHKEcLpWzGrg+ePkusGCQ0epmv/Jwj1gi7q6E6jiwneFgB7cDAE87sHo18Dd/Q8uf//yZAICGxkZg+/ayH7VpwQZk/fkLqvVbuLQpO17NeW9dQoMhBTfPhpUoYipFeZ3ooOyIM3v/AwDwYPE9VYX9hWYsyIcnbQhypmPAvF+Tk5UnlAbCE0ilEGPhxpLY88x+dkJ/JcymUIsEWoQIEeYMIoE2PTzPDHRclpkZCbeZjPJtvpJc/eQJGo2vvrKRVu7YEaRVPOLJiNFKoeL8O/1b5TO1YJRomGbrQhhr87zKs3CXB+nKLXrHENYhiQNfPm/L5YD582n5v/03aq+/fjEWXKbLW3N08xNPAADa1qxBopmKSXKywdhY+CTZLvJrO0nsaw38wJVywOyEPRZDQ0ZNZAq6bRsAYGLJe0K1NHc/rBnfJyYM05EPyE6T4BdZ0r2wC5Vsk4/rUjmlSllpAtJK55otRCpnhAgR5hQigTY9ZKQFg8Ma4jHD1FIpWqfjPtHebu7vv91PFuBzzlmLlt1P80GolaMW05SwYMTyjuhNZlT2Dcd61dRUuK2tmkFT2pvseSVdGTOVUCnOc3TUXLqc55IHe2537wb6+igUYtHplwMAWnY+Ths3b0bTihUAgPQ8Sknbv9/ki9rzm1SCzPrh1jYjYcQRvCrgOwD4h8PD5sTM0Lq6AACNjdWxSAnfzqdnnFKZjJzdhdqxsWAYhmRsrlAOG7IUiP0CxOPkBJDHSIqwJte6txJz2cuplOoG8C8A2kFKz82e531DKdUI4HYAPQB2Afio53mHpztexY8gYVQtft78jAGgtZX7Q+3mzUBr6xkAgDMmtOfONV2axHTqJ8pVzzCPp9xWrSCzl23h6HImTHcM/pZYiI2PG8EjW96fVc/WVjM51GOPUdvTcy4AYG3zK8ALLwAA8p3kHcivXo19B0i4sMPAdvjJa5AxZ7LlR6QmdIflDFW8LD58XpXkdXJWKz5ZC5UROnvgZ3gk/15IVIp94/thC8BsJhP0sExMmB35RksBZwsyl0CTozlLepfHcjqPaaVKAPLDmQ2c5AxtJr0rAvii53krAZwD4PeVUqsAfBnAg57nLQXwoP4/QoQIb3ewDa2av98QTpiheZ63D8A+vXxUKfUigHkArgJwgd7tVgAPAfij8GMZlcuOc5LB/rZmmE6b/ZuazPHefJPan5UoFOG9zZqpyRHPVXqIEfJQVCJRtYPAPkS19tqw8IcwlVYSUL48Jg6yDj+3pZJhZnreXhQKJn+Wz6lt6xicvxTvOaeZ/tm8mdrHHkPHsmUAgNpuKi55WPPxsbFgfxMJw7CzpNkinwfSMd3hITPfgd/aNbUTCYxoX0BtXrBv/g07CvgEjzyC4VPfW3avXO+Ty1FgmJpCllVZV0hJWOKrpO9hD9em+dKDwnBlu1S6GG6nq+N0PDjJGdqsKMRKqR4ApwF4EkCbFnbwPG+fUqq1wm9uAHADAHR1zZ+NbkSIEOGtxDvBy6mUygP4MYDPe543pFxFsBzwPO9mADcDwKmnrvMA92A1Hdj2wpHgEvv1hEc/3EVM7SNLngmOkGFMTUKMfCaK3FyrdBDw7vapZuI4qGRfs/eRUQNAOYGwbdSNjYaZNZR0YHKs0WegTEjY5rZrF1AqUeHIC88/n1Y+/DCwZQvtr43w+Z4eAMB4qtY3a8n+8zPLZjS9HhkJGvik54KhH/J4KelvRkG8wnyBvJEZXXs7eKJ13lQomGcmHS2uKtvc/6kpfV845cIFV5kVyajC3jfbW1LpuMeD2Q7jmEWBppS6AsA3QPFHt3ie9zVr+3yQllfQ+3zZ87x7wo45I4GmlEqChNn3PM+7Q6/er5Tq0OysA8CBao7Fwsz2IMrtYeD9XYKNn8H3d5yGa8+ixGU/kCqRqF6oMRyeT/s5S88nQ37UJ/qeTReZb6vsLnVUhms15LROOmAEB2vmfC9rqZgtRkfNbXt8C+mN565fb+LVOMNg714AQLqzE+lmraL6UzABGNZSol8Y/lnSsASULlPuiJawBw+a3XzIm2o7EZqbsfa57wIAHlnwcQDkIed3RoZ12Z5lmQhgajiywK83s1Tx9cmWy+my3u8qNyRHcbsTsvSPzA5weVVcRR/tbTPFLHo5lVJxAP8I4FIAvQA2KaXu8jzvBbHbnwD4ged539L2+XtAzsaKOGFxq4iK/ROAFz3P+7rYdBeAT+rlTwL46YmeI0KECCcZZs8pcBaAnZ7nveZ53gSA20D2dwkPANPhegB7pzvoTMTtuwF8AsBzSqmtet1/AfA1AD9QSn0GwG4AH5nuQOwUcA1W1YJHW88zgyRnFjBiMeD7T1Hi8rXvb6eV27YF49U8ryoKpfxBMe6so+8K5eDTuEIzjjfMw15XbbJ8mcPF9r6USn6SPN/Hep0imUya3ZlQvbK3Bks51/NRKqyI116jtq/PeGv4IJIOSf3YFesBEDvTKt54gZwO+7eXh+4AKL/JdsxKLgc8+ywAYFB3Y2zMqNQ+KytNIa7LpHMcJKNUCnZxbAzI5cjxkGumNs7q6PCwm3XawXou6iyvKYxxuRhaWEzMTHF8NrRmpdRm8f/N2szEmAdgj/i/F8DZ1jG+AuB+pdSNAGoAXDLdSWfi5XwUjrJRGhef6HEjRIhwEqN6gdbved66kO0u2WFHWV4L4Due5/2tUupcAP9LKbXa87yKw/VJE/bLA1XYYFUN5CDCoziTA3mcH95D6QYbNpyLjp2P0EqmJGFh5a5zJow97XiDbiVTO16nwXSxlrJNJIJ2yaEhYBTELLLC/pRK0b3h2yGrY9gxoGNjwJFWyqet11kEeOopavv6jNGNmUs+H7T3yAvhbUyfmprgLaNyUS9sNf3W/ofy38kgW0AmCPtBtkwmBwaABvJvBMuhw5QP4oKj8t7x5CtjY8aHYUxcdD8zmSySmpxy9EgcU25vw/EYVF0qnYwSth68BzV7URuz6+XsBdAt/u9CUKX8DIArAMDzvMeVUhkAzQixy5/cPtgIESKcXOCBY7q/6bEJwFKl1EKlVArANSD7u8RuaG1PKbUSQAbAwdDuHfcFvQXwPFMp2PbOudjNdOlFLscQYLx1Eo89BrS2ngcAOG+E6mchk3G7Cbl1nFT5Jgsayl2j4nRMrZrQjGphMzRXNeeREeDll2l5bXvGX5nLlzM0u4ScPEYqZUxE9YsW0QJPvDI4aPKhmMrk84Y6c5tImM6JUAsAmFq5GlspKsQne7mcwywkvX+2sSuRALS3tfvOfwAAvLDsRj+o2IcjCFXF6FjZVAqJumTZfRgeDkabuApsmGcRF0yerl2pytEalciYiyT9WsLDZpGheZ5XVEr9AYD7QCEZ3/Y873ml1FcBbPY87y4AXwTwP5VSXwCpo5/yvLAicyeJQANMDFWlPMZKDoNqVDz58XFiO0MpUxvyx8X3AAA+1PpIcJQJE2ilkv8hSsFWieq7yrzPdrgQQ/o7bAE1Pk7J6ADQ00MG9/qhPcgWSFXL6MTsQIgEjEDJZETsYIZublzHoeHAARO1z1/8+LgJw+CHkcuZdZycq+c12LrFyEc+TyYjVEC+wEymPGmcL5pbK6hubMzI2kJBzyMgBZotmWIxP2+0Qfc1n0/7xxBVjADQYMHdYN+ErCjEcAk+1/9ygLIL27p8B7Kd1XzyWZScOqbsHmvdn4nlF0DOx6px0gi0CBEinOR4J2QKzBbicaN6AkGm5gpUdY1gcsRjciqZGo9qkqnxaM8qzXffOA8fP1UHiXJioqzYYRfdk9G8ej+VSlVUP5VyV82YDVXTnteUIb333N1iUc+WBL+GIy6/oNWnLnV1ZEhnplEp/thOLYw36mKbPT3mpjIVZIs6YB5GLmceiFZbX9xBF3LggLnd3G/J0KZ0kct4JmN2sINzi0VDT3XKwFXn7Mfjr7XJVUjLi7CPMTER8IgkUym06OO2NBMD9BaRB8qlbctIjrCpPe22EioxOLudVRkUCbQIESLMGUQCbXqkUkBXxxT27I0HGBrDZTS3tzMq2QxKJUMKmCFls0G2lEgA391KNpyPX6tH5YceojaXC3osZF6MYGtKUyKbqcXj1dVPmw3GJo/F94Vt8bmcYQrMJh7dlMaGlXTibIlSd/LaSSBLjrlMin4/mQ21tpr4CqaCAwOG6nGbSABtulJHglihzp7CxIR7xiaGf0+lHc7VSb6ZHOh7883oXfGnAABdLATpnCN9StIrWaNNnlx0ip95QyaDBrbbcchKa52/PKXDO1zxt644XGnac8RCO826s465XOBxVnH4MHDnnWi+8kO+AdgOBXMJMdcs92GCzZUHKY3l8hh8nNt/RMJoxQqKFV7bd59Rj8K+arHOCLZgZdFK2QR2f49nm30t9rLU9Pg4LFt27QJqa+mjX9tJHvKmZhIUo6MqYDOXcxX49zGnJU8+73sr/ZpOR44EKw+k077ex5opf+TxeLCqbixmDsHvSTqTMYZ/Vmul+sgXyq7u1laeHMrXipsWJYIJ7izpBwaCOqR8ScOi96UHRQvduG7rcznUc79ZIBeEbm3liE4WVZmTgbsRllR/ksahvSU4OQRahAgR3h6IBFoVSKWAzk5kH74PdWdRHXt2fzNzkPfRZfiWyzwPgasQI0NqjfZAKsHH5XitXanLcVXiQfNjPphNdaQayiEduqWZhMr7Nl25pDD1s9pSS3YYiyxkYSpJmIKOuRypf0tjFNfS0tLkX54kKXw8VpVyOWGo54RarVJi/35ThYIt+/X1mGqfBwDo31F+ndIB4Joi0y+PhCSSzHSYOcnS12xX4IN0dmLDMxSTdt+yGwEAy5cJhnb0KLWsMvT1mXWM+no/vs1nfqzPF4tBx8LkZLA8UiZjXnZXxQ6LniZTKVN2XN6QXIVqG7PtFYgEWoQIEeYEIpWzOhTTNTi07Fw0bX0QDTtohqHiEpqYgweySlNqsj3I5TDgHL2EmGjlRMGD4eAg8P0DZE+7dv0btHLHDmNXk1SnUpBmqeTP9xhLVc4skKjWZhYWR+0K5WDI6tLcF54kJXMR2dS6E0fQ3Fxf1t+REWOykjGuAFCbSpl/OJSjri44e1Jra8B25gq1kf23p8McHweSNRbdlDfVPnBDg18VhEuOj44pZLlvHK7T10ft4KC5ScsptxSnnOLb/qZiOucTU2Z/ZnfcDg1V6VURqBSPYe8fNuXWbHoIIoE2PUZHga1bgfXrL0b2oZ8BAFr2UxzYvibyNkrjM8OlIpKjQEd+W5MVx2Pwp8JzvR8M+Y7x9yj3Zw3h+4/RVG7nnLMAC7f/G61kj5aUDLLAH0Afhj5wXLex1PFPRcZ9lNVyWR0PTxAx12ILIdfxH3iA2iuuqEdHHamLxWYS4AcOBL1zLDtqWzPB4Ld83txAXtfZib5eWrS0dKfjxxXQPz4OxON08YHa/3yx8geplO+BXddDq4aHgSxfBHsK2DtbVwesXw8A8M6hwXb3buDoS7TZxDfSw2hra0LSdjCMjIQnlvP9kIn8MviO+63394TZwk+wt4WjNH3MFJGXM0KECHMKEUObHpOTwL59NJHQhitodh51NzGeDm1s7Y0vCNTJByqzNIAYGYCykZrLwsRSlR0G07Eb27i+eTOwPfM+AMD7Sr+klcPDpm6MVEP5f8sarzIZCj2AYZiShYQNsserUVQKgQHcTI1xzz3AlVcSM/OZWmONT2aYgPpmggmFtO1xkZnl2mEw1dmNoRfK++Y/w3h4SIuMnDD7EUvKytr/tueiVPLzRs/Y/P8BAF4t/C5auPPMzPhEy5f7zGyHdlwMDppHyo+a+zA2JuYMDQs2jMfNTWezBavnhQLGKX/B9yWMHg7OF0FQ+jI5+T2u/0/OWn3HyIYWIUKEuYVIoE2PUonsaL29Jqfw3Msuo4V77wUAdK3PoXecwghc9RedIReahfGs59L4wutSqfgJPyO2VyUSZgT9ft+FAIBrPzwJ3H03reQRV9IK266Wy/l9S+oRO6GrXUxMmHO5R2cD3m7b0mSmBaMSU5uOpQGCqRVGUSoRPTl4sPySJiaAtG2slI4CzZB27w5Oei7LHjGkeci+/ljMVRVEM7VCIcjQJiZMiI3u+MgIgnN7ckd6evzYYN4k30NnPqaLobnA+zHN08zy6ETaJ4qyioedPeAuVUStLBs1Y0QMLUKECHMKkVOgOsRiNPr0am/X0ymyHZxx0UW04qGH0HXBBQCA3iNkV5MFEBjyfht7DHs9BXymVjK2jozeTwxEVU4zGrD93H5HEq2tVwMALjyqmVpYzpFM3NP2NQ7ETWcyvm1EFlu0R+iwopKe5/YWhhUQDKutxUzt/e/Poq0wrvtWbu9xVjKPxQxD015GjmoAqiM1xWJwXpuJicr5jFNTCtkcvTPsVS77gWZGiQRM0K89+3ptbWBy9Eo1+vx9Upb9UL6cMlytCxUAACAASURBVLLbRTdBZJJjeWXWFZNIe9J2CVk0ctZkUMTQqkM8Tix7aMh8DByXlEjQi7h2/XrgP6iibNellwIA9hxI+x8sa3BhBvJkIhZ0a4sYDRZssezxTeoaiwWrjiYSxq58e3EjAOBjG/XHcs89JsJcCjQ7/oEdB7mc/yFmQ3L6XEJOwhb+05VfsqMJ5MfB+999N/CpT5Eg47RNjusqFoFAkXdZbFGXjB3aHeyHy44u++YayMIEO6veKR0ek8oloSyJWyjACLQjR8oPNjGBmGXWmNZZUeeI/OcbyOfhkRwIZKknEmm/39LhYiezF4uVs0UqOVVOGJFAixAhwpxAxNCqQyJB9uFSyYw+NlNLpZqw8owz6B9dyqf7kkuwZy9RI6laMXi09NdllFEvXYGHel08kQjM9nO8z1GyGb6m2+8mQ3pz80dwcfHntJJVm0IhGIsgGZtgawA5DpI1RBkmi6QXj48HmZa8NHuba1R3jegyC8OVRnjbbbR87UX7AQD5fJv4nUWvpqZ8J8nhsazfx7BAZ1c/7NAaV8C9y4Ei98myKqh3KBRg7jmnP3A7MIDU/PJrlwTPvt8TE8B4kV6etAyUZeYnp83ii2ZKrxlsoafeVzW5ledghj45WV2ywawgEmgRIkSYM4gE2vTQxTbK7LQ8+vGAtmsXkFhCBayWrtJD1EMPoVs7Dfb0mmBUPoasmu1DG/59piYZmpglSKXoR5mMPUV3cH5LF1zGdf7d4CBwe5HsgJdcQuuaHri9POVF9mdionwySICYGlfv4LYm5bM1V30sexSvtgabrDztKvvFLPpQgphZo+7+wACCBi3P8xkaExKZslUt7P3DikpI9lZWQJT31y9KKoXyGWQAQ4127UJuxdnmt+LY8rgy/YvJd0ujrjgijcQybMcO5uUy7pkMWlronXeV8eb3aXw83EEwa3gnpD4ppeIANgN40/O8jUqphQBuA9AIYAuAT3ieFzpzbzJJBuWxMfNg+D3i/4eGjLE5tYzmJ12wpAQ8/DAAoPv88wGQYLPzdKWWyevyeV3iJpEoFxzc6mWVoW1ZP4o/GLc23aBllztKJs2L/4tf6GvKfAxXrdLJ7pwVzo6DTCb4oR07ZiLLhdHZF276o5gSye/8snsi7S9MVbHvn1xnF10E/EeBqy85qq+3Fig6TqAzBPjDrBZygHCVFLIHjmnNBVbC7ugokLerYLLxfvt21H/4w/p4cf/nlZwwsr5jJkP71zY3m+PxxY+Ommdqx74lEsgvohvc2UkFAsbGgnFossAjo9qSUseFt4ENbTZ6938CeFH8/1cA/s7zvKUADoNmP44QIcJcAI8c0/39hjAjhqaU6gLwvwH4vwHcpJRSAC4CcJ3e5VYAXwHwrbDjxGJAbd5De7thVzzC8IA2OWkGMF31BfGlC9A1Xw+Nv/oVAKDr3Rv8WLawSi2MfD6NeMrSc4vFcrYm2nQ+j1iuPPR6Jg4D1iQnJ4Hbn6DqHe2d1L4n/ihtfPNNk20g46h4ZOeigtlsoEggh3vEUymka+ikU6Wgeu5yqjDC2FsqVa5KA/BL7sQKtcEfAL5KXdLPU6rwrtg3O8ZPsh/ZuuartI9RFl84Va4v7t8P5Fnd5/vItHbbNj9IMpFY4F+SXQHGxdj8dNb2WqS50CX/cGzMnINfWL6R4kVpWkRtcX69s6CGvc7FwmcFJzlDm6nK+fcA/hMAfnObAAx6nscEuBfAPNcPlVI3ALgBAOYHprCOECHCSYe3gcp5wgJNKbURwAHP855WSl3Aqx27OmtXeJ53M4CbAWDdGWd4mJhAY2PaH7hsm8DoaLk9DQBefx1ILlsIAGjroGFIPfUkus4i4y0bq2UAol34AgDq6ih8QEmjhCt0Qh+ESz3n8/S7433GMjSCW1nKmqtX/LC0AQCwbAWwdi/VifOnQ2psDE6FPjpaPiGH3eptcW4TCRPNrjvC7A2orlZgqWRO6Y9Lw9o+VBAXyCxEWPOlHc52VEi7o83GREkwn90lk+EZDvZxk0mAazHyPXrhBWAxz9zOjJjtlHv2UNE+AKkVxNCkXdJmujKUghGLAZ2dlI8cl8/MnnqL7WuOB9C2ZAlK82vLNruek3znI4ZWHd4N4P1KqSsBZADUgRhbQSmV0CytC8DemXczQoQIv3HMZS+n53n/GcB/BgDN0L7ked5vKaV+CODDIE/nJwH8tNpjxkuTaGoiA4gMGmTY7O3IEeP5TCxZDABoKpWgtjwNAJh/OgXiyvLO9gxnnmdGsIaCCJeQIyhQztT0clx76/J5YSvSkKzArgIhcxHl/jYT4S68/DLwMqhO3Kmn07qlL91tqByziVyu3DYDGBqUTBpbm6RGVp5hXHTEN22VUR1rHQBPe/H8iiA9awEApX5xLmmc032T84rIKTq5u/y/7VGV6+Q9c9rJ4LbRld1/ff+2bAHe9+kltI7zuPjeDg/7Ad2N668CQO+ea1o/Pr6cQ4db7lPXvE7zA754fnacwMkGZIlYDB1LdB97aoLbLQQrkMwAc1nlDMEfAbhNKfUXAJ4B8E/T/oIl/8QEshnqUmMjPXl+QTzPxKTJyXxY/TQhHUtRi1fosFufAQAsOPVUAMDru5Qfz8MPenJSqrd0zgZZ955b+UNrNh9VLKKWZzdyat3lhwLKQt7KrgkIfnRyYmKt9WBbbCPWrKPlpa/dRwtvvGFmWWJVSX5hdvH/SpLBTth06W6is0rvn9fHenorCc7OTgD5XPBitGrVoSf4HRwMTqgkBZtL5bS7HY8HwzRc2Qe+sCtNGUnjD0zA/hSFBLUtWkTb2AM1OEg6KYCWBM03kMk0+K8Fw1WU060a0nvS1dUVfNfk1Fp8Aofe6BJsrmufVaH2ThBonuc9BOAhvfwagLNm47gRIkQ4yfBOEGizAS8Wp6KLepTK52mUZ8Ihswhk+RQeBXkg27ULWLJkKQAgu1vPYKEnmly4Zg1efY1GRra/SsZvmFocDaxq2LrE8HDQUSAMu7UcDIsq0gkQdH4AThLk78csZHJSsjWay3TZWuCUkSdpJU8kymEI+Xz51OMAMWM7xsGlu7miV2VrldlevpxCE8bHgWGP2EOe72d/v1HZ+yn3s6Ojzb9Wex5WydCk6mmvc6qc9sQhgJ9aWmYt1/foi6vvw08eo3t59Zo1tI0Z2siIUfHvvx8AMP+sjwUcVdIYbwfAugz08XgSHexNsZlasVieIlABHUuW+CytGlJ9wniHqpwRIkSYi5jLToHZhOfRYJQUVCSZIKNRba0pbGizmbExd+4nOwGWLKH5E+Mv60SGbduwWI+8r+wkpjYwYGyv0uZRKhFzaWI3vjyRbVdz0Lza9nZUy9LkuQFjL5MpW/bIm0yaU3K7YwfwcoxCVlp7qD1vGbEg/OIX5sD1NLcmcrnyYmo2wqzqkr1ZM6ywLS0/Moxne8kmtXbNMnMx1nx3Te2jSC2gEBhXcUg7zamS6U+VuLyGo+adDUmXOLRl1y5s0tlnGzbQu9OyahX8jjH7fpJY8ML164FFdH1cpDKsEOPIiLtLsRhde1tPT/nFy5feLkMDlMW2dGibX8zB1KJqG78B+BU+rWzfrC62mMsFc7ZlLh3fZ+koYMG2cMUKWnj+eV/9XLqa5vt8qRQvmwcWKM+XY8HWwl4vGXotq+45Cr3X8m+QNNcI9/clf8pwyRqXkGOtcXLS/Iav6Yd9pM7lC9dinXYitLxAhTKxe7eRDOxESKeDo7Ar49mlcvKFaYH1s+3d/gfe10cf36pVK9G9zOrk7t2oZc8nH5/7VVdnHrwuneQlTKaGr1ZKAeUK13cV9rRfnkzGH9y0zMLGDRQLiIEB81tWQ++4Aws/+EEAQPOqbn2dtGn/fjNXMTuzJieDIWbSGZTooatvkoLNfq/GxoI5nwK+M2O+cRQcb+J/KCKBFiFChDmByIZWHTgWzIMKBD3EYzQCZzIqEPwutRcJ21Db+6Z2kS9daozl22lm9uWrV6NUIhbIQfhHjrgMusQKOjo7g6P94KCJIZLzOerttZ2dehUdQ47KLtbmYmouR4GrHJDtPOBzjYz4YVRIJN4DAGjuAc5br3fgsh99fcHa265pzF31sK1w/yVLgL/8S1rV0EDto48Ca9ZQ5y66iErjNA3uBn6uC14+/zy1+7WqPDlp+qNVPrViBXCWdqQz+25sDE795arJ7WJojEwGn/oULXLlkGe7qeNrTz/dvCDMvO+5x59cofbd76Z24ULd1RpfQ2DWNjAQ9CdJ+Fr8IqqsUd/TE3wR+/uDmQWAeeCajrUtprhMzM9GKmeECBEiBBAxtOoxpeMc4zYD0G0iEQ9EjMv8R5dRnQcyDto8lE2jiV3kHIm7fTtWasPv1BQxqD17zOBnJwoUi0l0s41DPlymg7K2lWVAr9dMrViMBwz/EraNzRV0K21oEnZRS24nJ02wPt+rAweAH94R17+jgpPty+Db2uJPPU4LXL6kWAzNEfWNedrmNbzXn3oTj+rCIRs3mn5zFERh3dmIc1jHli3UckzKzp3wJ8Rk9vb006buNxvq160DLqeQC7zrXdRywUxJXV0GWFH47bQSZZnc0U9ZJs89R5vaLz0FbfxCsZ3q1VdNf/mitOGsoasLhTUL5e3Arl3mmtm2PzJSXkBTHmrx4g7kexxVX2TMEeB81/hgbT09QJeeuWo2MIteTqXUFQC+AfKe3eJ53tcc+3wUVLHHA/Cs53nX2fuUdW/WejdD+I40l24F96pYLFg80VU7XzL0VCt5+FgNxO7dvvq5WjsKgCRef938BigXbGNjdNKeHkpSTqZSRq+QLxu/tfwW6w+/0NwW+L5isaA3T6qPYeV9qhk00+lgwUaXHX33bjnj1rm6v/T/0qVAV1rPJvySjvE7etTcYCvObXAwOENRKmXkTt7jOdomTMChLtTpq5TDw0aNZydCb68ZkNhA/9RTwObNtMwxZO97H7Vr1xqpIvPRXGV9d+7UpyWBxpMnP/EEcNWVdNypmJ5v4uqrSagB/jvk48gRKN3vxTqiP52u9+89vy7Dw+bdOnSo/BDxOLB8uX7HpK4qBRmvkyWHrNb3ns4Us8jQdGHYfwRwKagqzyal1F2e570g9lkKSq98t+d5h5VSrdMd9+TmjxEiRDi5wHEg0/1Nj7MA7PQ87zVd0fo2AFdZ+/wOgH/0PO8wAHiedwDT4KRgaE7Br1d4MMUIXcm+tholl232NjpqBrJMO5VxSU5MGJWKmdqqVfA8ylRgH4Ks7GJXUe7p6UB9j1bBJFOzswz0QeKZDGpqiCnKyHKGHQURi7nn1AwrCxP2TslBnI8hnQi2usoEs78fiMXovqXT1DY1Ad0UsYCuefQwjg7Tjd+xw9w/Dn1LpYD8m5rdveAPxsGETcbUVPDiW1oMo2OnwMiIoTj79lH7ve9Ru3Ur8F5K7vdrHKVSwRmehoZ8FZavnUn2nj3Apq3EzFglf2VkHpZeeSX986MfUcv0tlQKzBLctWIF0quoPCA/g95ecw6bqcViRotfznmbMpdYpsnYxSFdubkzxfExtGal1Gbx/826ZBhjHoA94v9eAGdbx1hGp1W/AqmlX/E8796wk54UAi1ChAhvE1Qv0Po9z1sXsr2a2okJAEsBXAAqRfaIUmq153mD9g/lD04K8H3yxAQUADAhTAd2ZRzXhBGlUuUBKZEITrDT1t5uDsLsavt2nKIdBUqRQfVFnWzQ32+OwQPw0BDQ00Pu/e5ldeZYHFlpU8uREeTbab/RUeVfi2elHkomZYdhuGxpLkznRHDZku1zuEqY87PYu9eYtrZupWvhONwDBwA235yuyx797mengDsouBlPPUWtnPAlLFSEUWnGFy6PxEZ7xsgI8IMf0LKuvILVq81z5xCR11/HM+t+p+x+8Hs1OmrMdR0UbYKhIWD/GnKmtLFNj1nn4GB51QzdtqyilyexeqnfPVYQ2CQmqwZxP1J6spuFixYFDZNyVjDeJpla0gQizwizm/rUC6Bb/O+qndgL4AnP8yYBvK6Uegkk4DZVOmhkQ4sQIUL1mD0b2iYAS5VSC5VSKQDXALjL2udOABcCgFKqGaSCvhZ20JOGofHo78oqAspLQ0mXt83QADmqlZ9DTlLNx0il4mjgQEk+2MGDxp6mbTTJJKWlbNtm4iu5HRw0jK+/n0bS+fPnoWmpdg/y0Ctrk+kOp1JJf5XLnsbXI1O7bJxoieVKjM1eL9N0bGaolGGPfL+zOkpg2TL6A4Brr9E//MdvGarDDObYMcNmXWEhlexrsnOlUvDmcMcyGcP8tBcTzz1n1jFDu+gin8jx6Rmy8MUebflpbDQzDl796U/Twje/Se3QkLkhfJ0jI/4L3XAKUdxTT2XPOpWzAwxDO3bMdM1kZ9X6eZtlNfrsiTl5m0xNmylm0cvpeV5RKfUHAO4D2ce+7Xne80qprwLY7HneXXrbZUqpF0AF0//Q87xDlY96kgi0yUnS0MIEmngXypi2a44Afo/tbeUJwdQSI6cvMM+CbXLSfGBahWCjbGpdkx8ixfbf/n4js1j9evNNYN48UoHatQOioEOtUqlgpRgZFsXag2teSfvaJGZSO3461dTuh4RdyoedMbW1piqtr14+/7x5kGygP/VU82O7Aqe8MTIbv5oZdfnBj42ZZe5Qba0ZhXQ+JoaG/GiRHTuolWE1tqOgUDDrfnIXPZirr9NhUj/4QfAhjYyYcBPOXR0fx7p1Z5TtxoLt6FF34cvMGsrPbWB9XmbC29Vvh4eDI/tMMIuBtZ7n3QPgHmvdn4llD8BN+q8qnBQCLUKECG8TRJkC02N8nAYuORi7arW76t+58hn5ntvqqAxJYEgWndRBt+mOotGteBTX8QcLe3qQW09WYdZYXn7ZhDZwZYX9+81gzFMxclB7oWDstHyasTEzyMrSOUC5XddK2QvgREM55D6u2Yqm+42rT/G4uUd4bRe17e3GMH/BBQCAVw7U+/eKma6cSYrjneMv6uDVV18t1/eBcirjinGxt9XUAJ/7HC1zbMkLL+DqBC2/cd0fAvBTNTExEXQUFItAVxctc57s3Ul62Bvf/36THyvZpl0zfmwMWf3AzzmHApn5Pr7+unmfZNom35t168ixkBwcDFaYlCEpIcUhjwtR6lOECBHmDKICj9Uhm6VRWNqieFCRLmxXYK1tJ/O84AzaMp8xjMGYog5NSLoqagDAzp1o6yQqdf75VNGgrs7YXNgFf/SoGV2ZTEjzDUcpsPFZDnx22Ia0afPoLOf2dFXsmNW5GKeBq7YX/8/plH78xgc/iO0v0YPZp81q/f0mmJSfNxOMLVuABx6g5fnziaqdfvpqLF6vc5J0fTu89pqxoDNbk5SeH64sxMlGUC7iuG2bf4MX3PUPAIDf02lU9429xyfrfCilTK4qt7/6FbXJ5AJczmlcnJIlnEH++7Rrl3+xnN60QTPXeFz5Pgze/fBhk23F78IZq1YZTYJ3lLMAMfWfDUQMbXokimNo6n8JTYUCsIT0siPDpFvxcxoYCAq7kZHABExlGQX2JLDTfeSyMGtrq54QllfwgxwY8KVWVr8o5566BK2t5FjgMKRdu4yDgPvL35mMK5Mzy7lq5wOUhsiCgQVhTY0zbc9Z25BhC0oXTuR9ddVOZPg1Lid1x3fuxOpBkl6r9YS9vaobz9AEXWUl/AF6rmzflimMvUvo+aw5/WIAQMOSJeU3Hyh/AHxj+CGUSn7m+dQGKqcUT6XM6MMCWHfo8uQe7Lv443wJAGgg5pz6c86hluXrffcBAA14l5+ub4jM9+SXc2DACFbp3gRw7oUXIpWi+8ZxkIcOGeHP69LpGqxmoczJp3ydxeLsTvsUCbQIESLMCUQ2tCqhFNERQcPqtS5W30jUZP78ep9NS3Ydxtpc8yLywDjdoMXPrblZMzVp+eaQDvYEjIxgsa7e0XUZBT+//LIZyVkN5d2PHjX94BANIBhuxeqoTNXjFEbPM2qofMdsNdQVt2aXq3H97kTgCiWpL2o6wcxhyxbDnDQL6rroIhzuoRr+e61Y8VIpOFlVsWjuJVcUWrBgAbov0RVUuBLIKzQ/K3bvNi+NnMRXq6jxf/lnWnfDDcAXv0jLXDfolFOoTSTQcS/t16GrQO7rU6jP0YOs16mW69eTOv2jHwF33637/X4y3l+6WvSNMTVlLobLJIkJh8/Q6mc+T46obdtMQgszteeeA2rOofdu4cqVtJLf0X373C/BiSISaBEiRJgTiBhadSgl0zjavBC1mUlDuayctPjwMJo0JWlaRKzt6EjcZ22ylWwNCIZ7AOXhIMx+bJuURGMjlUVOxmLG8CUYGtta0nrdKZ2dWLaMXPhsImGm1tdn+iu97S6Gw7AmSipzNrkcC3ZmgSv3s1g0DhQXazte2OzO82AoF9uHXn21vDgaAOzejebVy8v6Lefn5N2YkdbVGZsiZyVMTAAHB4kdFZaT8yDJSZe7dgXp8qFD5oVgm9tddwF//Me0fJOO5dyk0wbPPNN04GYqGtFx/fXAhL5ozTo/8AEq6vj448bGdccd1CY+uhQXrhRJswy+ccxiZbE0TcOW6xLf7Zed5sco83t16JDxO9RdREG6TTKOSHrWZoq57OVUShUA3AJgNShT/tMAXgJwO4AeALsAfJTrGVUCzw0wPJxEKkWCo65dC5CSpt8jIwEPUW0qhdouKtxXKNCXeeRIedI44E6VctXwl9kJtnDj/QuFBmQ7Lau99FiwajM0hHSePp6l2nK89CxyhY3GagJxa8eOBQUwf9SuOQhkcUuXmmgVLi1LWwrbXwq540VowjxvlB+XKKsk54mwjyWdIwAZ4tmr2ESvCerrjbzheSh8qbdiBSALegJkE2Bhyy/Iyy+bg3z1q9R+6UvUbtpEQg0wN+uWW4Drr6dl/aEne6ky6Mc+ttCfT4G1zH/9VyBxPTkKzlsVMnkze2sHB43qqwVb/am7cen69QCAV3pa/OPzbeUCuhddQlU04yMjRi+fKd4GDG2mvfsGgHs9z1sBYC2AFwF8GcCDnuctBfCg/j9ChAhzAbOXnP6W4IQZmlKqDsD5AD4FALrq5IRS6ipQ/SIAuBXAQwD+KLQTagoticPwmht8omMqCpN6l06nkdUDbrykqcvEhE9rajM0jOdySb+YoGQ/AJEoOx/UlRJYKpWnwklMTQH19eRKr+3StCKXC8YBDQ8blZS3aXaQzeXQrXWlbo7DaMoE4zWEl4DnopSqJ98rqaXb2RHy3bJVWlfcmkttnQ6hjgQr/AGTk+UTiQLA2FiAoXFfZX1CZmiFgmFmLURSkC6NAoPWzL6yY3wQLpTY1WXUTw4iPHDAZA1wWsdXvkLtn/yJmeeAUxcmJoh2AQDncOoLWZQ3hSDvv9+c5p+1/6H0SQpZec8aEa9jM7UDB8zD5fvX3++rz0t1jNzSc0/HwRLdEH7VuMZl1/nnn5TJ6W8VZtK7RQAOAvhnpdQzSqlblFI1ANo8z9sHALp11gFXSt2glNqslNp8kD/8CBEinNyYqwxN//Z0ADd6nvekUuobOA71UpfjvRkA1q1e7aGvD2pgAPVs99DxCaNFw0yYNSU0W0mlkiZyXttN4vCQzyu9nbbJYFS7fHb5LOmyf9TKCasBm/GQgSpfaEGS6QP3/9AhY8yzI2tlYp4MB7Et8mLEVno5qy8qm8uhic81n9jEaKahbA5IeWrpEAmbF9Q1ycx076fN0OT/rx6g9IjFfO2ZTNBTIRia3UfJ0HifmhqTdZGGmGXLjrKWsNlPImGqfXBCZm+vYWvcbtxI7U03AV//Oi2zna2z04SgsOVf29S2bzf95uDbvXuNPe2WW/Slf7wDl58lilnKPspUI35nDh0qzzIAgC1b0KJLCrUwI85oHjESM9c5U7wNUp9mIkp7AfR6nvek/v9HIAG3XynVAQC6nXZigwgRIrxNMFcZmud5fUqpPUqp5Z7nvQTgYgAv6L9PAviabn9a9UFHR02kqaYYWc18soUCpjJEtVzsKhYjViYrNnPLA2oiYdz88hiu9CmGzVbGx90FENNpOnBNO7Wqrg4Bg6C0r9nGvGIxPG+JwSwumSyPYwCQbWzEQp1rNH8+GZfYkSfzZPk6mfHa1xLGuFzvatj+TCIWc9Bod7exBzHGxqCGj+rjE/WSHllJqgCKmvEZnas4nt1Z10sBBPfv6TFsxpo4Z8+KS9F9naa/zMZSKdMRvqbvfhcAcOH11+Ohh+idZLYsq8NwFMkttwBDQ1S+/SMf3EArZRyOzdrl3IB8T/v7zfnZuMgz17S3G+PjTPE2sKHNlD/eCOB7uoTuawB+G8T6fqCU+gyA3QA+Mu1RPI90DPnw+Ktj+9rQEOL6wdT6amkdxidU2e5S27DvPSckyG3ynZTHCJMtYQZ3tnMnkzXItpIA9ifRlQLOtujLdACWNK7KlK64Bgm9Pa6P1a3vVWdnvW8o5nAnqaXJ2olhua8uNTRMoPHl+Td+1SpjeOcdx8b8rz6TqS3b5CooUFaEVT4oW/JlhNNGC/8pnZ1bLIqyUQl9L+UzsCaT7o4dxoOdnwAAXLzBn0TBhIPYkup738N//cpvAQCOHqV39Mc/NreBfQ4HDvgyEH191LdrrqHJj1oKhWDpXDniyOq0bFvgh8ttV9fsqZzA3BZonudtBeCa2eXimRw3QoQIJyHeAQxtdpBOA4sX0yjNI6Q9ffjEhOHuHI9RW4u0Zm1pzUQmkQwlODZcVS4kQ7NbCVe+pAw7ME4MCj1J5UkNzDY3mxGVKczhw8Zj4Up1cHXANtCGVMiMjxxFVwO9jPPmEXPs6zM2bVl1xla9pyv3HcbQ/N9+9rPUbt4cpL/j4z4Tr6tbUHYsz3OXWQ9A2m6Y1eh3YxRZDOrrdIW48IxqqVQtCgViiEy80mNH/FNwee5HUpTved7QnxsmJBNvAarOqJM5/+qvaAb3tjbg29+mzfwqL1tm3rtf/pJaVko2blyOpuEBpwAAIABJREFUMz9Axn4/FWD7dqNeygBcNtXwRfHNisVEDadZQCTQIkSIMFcwVTrBNJJfE04KgXb0WAwPPpbFqlXz0LFI2zPkVOVAedKlnIncCo1I5vN+CMVUlpgRD17T2YekXc0miHbAp4Q030gWYZds5u4PDSkkEsQE0vXU5js7zejO1y4jg+1rl7YU290vOyw8IzznqZog6tiRn0BHK9mWDg7Qtr17gzUCpd+iGvsaQ96LR5+l69ywbZsJk+BnOz7uR4Jy/TQ2p05OBlPBZD/i0thvMbTJBDloDu03lSmYGR07ZsyQ8vaxnZ1jaBsbKUp79WqgKUYZfBs2kBF/35L/io5/+C+0oyybwtemPTLJR4l6ff7zF/rXx5U4xsYAHR/r2+45O+uuu4C+PgpRWrKEynP3fPxcZAd0J9let2ePYYr8DvFLl8uZEi0zhOf9eguHnghOCoEG0Mv0xBNAMkkjwCmn0EvT2kptNjYerJsu9QYpAPVDZSdCXgg4O19SfhyyL1K4AeIDirtVH1t+hJXt8Twp3Kg9fFghlaKPPqfVnnwXearU8NFgYqorgE5ay+3ps0olHBslKTE5SYI+k0kjO0ZqbgvoxrSsqMPBIdpul+13ZSK4vKIusEBBKgV8nAol4u//3vRRf5A9egIkzv8fHQ06fKQGnpQ1lyxngAz/kw5mBueDcm3E2oE3TGK4fvCjK04DANx5J5DL0bt45ZW0S0fpTeCP/qj8WuzEe8CXkukn/gPXXEPFJNlPtGuXkTc81YKcU8B2kO/eDYyMzKNriVFb6gbq3kXbeaxowUG/Pw++zPP5fhozRSTQIkSIMCcQMbQqkUySqpFImBH0iSeoZcazZk3aL4tdpw228ZGjwdmHx8eDNeX1qBnP51Gr3feTGaLyLtYh8zvtMCD5v4ycqOZBy8oX9u+Gh4OsjllKJlOLujpibXztauRYcA5LGbPiYGrpNF0z+x4GBym8BAA6OnSIycBBtOjjtSyj2IKDw6S6SXXU5a8IK3/k39vP/h6SDz9Y3l/h8OnIkFpXKBAb6u8Pqr5yBrBsKsjQXHmv3F++HY2NwOmn03LyoZ/TwtatxiKvX4Is/hcA4NpMBvgIRSD97TeJtZ199jxsaNNFJD/0IWp//nPTH6ZeokxRWr8EF1xAMWdPPSUruVDbXUemhtWLU9g/RPeeVeXlS6ZMKW9t659avdZ37vB1/ukt9K309ACfadP67SwgEmgRIkSYM4gEWhXIpD2sXDSOw+1p3yjLHmkmIVzUDjC2hubmWtTqpL58o8jpsxMYR0QVBp5hR9vV6utyGD5GRgs5GUcl1iFrhcll25vNscJAOMtjlErByV3YzlwqmfAKTg7I52tQV0esKq+DNOMTo8H64yLVIRmb0segDoyNmUrNzLxaWlrQ1qxPrI1ouqAFWpZ1+oyBTU2uCiauoGTuxtatwJmPP07/3Hgjtbfeap6Zrt01bx4xmNdeCxa3lHY15IIMza5vJ58n96O9HUju1BUYeXKVAwfMQ+AwDE4tSSTISg/gi2M/AAA8Hv9/8Nd3UnntP7xJzPcJkHGMrfz8Mr/5JhnGANRqOrZs2Wo/m4J9QN1dmnrt2IE23fE2XSVk34EsOvgluO02AED84YfR/gd0L7k+5V/eqKOoAQzXbsRsQIbQnKw4KQRahAgRTn5ENrRqMTkJ9PWhIZdDw1Ianbq7yYDELuy9e80gzjGGExOm5lRzM+2fz7egUVfyVEN6yJOMTc5iDQDFIvJ6xEsmje3FFSVhQxZFYLYm7Wq87GJqNkuTTjGGjDu1vaKJRCB+FPl8FnV1xChy2h7jp/WIXB9Z0lpODQcQsThYQ8fo6aHqqvljmmHs3Ik2/eO2ZZROc3A467M1ZhjMLMfHzT3g+zM2Bn/GdHCJ7EOHjNtPTzq59nRiaE8+6WZozKbZXqYcDE22fC+Z3LS2AnhsF/3D9HRqyhguuZwHG7ZqawOJwOc++Bc492xKU7rx85cCAC6/nEpwL126EPPICYl84RVzHjZgalWke/ly9Pcn/fsFAAf76Wa1tLaaINrHHqNb1tZGQejyhvT2+tf6l9fqCrcva6NbUxPy7M6dBUQCrRqw6BcWep716ZQVPOtTsky4ASSn2D7KH+S6dSZPLp+nGKLGZmrTpdFy9RMoM6Sn9TkT+XRAhXQlrstoibAAapdgs3Ol5Tq250uhx0KCzz88bAQIG4wzGSncqK2pUbpN+nUVGamU+UZldXMOaeLvfN48mhth4YpG84HpG9+Sz6NlEQm3wxOkbh0QUfn2+JHJAFhH9fH9UIc//mNTT0dLx1O6jHOABSY/upER8/GXhW/oG1gMyXTgQaA+Y7ITyqYAs/NA+UY2NRnhxvv09PixYP+w6O8AAL/94y8AIJV2xQrabeNGUkubFvSZbP2y8uMtvAjAvMstixqDK1980RTIFJkiybt+TAusPnP+5sgIsKA8++JEETG0CBEizClEAq0axGLBqgLWhCj1qRROWUH6wvz5NELt2WNKDbPGsnu3CflgtzxHZ+dyWRQKRElqGzVtGhkJpAHEYzHkcuV0htmTnJ3JVRQxbOaoSpfOrZ2dIBlbGGuTzIUHdFkMESCNickGa1XJpDkeq2JTU4ZJsnbExTH6+5NYtIhmZ2pq1DRu927wtOcNuvZ5g47uHJ3f4AfUMqOKxYDJIrHGJOcYLlpkDOcNFK6BX/0KALB8+UbfUSQDZWWSAQAks4ahhRWt5OtEf7+huK56UXaCbyYjPTJmf+6vNvb/M4ipffq5L/gEkJ/Fx96/Lvhwha2BN/FzHR1TyPKPuT/j44bdcT927PAdBD4tZFrb3i4uemaInAIRIkSYU4gYWjWIxYKjiKuEgx6W6/VoVb8yg+5uk4MIkP2GbWg8it97L7WrVhnTQiZDLCGfrzHMJWWqCnJJ71RKlXVHdsuVVulKLayGrcn97RnUXRVB5DpXsUpumdUcPhy0r2Wzhq3xudNpYw9ncJyyDKzt6iK7z7JTWxB/7RWzA+Ab0bKFArrYIN3eaDrOneKH8aMfmdlDvqyruOtZzy+9zidrZZOe23XcvBpDXW3bWalknBJ+WmNfn6GgLuMm/0A+FNckNsyg+Ebqm/vtxltx07OfBAA89BBt6uzM4jxWG/Q9GI7X++Yx7i+z5lQKQZvv2rVG5eCE08FB4Jxzys7v13MrFMpLvs8AkQ2tWsTj5kFUqkXjqlczMYH6DL1w9SvoJevsVD7b5ue4QRcCLRbly0XtihXm+8rl6CXOZOIBr2PZS2Z1w1Vs1iWgXILNpXIyZLA/n5ftwalU8PsSaaz+B++rL8Ifwh9QLmduO3+XUg1ldZVxVKSUGjUUWLiQjN5dp2rhxUnT/f3GJsAHk+oa3zSp9rOHQwvFtlcexfz59ABfEc5C+zqlKuQSaCxvfIffi/vMDWGvTZh3xxVUJx8Yr/Nr+mfw9VOfBQDceMtaAJSQnkiQWt7VRe2Bl4wThm8Rm0/ie/eYQULPNDVc04Y9e2jVyt//fVo480xM6RJV8Z160oIvkHMCo6PAd77jvqYTQCTQIkSIMGcQCbQqUCoBoxNxPcgRPfAHSmYwoiy8gig4bxn063NA3TKiMczQeNAfHQWuuKJsdzz1lBloWRsYHy9nQmX9EcvSQO8yRNsPn88zNRUkArGY0XL43K5qQMw0xsdlrie1w8Omv3aNyNHRoDp69Khkp6a17dAc2iFj7Ph3e/ca2/rBecQ6VqyikhnZA2+YMAV+CHLKQqlj6Xo6+795OwCg7cu/Tds2bcJllxFDY1YoK5jLcBp7/gfJ0Nj/UFvUqRH79pWHawDlD4EhH6hNw+VcBS4nglYV+Hk+95x5Z047zezOh2CtoW2EHAzYsQN7Vr8XAPCfPk+rtm0zE1JNfU1nXPzt3yLONYg++EFqWYXftAnv/d+jsI0IESJEKEPk5awSwt7vZC7c8uAZjyu9Lo6YLlrIA6SCB1UiI2+DHpXr6mifoSHD2ngg3bjRjDqcgXDggLG72f2Q4JHX89wFIcOKQrqOy6N3YPKOUgkAHSydpY3ZbNxnVXzvMhnDruyK0K6JpiYnzf1g+3g2G7SrSVsdb+M+Dg+bY7C9jv/v6VmAhedoAzbTq97eYAXJeNxf57MTNnK/9BLOwNMAgObmM/zjM+GTNnN+P2wbWiwmbGdMb/bvNx4F/qFrblTeR5b4kDRFztAkEYv5xrCv/z5lP1z+ucWcqoq1ZFZDLmcY8PLMG7SwiyLIv3D/e3HLR2mVLvSB5+961c8pxVYdVb5kiSnqxi/xcgqvWfvHG/2MhdlAxNCqgBRoRmiV7+PyGsp1vH88rpBIlAu5uFZRG+pKyOeNcANIXdLhU7j4IqNTbX+eOsKOpEspswWZTFBQpVJBp5h0FMjr5G22euScKk4WabTUnXgshqy+wGwjfVT5vCpTHQHzseRywRqRx46Z75XjuSYnjWBiIzUfK5MJxq1JiEwcAHS+gQHSi5csOQUAUN/c7Kc3+bFngBFu+oM8+MHfBQC03HA18MADAIBLLiGB9q//ajRXWaDVHlTk81nMlZAf1gLt6FGjQ7PETqeDgok9oGNj5QF/fBP0SYd18UwZGzg2RusKBUpV+s53jEYo4yYvu4yWx1tJNVx9CbU7d/oZT746irGiuei6OtPy1FE8MmmBFottwLt08cef/QwzxmwKNKXUFQC+AbIz3eJ53tcq7PdhAD8EcKbneZvDjnlSCLQIESKc/JhNG5pSKg7gHwFcCpq0fJNS6i7P816w9qsF8H8AeDJ4lCBOCoEmS1KHqZx2iJArJ1Laac2cvDyrTxxJ0Ijb1Eg7jY4pMcrTfoUCsHohjXSru2mYP1QkGnfvvcaxwDFN4+PlmglA/4exMHtqxUTCUbZbUjpX7WuGZgnpTAbpOmZrSd3SLsPDhlWxET+Xc08wJedgAMyzkQ4Dafu2n4EsIGkn1ff0dGDhmZqecBzGm28aHZKLS+7aRH39zk9Q/6XfAQBcfgG90z+tOduPBpGT+No1LflW1dXBUG1OCJ6YCBry4/Fg7Azf96NHgy9gKuWzpGPH6GVjdrp3r+kbh9tdWHoQ/f00w+OnPkXrcjng3/+dlrk25Pr11P73/17+XACQ6s6hHNzefbeh4p+k2DfockM9fWJu1FnALDK0swDs9DzvNQBQSt0G4CrQROUS/xeA/xfAl6o56Mk9J1WECBFOGjBDq+YPQLNSarP4u8E63DwAe8T/vXqdD6XUaQC6Pc+ruuTuScHQSiUamKXH/HhtaHLwtIsRyMKJ6bS2r+n9s6kpZJrL7WoDA0AuRwakbF4zuiFy939swwiQIwvzv91LJ7rsMiCfJjrD5Wykfcqetcg16bks8BiY0UiulFULXXEhmm1w5RCet7SuLh3IFBgaKrclcr/t0kmy+pKcjBwoZ2i2w0DmXDJzOXYMGB5muxoFi2YbGvxcSD/5U4c81C85bKZFuvNOAMDFF5/tsxoOoh4bMyTFRmcngAe2l1+oUsEo/3TavDT8kGRZD9vrMTjoX2CbDnzdu5de4pERM2HJhUVNvX72MyQSF/vbud9s27r+emovuohaeg9pmSst4Vu/NMnKb2gnQn29URs0M+Mf/uSLj+KT/9PycM0Ax+Hl7Pc8zzUJOcM1H57/ZSilYgD+DsCnqj4jZsjQlFJfUEo9r5TarpT6vlIqo5RaqJR6Uin1ilLqdqWUo9JXhAgR3m44ToY2HXoBdIv/uwDsFf/XAlgN4CGl1C4A5wC4SykVJiRPnKEppeaBjHWrPM8bVUr9AMA1AK4E8Hee592mlPofAD4D4Fthx/I8Gv0lIZETP5tzUivDG1yszS6oyK20U/G6bDaOuA7zqNcMY3QiblhJinbM5amqQjKf90fo963gkboOv3yBaoZx7cL8+CHDAAo0Wo6O0QW4JkKfrqZawBDnsqvJG8F2IU3BkrkcmrS9h4tAyjAWZgKStdlhHrLffOpcrrx6B1AeyWCnKB06FLTNLVo0D03v0h3gemvsxty2DaOfpfLS2d8iF+GHLvo5/h3kdmaT2OCgKXxhp5p1tE4ZNsPMK5MxHWdq58prk/fYDttIJICXdKqRrkN2mnYpnnZWjZl88/77/U5yZpT0wF51FS0vpQwy/56l0yL/9pf/Rgv33mvqpnOds5UrzcVzH/kmvPoqbl3xCADgXzBzzKINbROApUqphQDeBMmO63ij53lHADTz/0qphwB86a32ciYAZJVSkwByAPYBuEh07FYAX0EVAk2GMgBu5wCvkypbmJCzVc6pqfJ3kY+V0kKL3+dsYhKpxvJsA/7I0+kkknWUmJ1mt3l/Py5s1pVCd9Eb+PTAQpxxuu6ojpbP6hNkczkgTx+wl6KPyhm2UXS8PdKbYE9YOTUVLBErpQwLN/2VNNXVoW4+qdZ8KYODwWvm9tgxo4HJHEo79k2q/CwoZfklPh4Lo2IR6OmhD7Jj2TLzYwDo70f2BYpDG//+HQCA9O99GhdeSAKNpyc4cABYSMVi/XP6Zou+PpPZzvcllzNxKdxhOemqzDPljnOfOOaiUDA3ggsr/sd/UPv66yanlb0ktbX4a/0lrI7r+QyWLMHPHkj63ZSX3tICXLhO9/vT/2L6eu21/vH8i2chxwfhB1oqBR0dJ4jZ9HJ6nldUSv0BgPtAYRvf9jzveaXUVwFs9jzvrhM57glfqed5byql/gbAbgCjAO4H8DSAQc/zmDIEDH0MbSS8AQDa2+efaDciRIjwa8RsxqF5nncPgHusdX9WYd8LqjnmTFTOBpCbdSGAQVDg23tdfXH93vO8mwHcDAArV67zWPeuptSOq6KFa6Zyx8RHPoFxpeixKpTNJhG3wjuODNHIPjoqVKaYnoG8eZ5ha7pKxBmxZ4DXaN1zIxRYycHc8e3P+n1Umi3FZZi/fRNc5TzkcCn1QFfZD6C8/I0oL81FFlt0DEqhUOOHG9izdssqFzKcwK5w4wrpkPU77dJGe6XlpIdYR8eiReY6dIfSvTog96ab8ImtFEj6i1Gahb2318wGZheyLFMXZdyJnbQqL4L3Z0ra2AhcR4rHS8M0Rg8MmPeuQwfHLt5L6h1+93eN2ix04NUfooBXnzH+9KdobDyz7NRc/urc1UeBm2+mf/hE3/wm9iyi2de7c9qBsnMnsGlT+XElZilfaa6nPl0C4HXP8w4CgFLqDgDrARSUUgnN0mxDX4QIEd7GmMupT7sBnKOUyoFUzosBbAbwSwAfBnAbgE8C+OlMO8mwSYfL7uRiahI2gZF2NUl+MhkywHAgLjsMhkfjvkmEQeYVYhYFzTDiAwf9EfqUkmZkfTT0Phdb6xuHa599lBYmJ40tx2XbsS9gYiI4U4hrQlHXTeLjSq+AZkHJQgFtmq01NtbKTRgYCLK24eHK83FSmAwty6KVTIyYBI2OmolVfEK5iM7dMn++OTBX7jj1VJoOCiYIdc8eY5tjExeTZshZj2RQrF1JU04PxRel7VTel/6QM7DK6spxFAgTo8WP6BiMqSlDtc4kBoa2NuAb36BlfgEGBvAuPWcMk9KWMR2e9d27jU1M50z96QPvEcSvCQCQTDahoYFmn2Kn1HklbcvbsSNYVeQEMaerbXie96RS6kcAtgAoAngGpEL+O4DblFJ/odf9U1UdqVAEMawoYqXtjJncfDNLEQkTrmabz04hmaR1/KFPTQVjturqWtCwSH9R/FLqH5ySGAKRV2BTmmKEWruBBbu1usIfrvRY2h+fK/vd5YZkCe8Kfjt6NGjRHxz0JUFSC7Y2LSEaG2v9j4nDxaQTgcO0+PuZmjLLsvu2I3Zy0uwnZ7ACgMyiJtR2WvPYbd/uB239zhN0zz73zHn+bxfOp2tubaXntKc/i27XPIM2SqWyWcAAAB/+MACK5rfncBgbM57Jy7f9NS385CfUDg2ZyrJsxN+40cx0xRfY24v8LoqRy7MtY8sWavv6fE/moz2kWj/9r+QsAModLuwT4NNvXUhq6Y03nOPH780G5qxAAwDP8/4cwJ9bq18DpTVEiBBhjmFOC7TZQixWOcrbzhio5liV1lViczxo8wApYeaVNOW50wliAM06w6C/PxgSduAAMJwjfau9k0bZZE7XWh4Y8OOizmwllWaycwEe3HkeAKBOR5ifCW3ofemlYGCeq9SN7ICryqGtW8dihm4whXKooVId7dBqVHNzrX/tkq3JVk4ILE0B9vOQ84XaTC2XA/JLyAivmP7u2kWVDgFMrad79p69JnLi9NPpvjTk6YHu3p1EN9fQ4SmkXOVQ5ETUOgHzl71EwQ4fDsbUzZ8PXN6rFZBvf5tariCSTJpUAVl2nGkp35AnnjDruKQG/279ehxuXwkA+LkmdoWCsUjYYTKAeYc5JOYb/yONjRs/prdeg5lgTqucESJEeGdhrns5Zw2JBNkFpPS3jfvS9OGqWlEtwkpfy2OGFWfkQNxkzNhq2LbEhAcwLINH9tZWMn40deWMFVz/ILnrFVx8OrGf/RMUZPqTx8iYnCyciY0rdcjCU09ROzgYvEmSsblsZ66yHwxZosSu7c0XUlfnOzqS2q7W0dyMZj0zvcu+xg4UO7RDwjXXiJhY3DdBdjCDGRryJ1+Jd1J7wQUd+PrXabO/f4ZufHt7g28Lw9/8DbXDw0HDrUwo1uUwXvuOOSUzT759H/gAgOu/U35S3tjWBnTrzB42dm3eDFyjWRLnp15wAXrbqM4bF3/0y63tBlo1G+RLP3gw6DPyPDcTBujRzUYdNEbE0KpAMl5CW91o+dRH+iWbKplp5PhjcEX+h9Xyn6nQk8eQNmMWbPGYh9ZW6ifLKRlTZ8duDTfXYH4PhbWrvVoFGhryP4q2PP3g6o1kVH5xZxK3PrpY943aT3x63KTUcOa3nAW50gXYyy6w9ZufBX85slQsS68DB5DUaiiro60rSSAPDCAQ0zY87M7YYtiFMsfHjfeyUCA1N9vZaVZq1bPm3R0kYGBs4L/3aRIkbaljeGWI1P6l7AKV5YB4IPA8f6LeH95hzAl8LXxK7mMmA/PAGU3kecRpp5nAQ5ZAjz8OfPazAICXWklV7uwECvp4559ffu3pxBRGJ8rLEj37rBln+P4dOVJehgowWm4iYYTdbCASaBEiRJgTiGxo1WJ0lFzxiUSg6F5c/x9PpZB0TvQqlgF4sXggFEtqXy5Gd7ywmUUiofz6/8zUYrFgSBirX3v2GLbW3U3G6nwmY+gMBzdp6/PK5gJ6PkrqKtcp/Na305iaeh8A4EMfonUdux4vnxpJHAOTk8E0CXvZhs1gjh0zNIWPf+iQoTE6zCOumVpLYyNaltDy4aG4/zM7+d1VCUk6DJh98O+yra0mTkGXHcq//hwKBSrzzdESP3+YVOdLTx9GT49mSTfdRO1f/IWhzkx5Wltx8AIq3v+atvFLNdqe8+GJJ4Crub41szGeC2HJElMH6Be/oLauzs/vXM5lfkZiJkyHL5RPNDGBrA6hWbaMpom6+27zSNm/MThoio2eQreAp/FEoVBuBpkpIoEWIUKEOYHIKVAtxsbI5y5taKly5oVMJpiLKPMf9TaVyfhMLsDoUil4epYoVzSDi8m54HYYqLLuNjdX3n942OQvMkmYP78JbV3aeMwGZqZ0/f3I6mH23DV04CVLany2dtttfPxzsWHDubRfj65RzbMA9YlazGwjk0ZIaU22g09dNc+5b0NDhi2x4YbtSo2Nfth+A7ftzRhtN+WL+B5UchrEYsaExwRmtJhEto3KNflMcedOrPzAan0M6i+T1Wd2N+G0LgqZGedKKZ/7HJXikf3+wAdwp76XHH3BDg4A/uxJzNq2bAFO/zuqACJInn9NtYM64p+Zd0eHuWimWfl8kPXyMxHUavmV5BVIpdr8kAwmdnV1hpl94hPUdg1uL99pFhCpnBEiRJhTiARaNZiaolEqLJxA1taWTM1O3cnlyutD8zr9v3Kxt/+/vW8Pjusq8/wdtbqlllut1ltyJFvy20ZxHMfEjjFJIC87lRCWybKEgcmQMCyPLOzM1oIzLFCbqlRlgJplWFggm/HyKCC8wmJCNpAHngwQkwQIjuPEiWMUR35EbmRZ1qP1PPvHd757vj73qtW2ZFnWnF+V6rZu3z73nHPvPff3vc1nZm/SoloMgRkfD6fPLi21MYVunOnYmH0pMxMYHAT6W4i5LFniWEBPnAi92evTaVxzNSmLli0jRrJnD/DEE3TYo49SzuZVq0jPdu07gapDJmcb5+nKZvNr2nEnJ1NCyklgU+XgoHXz4LakYy6zH1by1NQgaSaGt41tmVAZOOvQHHbAHRsT7XFu6lOngF27AABNHW8BYP1Tn3oKeKGcmNnqBOknR1vaMXHbhwDYqd29yzJnHgLHaK5aZbNb8zCXLbO/5Snla15fD1RyDTpZf9T1SwGik8wB+QnoDB1fu/Y6fOc7+U3IrDOBRZMtq1qHLbHTgF/QikE8TprcXC6ctFBmCeW7nRElospFzi1SmUpFL3ZCXAXIAMHGCJ2IB6cH8uOXowLhWSySefWCIGkDztArh9fTY+9nljTa243BIJEIVUWSAZDt5uFeuLUanB9xr5E4+EHbswdoaCC5ZMMG2m56O6CeNwe+ZsSjbNYaJXi+ZdFOtzCCLNnlPpj9/XaeWXbLZu2EiEUuZT7zFnW0EA6PqFB4ZWkpwpNbVxe4x9f3mCyyJtr7ujXH8MsDrea35DPX1mYXJnaJ6Omxc8/rO3f/ppvCbnlbt4bcCYP1u0W/ZsU9Nxrf/cxz6RYWlZ/NSnvDDcA3Ta5HmQRASN50/ivppVg2MBB2tDxDeJHTw8NjXsEvaMWgvJzyog8Ph6tTy1cmMwBZOJLfdJIVuA6hsny4ZGu8dZmcYHkBaxPuI+zsK8VSl61JMZS7wadsagobCqTPauCAaxjEkiX1YYPB4GBIDC1LD2L1IsPWFpLIwYztxRctYfipSU//6KNFYUu5AAAgAElEQVRASwsp0letou2azUDVmGFTTF2YhkjvUplyYrKaoTL7B1+fgYFwrGgUazPbskwmqFzF10RD2XMwM0+lbBtsDWA2OTCAtyyh4x95aXHQVb7c8iGVDBuw6Xg2VjwXxHeWX08sr67OJsbgW47FUux81DJd7qOcK56/iYmwsYvnLB4PGWGa+19GJkPxpTJmlk/FIvOLpkD8RR2r84M9pwFv5fTw8JhX8AytCEyUJdHf1oEFCwCVM+yL6Qlv+/qshpa30t4vvTRdxsDMTup0JCtjBsCK1MrKaNZmtjHzRi0x+jX51opy/XDrlqRSNjZPMjUmLK5lf3AQGFxCfWvnLIBdXSEHXIyNBZ+rzJguWkNa6paWeMDQuLBSVxdC+3btAmpqKHynqYm2QRKIDqC5yQyGmWJ3d358kByAzP4oi5S6ZZ/6+uw14DaEfs1lbaqiIqxcl1V2+FqxxUWk8V59ITG055+3DrgMGfLGp/zQu4xLys/+GIx5pbE27D+wWrBo2tZ2UkEXHDwYpu1yPniOJibClWSkUcbNznHgAD78YWJo27fTrsFB6znD08cq0VQqhqUBbZwevA6tSAwMUAJSkghJPEyleEvWqXQTVWMCkJ+nJqpEUZQrOkA3iLtvcDCs0R0aQqgqL29FmSMWR8limq94jdLrMsbH7TMnFzYZkC2HlMtJMZTOs2xZK5KuljqXy68KzPtAFZ5q19NTumgRtdHVZdPMyByU/JnFl+eMcTSZBCoq6LeVlWRdzGSag/DFhjbasotYavSEFVu5sePH7QD5RTMyYve5L6j+/nDenpoauwjIzJo26JO20oHNtN+SJHH6t/21QXO8sKXT9icmpNN6+R8/bl3zzSLzUt/qQNTkegbYYWTQgYH8QgoATS6vCDymkhIrKrsip3hBBdtsFtu2kk/dPZn6oFm+Zq6h+cgRW1x7JuAXNA8Pj3kBz9CKxOAguRVM5lYGsM6XXp9p4+2dydSjpo2+j+eMGCpTPESlenBFoNHRsJv62Fg4X7/cRgSCljr1NZUKHxZ1M/AbnpmaPJUkH65LR38/sGwZzUP9QjNZ3d3h6sCyMfNdvWGbdeuqAl8tls5ef91KkC5pGhkJ1+o8ejQvEAOAvHbVyGQo8wa7jCzaANSWGPmIfQw6O+1Jud988igVgoQcn2sg4lzVsZg9zpyntLQ2uAUWt4ybr2KB1MdhmPi1ibjo6bHy+W230fEPWiNAbN9z+RMTj1v/PO7XyZPWQCWdyJhRuioSmXBSShQmw8jdd18FAPjIR+ytzkyNr2tvr722MwG/oHl4eMwLeCtnkRgdpTdLSUk4hLOQv6y01HOurJqaStQtI8VvZcK86djtIJsNK64HBsK5t4eHw8nMCqGkBMocX1oaD8YUpeMF8nXFMoMD654Y0vvBJV6Dg9JjncbeviRldVU8Pvlmdxxf1eAg6s1k1r+BJnLxYhVKeRZFcCVpchmojFnlPjJzePFFIJVi1kYJLJe99Y2o7aeU5IHCjvVVuVyYFsisLJLSu46pbI1JpwOXCz6+psY6z/Lx6bTVNV0cNw7H7CLU1xcoHF/L1QfdCPTtnWagUg/LkI7KzNqk2wnf5K6hY3g4rEMbGQnu5y2bKNPIqlXtQfpxdtfg5B+SVc8EPEM7DUjFuDSKAfQ8utlsysrsvcCLXDIJVJGbEDIZunmqq8lLvLGlFfVrzWoiF7ko2Yrh1syTuej5uEQi2FdSHg/6GOWbBuS/6XicMnxFBrbzqaX7GXefH0hrCFZYutREF/CESOc27oAUsZ2qv5Xl5ahsoN+2mFAsaWh2dfYyp6QM6nCnSm65PX7Ou7qAujp6Ca19E22rD5nSf3v22AP5hz09dvFica283H6WIh5A18dRvDc12RoET/6e7pPOTuCd74TZ+WT+YEQeHr5d2tuB5gYzp8fMwsoXb3TURkdEFf+Vb2rzeXiEDC5l3EaUFV++JYy/3X33tQciMl9udsVbuDBcyOpM4XVoHh4e8wp+QSsCsVhQzzVvn4uo1Niuvri/374YJZMDmL2ZikDV5HbQ2NiMJmNyD0TUnh77FmQxTcbbuXLxJOWkogoYc1+j6gAzXPKxbJk9BXtB9PXlSyY8dhYvli0jsa55kUjpw19KSuVSYeH6wdERVWa8VZlyoImYyHgJMVFZ2SkqDJe/4630kpGuhsx6mDh3dFwEAFi9JW3d8WX6I6YikiUzZPgFkF841bCfRYvC5Puyha9aNw3WpLMe4PLLg++qaWpJiuU+MSPm4/v7rTMYs+CKCntTmuN1oixg2tzdCy4gkVb19YXzt+dytr3j5L5ReeAP2L6dEkDedRd9xVmjNmwIxxJPB35B8/DwmBfwRoEiEYtZNnK6vytUaZ3fJpIdmGJBeaUpWa+cThOVq6lpDlQhvOX+pVL2eAWiXBrK6oiE7tbVk0k9UqH04DJjEkAvc1Y+MxHp7Ax7pQwNhR0r29uJSS1degGSzCJkem7ugAxiLHTXms4xe0uVllI2EMCmQ09bP45R0PmZYJw6Zb3a2egwNhaQjZA6s/zydrSvN+fmcIahIct05KTyGHjipIuEqYPKDZflcljak81vt68vnDmEqzO1tAThAJyxKDYyBAwKHZvE4cNh5ZWsqGWuxciItR+wqi2oWlVXl2+84v67KZ+6uvAXW8jv5/4VxBBZh/arXwHvfjdmBOeDDm2S0rsWSqkdSqlupdResa9GKfWIUupls602+5VS6otKqQNKqT1KqfWTt+zh4XG+gTM8T/V3rlAMQ/s6gC8B+KbYtx3AY1rre5RS283/nwCwDcBy87cRwFfMtiASCUpvXMgFYKrydJKZuZW5ZTookYABADECN6mfLCrBbCxI01VnnWC5IEp1tdUB8vFRTFHmUXMTQha6EUZHrV6trY22FRX2Lcx6J5mQxGVqJ04A7e2k/GlZZJQq2WzYsjtZTnIg2mwpwZ0U2U7ihpFwbGlVdRqZDMWlyiQUzNBcNd/Bg0DTZsrtlWSXCOlKIyfareHGF+PgQatU4gnp6rIT56ZFAQJl1I8eoGv8jksBZerkBacsLbU3hnvDRvUrHg95IUvfWde/O7OsCklp8QToIru+M729gb/Gjh3E0K6+mr76/e8pb9tMYa4ztCkXNK31E0qpNmf3TQCuNJ+/AWAXaEG7CcA3tdYawG6lVEYp1ay1PlroHOk0cM01RL1dtxu5da9jlBO5XBSjwPcsS1+ZjHUo5/v70CHrFM77+H6S2V5Y2drUZBcadndqarL3Op+Lf6dU2OgRlSySMTxsj5c1C7jdV0wN4s5OK7a4/e7ttfuOL6HG2toaUZ0ZtgcA9FS5D6X0LXELlOZy9nt3EDIBp5XrkTKJ95csaQ36yC8V122np8caQrhQc6KpPpSFR4b18ph5vq/YvIxyJckTyJvELIB/vvk/4qGHaNeiJ/IP6+oCWs0qwSJhIhEX8fA0p0npU+Yu8HKRM/0oLY02bPE5l7eZQFPpuuJaVQYHA/1DZSf58d11F0VmbN8OfPvbmBHMC5FzEjTyImW2pjQELgDwmjiuy+wLQSn1AaXUM0qpZ7LZ42fYDQ8Pj9nEfBA5TwdRuX51xD5ore8FcC8AbFi0SKe+/iWkFiwIp+1hGpROAwvT+fsyGfQPE+ViZjKZpRsgpuOKeolESE+LTMbmhmd9sWRsrMyWyfT4JcxtNDSQ4yUALKVi50FWh5qasFSUSIRFZUZJST5J4rFwv1evpm1zsxVD2WOA5yWXswyG+3/4MNDWRo20tpKoUpUZDucvcj1ngXym5jrsRsVeSqZmOhI337e1tYeKE7GivKfHzjN3p7TUNi0ZmtvtwF1hYsIyHFkQwhzwmWNUW+Cv+4D3rjUOvTxJl3IoQAlGy4lRdr1oh8Ln4OuS5A+JREFmxnOQnBjA2NiCvLHzNTtyBEil6P5uZkvEwED4xh4ZsTe9oeHXrSeScP+6eg79nDbms5XzdRYllVLNALgKQxeAVnFcC4Aj0+mgh4fH3MFcFznPdEHbCeBWAPeY7U/E/juUUveDjAEnp9KfAaDX8R//GB3kKIM6mc4wvUmnbXENk5SrubHRFkc02/5xMt+fOBHWswwOWjYgHS2ZTckqTgB1wY1xlJmV+aXZ02PZEqtVmPUtWWLdMNjAUF1tQ7ZcnVtpaTR7Y9YmdeGXXmrPAdjYvmPHrJsHj12m+OKak4sXl2HhQtJV1bbo8GTxAKM65zI5qfGWEynTTwOoXZ9GSwtdP54z2UfOu8k/kz60ct75FHybcMYJ7N5tGxEJJP/u1Y8BsM6oqV0PAi+/TP8wLeQxbd2KOI8Fy4M+hoxA8mLwhZTafv7Miri9e1GzhOxmbsxld7dtt2ItzU/VwgjPZOluwo2YuNMvfrEeN9yAGcH5oEObckFTSn0XZACoU0p1AfgMaCH7vlLqdgCHAPx7c/hDAK4HcADAIID3FdWLigpg/Xq6KGx+ZHMd/x/hIY1YzN5A7HNUVYUg46Dx2k4Zup5qbUXrGrrLX8/Sgtndbe9x6YrlLmTcfCYT9heTz7mUttx0+jLX4e7d9JkXubY2uwixdMFO55mMXcPlOuIa1kZGwsWQtmyhbX+/XdzYJaunx0pi/Hx1dtrzL1xIGoSmJjLh1tVVorLBWeRktll+gDlUo6TEvqRkYLw7gakU1q6jAsmccFImJ2bfQW5CJqyVBlk2+PBCFiRd7FtrdQbmh5/Kfgx33027kg//mD7Iwgssp/HFo+yWAIA3rqUX5d7XqkI2D/SbDqXT1irEb0CZZYDv4T17UG8uan8defvLusOs8uBxXnJJK1Itoj4Dt+saa0wjldk/Yft20n1wicPpYCYXNKXUVgD/BCAG4D6t9T3O938H4P0AxgAcB3Cb1vrVQm0WY+W8ZZKvroo4VgP4yFRtenh4nH+YSYamlIoB+DKAa0CqqqeVUju11vvEYX8AsEFrPaiU+hCAzwL4D4XanRORAigvp/JE0i0gin44mSHQ329f5dIdnxmCWydyeDhor9HQoVxO5eUS5Gb5LemWqJR6cTfFkdwX5bIlh8b7ZGp+Dllk1sZMo6XFioS8bWgIZ24uKYkuvAQQmd2wgT5v2mTPyRknOHTxtdfsZ7d4bkMDUFtLrK2+3qZrqllBnYr1m8liytvdbZkIhwf094crH3V1IZkid4NrryV3AyZ9nZ32MDezukQ6DSwnSTBguiw1NjVVI27O+T9LSMz89KeB+M4f0QEvvEDbw4etyMkXhllWZ6ctdmoueEdDg/XXkdYJBvuQMc3q7w+XlRofJ3d+AO1raN7iK4krDA1ZdspZlU6dAt70JhpoNbcxNJRfowDIi2fddqVTz3YamEGGdimAA1rrgwBg1FQ3AQgWNK31L8XxuwG8Z6pG58aC5uHhMedxmlbOOqXUM+L/e41nAyPKxauQE/7tAP7fVCedGwtaSQm9kqOcEeXW1YzLauq8lfmiePa5jbKy4LfaeJhItYZ0a5ClKOV3MieY9GqISrPN3eTTy+/cfVJv5xodDh60REGWr2TbBzO5RYvyC34AVq8Ui4W9KTIZW3eSWV4uZxXzbgGVY8fCdpnKStu32lquV0nbpjXtqITwQQBIkcgDZFaRzQLPPgsAqDITv62DCooOXdmal2GEwePjsScH/xx899pgbdAsALQ+/g38tOlvAAD/6XLDIn/4kKWgTIOOHbOMkgfKCsWyMsu0mJKvWGFvFJcSS4UmK2CPCIM/SxapVKjCWYs5ruXtb8dPHicmzG44zz5ru3j55SsBAEs7Jix7dH1+xsbsBZwBnAZDy2qtNxT4vmgXL6XUewBsAHDFVCedGwuah4fHnMcMWzmLcvFSSl0N4JMArtBaD0/V6NxY0GIxeuXK1Mqugkq6bZh9oxOxyHRlQTaMQWEhBSwTRH4iVzcaZnQ0P0OHu3X9R6WRSV7wyS5+FIuTGWsZ7lSIoWPBAsu+ZIEMV78kmQyre9hVRCaIkLo2dtRlK6G8BMySZKZvNmQyo5S6w4ULiWFUMZ2USkh2pSgrsyyGg2LNQJJHXsFynpe0sI4axd6r3RSf+otf1Ab6QO73rU0/BwCcesetuPGgcZjdaayXr75qJ04G9nLnebIuvNCek28KjjUbGgJazTPplhTs7bU0llleZ6elV4x43P6W2R5P5MmTuOkNbwAAPHnJWwCQ8ZWJIrvcXHHFarx5rZkkG5dl+x24m0wfM7igPQ1guVKqHcBhAO8CkJcXRCl1MYCvAdiqte4ONxHG3FjQEgl64qTIaS7I+AQx06EhYJTjOkXYIUuabpEmaoIemFSKt0DCuSC1tfbasw63pcXeW/zg8vb1161+W3qmu+JcoRCQqYLqXVeAigr7nLMrR0uLjRuVW34Oq0udoM6RkWARODpBjTz6qPVOkGIot8fiHM+LTJ3ECniZBp2/K0sI1w7p3gHkP1z8FpL7eKCmEy90JgMRmHHJJUBLjsTExaX0VL/97RcE1+heo6k5tPU6AMCn1h61iwR3trLSLmSsqB8ettkbWRY3CwqOH7d+Nzymw4et6MhvB1mLgjvEE3nddYEBIHAPGRiw/eAbS958Zt9l6+icl/3V1fjG95N5TTz0ENC7mcTPGzcZC47rizRDmKkFTWs9ppS6A8DPQW4bO7TWzyul7gLwjNZ6J4DPAUgB+IGiDBOHtNZvK9Tu3FjQPDw85jxmOvRJa/0QyHdV7vu0+Hz16bY5Jxa00fESvD5YidFRK764Lg8ys4b0Dnf3yQl3U/8IiROV5ebNlcgBxkMfS0yxCpQFkgG7MEinWNdgEJUXcHw8nIWHoSNVnzbNEYuSsgAMi47MlhoarAGApblMRjh4jjgycG9voO1vNgN4b08PEDdK8nEzgJMAOg1LGjGyaZJYymjHliBpBTvp9vXZU3C/y8poIOXllSgvrzRjMc7NKeFM3EHb9swJ4Kmn6J/f/Ia2Dz8MAFidy2G16+XcmbaDNiJf/coE7riDIhyuv56+YqPGnV9sxrPP0vm//33aV7nvt9ZnRaYkueQS+vzxj9NXAzSW1OH91htaJoR0HWU5wkW6GTE7XL+e0l8A1mLx9NPWVYTbYLGjtxf4wx/oM99sExO4dROVdf9pDbGyV1+1bO1fTTX1N68TYuwMVUmZF5ECHh4eHgy/oBWBkRF66UW5P0g/wai8g+5xsranrN/J24CZsfI0m81PdgagrKICLYYBtKwnXU4mo4I2uX1+GScS4Zoq4+Nh9cVkzIxRDEOTiSY5jxsr+ZuagLIRo9Nh5iATqDG96zDUSCrwZJyW68ditM/xHV/DNjPQbUE8pqDEPF5tzjmsgP6Iajd8AX9t/pchbDxoWZ5O1igESP/FF1XQb2Z+bjq3khI7R5zscOHCjfjBveQaEpx7zRr8awdl3tj9edrFbW7evBKrLzU3mSwf72Zl5BtgdNTuk4kmjW7w9Q5ynt3TtxEJY3dgxs160LJjr1ofGr5fT50KbixWzZWWWtUjb18fpA+Nid58Z99pwDO0IiFl8yhXMyDf0hc1qbJmp6zyBNjQzoYGAEeMvMgyZFQ1pIqKUIHhxkaqF9nTY8VQXnhKSvKz4jJ4wePnN+qYQpDp/t3gd6lH5zWgbGLIysPumEpL81d2AKipwdBYPO/wXAIYM+sH6uxP+Txudqf42FBY8c//y/jbQplupYnXyeiKiopwv+VbhTEyAtVLCvRG81XjGlpIVqywhgXu949+BCzdQAaA669/b9CNEWMk4WvM91I2C5S+g8y/y9cYZf/Ro/nmch6z3Dp95Mnk69nZadcsDnDh655ILEY8TvedTKY7+pv8Uxw/bgsLf/CDtGUD8mN7W/Hkk9I7YnrwC5qHh8e8gV/QioBU1heqGxDl6yXbAOgF6IqaLJWovpNh9wFZnqkAZFr9qFKW3Jxb4Wmy/hZzLmZ0Unx1U1QD9u1dWZm0FZjcYo90AG1ZLEokUFJRndee9LNzp0V61fA5k8kkKiqIClc2kgycajMUIyrbpnTaYwqbTIZkak7c2dsrEq4It0tXMm1oiCPG53BcFirLynDJEqKbLS003lQK2LWLDuP2L7rIts9SIivbd++25/zoR0lcjO3ZY53B+AbgxsbH83US1Elg82YAwMNfp11PPmmjAGQ6KoCmyXXlkd/zMHt6gAcfpM/ssnJiH7m1XNW9C1ddQ8Gtn/oUpoX5nODRw8Pj3xi8Dq1IcKBAIS97maFCGgrc2pdSF+tGADQ0VCHZ5DQcj9u3Kh9YUWEVb0aR3iPiK53Qu7w6IfKchQqfRFWGl2OViNKhRVUgHxkB1q0jfUkl/4DH1tNjKQBvT51CmVFSX2DSeHR3Wzbgqr+AMEOTwR2BnieqYg2zlFQqoM7jqargnFm20ZiQRJlZg+dRFh1nZX1egsWo4iEM02Cj+eE739keKN/ZG0NGjfAQ2M/2z3+2niVcSOXG5cuDGNRgsrizJSVWkcUpUq69Fj99mCaJQy9PnIhmwkC0A3ZFhfVY4f739VlPEvYGKWsjN5XPfvYWfKz3K5gp+AWtCMTjdJEKlaqT30UtaFFJUmWdAYCofWUl3aG1tbRtXgHEev+cfyAQPHSjJtpA6nxdsU8uotIS61o1pVGg0GIXBTdP4tiY7Qe7LckatBs2UCGD+lXmEr/0Ur5lF6DxGk20MsaExqYmNLbRIs4GA7lIu7r78nJA5TgxQISsahaQ8YrK4NTdxj9M+vPx1PO5ZGU5PpdMsslWS36o1aFX7fiiohI4D5RZzFU2i40NNM6Nd9Ci/i+/igVJEF0XskTCNsFpnm58/8XAAw/kd5hX3aoqYBX5i+G22wAA3/v98kDM5a4ODuYXlJbbWMwu2Lw2ZjL5Ra+5r5wyifvGQQp///fAo2/9kJmED2O68Auah4fHvIAXOYtEidLWPyzYGVFhz9mnoUL57EZHLXNyLegDA5bNsLK3sxNIJkm8zGRoK6sy8SmZEUhmVahwsHssUFjMjIL8vcv2crmwJ8TwcH4WJQBYu5bM/iu3NNh0OUwPZMJByW7M650L+wa1JqV8meOYxZKQhr5/iAZ64gTQY5qVCmwpqgPUBbeEhCgbEYiXfA0WLQLiPcav4tkjtmFmZm6MZlSx1ooKS3WMC88Vi1owuIHSVXNGIWZosZjtRyCxpy9AJbfhbt/yFryymdxB7ruPdu3fb6UGSeiYfcmaEwAFLWzbZo7b9Rh92LNHRA2Ya7CiDbiLch9+7gvEqjki4siRmUm9zfALmoeHx7yAt3IWi7Exq9dxUcBHQwGImc+8LZO+BWmzbSBFxDhioXRA0jFe6pSlRwFgHSwbG629gJ0Zjxyx5IfjB7NZ2x4TBcmyCjnZylRIwNQpiWSpR37zM0lhXcq+RUksWkQu6Qs7aFtTA5SVjuc3AoQUZaNj1FnpnsJjO3UqzAr53END+ZETbvMyhbjrYsOK76YmoGzQyULxTE++tz53yL2QMhWL+ySWl+fnFjdbdm7layadp7m/fE/s2gXc+L735Q2+q5WKvXzrW8Ce/0ZfSd9b14E8kbDnYAZ6lanWsaXyj8Cnvkf/PP+8bYyvVVAMNAk8/jgA4L/efDMN5Y6bAABf+lJ+ctLpwjM0Dw+PeQGvQysWo6NWh+PO2FS6NNfzsEAa71hpKSoN66hMG/NRXQLjIDrGL/Tx8bDLh4SbP62mxuYHk4kSOHxGZngGSLfkWvmj3DyiYj+nciqWldUBS3xzOesDyv61FEEUM2NKBm1FTXkhuOFqXF+0vt5a6WRuNzeSKY7RcNZMnqBDOTsYWanGLTYizb6u+4Y0TTM1VipgZieupUJCO3ZYNsPkjXWt0keYXS5+9jPgs2liuxyHyWwrl7P6Munm45YpLSkJ55nkuWr964uw+CpHcnnlFYSq+sRi9iSGqd36bupQ73s2YscOzBj8glYMJibsBXFlMalJd58c+fTJO8S1FMjFzn1gSksDcTUlj6uIqOwr++si5SyspaUYHqGx8PMlRTEpsvF37nFSp13Ia1+GP7rJFqO+k+s8T2+UXxl/JxX2cor4O/4+6t2iJiLyQQUViSJCLVyZtr8/2r8sqrADd4CdxzgIf+1aHE+TGwsvRseOAf0c7no/ba++GrioglL5/HgvvaFkASseJyfZTCaBn1NS3OC4oLgxwvUoALvwXWGy4994o/XuYLB0+cQTQDpN8ufmv6Vt/aHfUXgBYAus9vXZsbOFwaQkuv32jdi/n3ZxMs8zhWdoHh4e8wreKFAMlMrPgS63DMnGeCvZnKQJhWQmtzpPsVcoqqim3CdzGhmUmf6Vmb5VS2pUZ8a7cAoKhXz3FIlgGiQLcj2NZcyqW949l8tPd+O2ETGmEJQKU7SoTk4lK7vlpFhDXlOD4VJiHcx0JGmTBG1SN5luoCSb3/yqVTb9UsuAoTAPP0xJGAEsW7Y8OA6g7glSD4BKBXz1q/T5zVtojv73fXTNDx2y03jXXbSN3/8tyzJ57HsA/E6kAAewxfxwS0kJ0MsZO01e9Ouvx8lllISyqtxcu85Oa/3hThqZObVA47LLqE9fmWbAwPnA0KbUliildiilupVSe8W+zymlXlRK7VFK/VgplRHf3amUOqCU2q+Uuu5sddzDw2P2wRE7U/2dKxTD0L4O4EsAvin2PQLgTlPo4B8A3AngE0qpNaDqLW8AsBDAo0qpFVrrwoE+Mu7N9T6NMgBM9v9U+4Hi0nnIz4UyTUqmFlUmfbI2ovpRoM9qYgLBrER57jKDijpn1DmkAkwqzwCiIi5TlIo4d5+0REQZY5zjR8dUyJWjEEmeGAyTapnEU1aqn2xKlbLD5LCiujogmTVpLjghWSYT6N/SxnDCxoFs1vaXtz091l2nro5Y0N/cYCxATz1lfXg+b1hZa6stvmJitnR50jJsVrqxJeLQobCy9f77UcUTwZ27/HIMt63MmyPeZl+y7ibTxfnA0KZc0LTWTyil2px9vxD/7gZws/l8Ewp2XjIAAAqXSURBVID7Tf28PymlDoBKvj9Z8CRyQZvquNNBodmP+q7QwjNV8KX7UBd8SiPOXazoKxclV0yPWEDy/i9kRZCLkNOuLokFXXSnSMasBms5byNS2ctuu5mFZRvywXQf0pGR6LXc1UTI7D3sR8gW3mSpiPhncTedDhp2izh3d4elc2k55jWxbjMFhdevW2fb5WSiGzfiySyJsod22jZgXlfl5SQD19XRduXGN6Jl3Cj+eeVkSy9gO/TggyjjbMsc3HrppTQvNQuwdClmDHN9QTvNFSISt8GWaI8q735B1I+UUh9QSj2jlHrmOL+ZPDw85jTmg8g5KZRSnwQwBuDbvCvisEiNstb6XgD3AsCGtWt1Xga700Gh2XMZXVQ+lqh9U8HNrBDFxQulDim2v5KNFfKhiGJXBcS/KMYVRDNMkJjH38tuny5xlZkypBtJrCT/lhifUCEXMunkH1Xz1IVMg+5K0YmE9e63VbEEfZSM1Fgekv1UgWnFCmJL2az9icxuzi42LF2yFJjZtBhxt9rSsWPo7iGGxul+crl8jyPZ/337gPZ2isV985tpW3XkBRsxwYOqqLATxx0xjK6+vBz17CsyTczr0Cel1K0AbgBwldaBGayo8u4eHh7nH+aFDi0KSqmtAD4B4Aqttciih50AvqOU+keQUWA5gKeKaPD09WMzBXle91UZhZm6osW4pUSFAEjGFbFPG5LsspnRUUA70Q+TVdmaLGJhsqFPVvG9vBxIljsKtrGJ4ECO0IgKw5T6smLsHFLx76oKy8utDi2o6j44En29mX4ZvdfiDjLgZ1fEQ3VnZGwma02YeVVXAx1LW/I73tmJyy+nj889R9tTpyb3ijl50kZ6cLTJpk2rcWFHQ/7J+vvDF0FOKHvWzgDO+wVNKfVdAFcCqFNKdQH4DMiqWQbgEVOifbfW+oOmlPv3AewDiaIfmdLCGfQkIhK7GJzO4jNTi2Yx7UxlnY3yt3Pd8CMiIlhclBJt4DY2HF58CiXIjHKfm0pSjjKUSskHsAuaGhkGBsPR2KMTNAYZzO5GN01lAXX7EY9HL2S85T7mya9R9xovBBynZga1Zs3KyJz/3Bz3n6XBl18GamvppM0cIzc6iuo9/wIAWL2aQgV+/Wv726iEoK47YV8f0L2OsiNceSVtY/uesytfodV/BnDeL2ha61sidv9zgePvBnD3dDrl4eEx9zBvRc4ZhxQ5z5Xo6aJQP4pIPhkZsRA1xigWZsTGPHZVIMX3VCyMt6fLxlyJLCp+tKJCpIweM1SjX8i2zg+Gx2KhlP9R5TujiEZUP6LsIW5AfHk5EJtwsn7KAHcJpkScR8g0mkylsGYNGexlbLgrhvLYurqAF1+kz5VvJCaVqrX+Jn/R8goA4MCBpUFIpjvmkpL82qy8dY0TW7ZciPrUn+gfNgrIPEVnanCLgF/QPDw85gXmtZVzRqHUjL5FisJ0GZi77zQYWlRs5mQsDMjXrUzFxiZzp5DiQjGBC3JIkmS5IZeU+ieiPJTzg+ExqzeLMgAUKqwe1Z8oH+GoDCMAEC8ZB3IFvHOjJoKPY6VYWRmaL6RBr1xZFfSf+8uO/Mykenutzp6TVl68tsH+wLT7/vcD99xDu5hxyYgEN3BmaMh2iZsaGAA2bqTU4cs7TFgAe/p2d+e7qEwTnqF5eHjMC3gdWrE42wxtNtiYgRa+xSEmFMGkGFFl76KYl/wuKgwp6EcB/ZrbvjsU18FTWjGTCXOSqEro/EOhvBoaieUdLl00pMGxEDMrpC+TejNXh2ZdNCLyrUlaGEVZ+TumTUePBuF5HevWAQB6e1WoyrzMQcmGRxsqGkN7XSbvXLUHn8aWLW8EYHOrSdWeGz8ai4Urp0v9aP9a0tddbLKGYN8+q1ebAfgFrVhMZQw4E2PBTC9kiF6wUKRyvVAVp0ILTlRbk/mLTWa1nyokRS4WrlKdxUs1OAD0Fqg+LNPSAhjKqVCFp6hivoXE3SgDQFQiS7mgBQtZodRJMkg0ajXlyZW1PXlhMOO78MKVgajpZjgeHg57gLz0ElBTQ8GkVRU2zc9N6f3mewow57BNKSlyFMT4uJ0bdvc4eTJsC8jlyPFuw4aLEQ/CI6YHz9A8PDzmFea6UUDpQsn7ZqsTSh0HMABgktJPs4o6+H5I+H7k43zux2Ktdf2ZnlAp9bA5bzHIaq23num5zhRzYkEDAKXUM1rrDb4fvh++H+dPP+Ya5ogXq4eHh8f04Rc0Dw+PeYO5tKDde647YOD7kQ/fj3z4fsxhzBkdmoeHh8d0MZcYmoeHh8e04Bc0Dw+PeYM5saAppbaaOp4HlFLbZ+mcrUqpXyqlXlBKPa+U+pjZX6OUekQp9bLZVs9Sf2JKqT8opR40/7crpX5r+vE9pVRiFvqQUUr90NRcfUEpddm5mA+l1N+aa7JXKfVdpVT5bM3HJHVoI+dAEb5o7ts9Sqn1Z7kfvh7uFDjnC5pSKgbgywC2AVgD4BZT3/NsYwzAf9FarwawCcBHzHm3A3hMa70cwGPm/9nAxwC8IP7/BwD/w/TjBIDbZ6EP/wTgYa31KgAXmf7M6nwopS4A8FEAG7TWHaAab+/C7M3H1wG4DqGTzcE2UJr55QA+AGCatcmn7McjADq01msBvATKHA2nHu5WAP/LPFf/9qC1Pqd/AC4D8HPx/52gIsaz3Y+fALgGwH4AzWZfM4D9s3DuFtCD8lYAD4KqZ2UBlEbN0VnqQxrAn2AMRWL/rM4HbCnEGlBo3oMArpvN+QDQBmDvVHMA4GsAbok67mz0w/nu3wH4tvmc98wA+DmAy87mdZqrf+ecoeE0anmeLZhCyhcD+C2ARq31UQAw24ZZ6MIXAHwcQT4O1ALo1Vpz5NxszMkSAMcB/B8j+t6nlFqAWZ4PrfVhAJ8HcAjAUQAnAfwOsz8fEpPNwbm8d8+oHu58x1xY0Iqu5XlWTq5UCsCPAPxnrXXfbJ1XnP8GAN1a69/J3RGHnu05KQWwHsBXtNYXg2JrZ0vcDmD0UzcBaAdVDlsAEu1czAV/o3Ny706nHu58x1xY0M5ZLU+lVBy0mH1ba/2A2f26UqrZfN8MoPssd+NNAN6mlOoEcD9I7PwCgIxSirOhzMacdAHo0lr/1vz/Q9ACN9vzcTWAP2mtj2utRwE8AGAzZn8+JCabg1m/d0U93L/URr48F/2Yq5gLC9rTAJYbK1YCpNzcebZPqqj+3j8DeEFr/Y/iq50AbjWfbwXp1s4atNZ3aq1btNZtoLE/rrX+SwC/BHDzLPbjGIDXlFIrza6rQOUIZ3U+QKLmJqVUhblG3I9ZnQ8Hk83BTgB/ZaydmwCcZNH0bEDUw32bDtfDfZdSqkwp1Y5i6+HOR5xrJZ55yVwPstq8AuCTs3TOLSBavgfAs+bvepD+6jEAL5ttzSzOw5UAHjSfl4BuygMAfgCgbBbOvw7AM2ZO/i+A6nMxHwD+O4AXAewF8C1QDdhZmQ8A3wXp7kZBzOf2yeYAJOp92dy3z4Ess2ezHwdAujK+X78qjv+k6cd+ANtm656da38+9MnDw2PeYC6InB4eHh4zAr+geXh4zBv4Bc3Dw2PewC9oHh4e8wZ+QfPw8Jg38Auah4fHvIFf0Dw8POYN/j+cF2T+I87c7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(u_pred,cmap='bwr');\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
