{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "# Manually making sure the numpy random seeds are \"the same\" on all devices\n",
    "np.random.seed(1234)\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "import scipy.io\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyDOE import lhs\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from scipy.interpolate import griddata\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to load dataset and bring to proper form for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(N_u=None, N_f=None):\n",
    "    \n",
    "    # Loading the dataset from .npy files\n",
    "    t = np.load('time.npy')\n",
    "    conc = np.load('Conc.npy')\n",
    "    x = np.arange(1,129)\n",
    "    y = np.arange(1,129)\n",
    "    \n",
    "    # Creating a meshgrid of all possible tuples of x, y, t\n",
    "    xv,yv,tv = np.meshgrid(x,y,t,indexing='ij')\n",
    "        \n",
    "    # Preparing the inputs x , y and t (meshed as xv, yv and tv) for predictions in one single array, as inp_star\n",
    "    inp_star = np.hstack((xv.flatten()[:,None], yv.flatten()[:,None],tv.flatten()[:,None]))\n",
    "    inp_star = inp_star[np.argsort(inp_star[:,2],kind='mergesort')]\n",
    "    inp_star[:,[0,1]] = inp_star[:,[1,0]]\n",
    "\n",
    "    # Preparing the testing conc_star\n",
    "    conc_star = conc.flatten('F')[:,None]\n",
    "    \n",
    "    idx = np.random.choice(inp_star.shape[0], N_u, replace=False)\n",
    "    inp_train = inp_star[idx,:]\n",
    "    conc_train = conc_star[idx,:]\n",
    "    \n",
    "    # For the case when N_f is not given as input\n",
    "    if N_f == None:\n",
    "        lb = X_star.min(axis=0)\n",
    "        ub = X_star.max(axis=0) \n",
    "        return x, t, X, T, Exact_u, inp_star, conc_star, inp_train, conc_train, ub, lb\n",
    "\n",
    "    # Domain bounds (lowerbounds upperbounds) [x, y, t], which are here ([1, 1, 0] and [128, 128, 5000])\n",
    "    lb = inp_star.min(axis=0)\n",
    "    ub = inp_star.max(axis=0)\n",
    "    # Getting the initial conditions (t=0)\n",
    "    initial_cond = inp_star[0:128*128,:]\n",
    "    conc_initial_cond = conc[:,:,0]\n",
    "    conc_initial_cond = conc_initial_cond.flatten('F')[:,None]\n",
    "    # Getting the all boundary conditions\n",
    "    top_bound = inp_star[inp_star[:,0]==1]\n",
    "    bottom_bound = inp_star[inp_star[:,0]==128]\n",
    "    left_bound = inp_star[inp_star[:,1]==1]\n",
    "    right_bound = inp_star[inp_star[:,1]==128]\n",
    "    boundary_ind = np.vstack((top_bound,bottom_bound,left_bound,right_bound))\n",
    "    \n",
    "    conc_boundary = []\n",
    "    for i in np.arange(boundary_ind.shape[0]):\n",
    "        conc_boundary.append(conc[boundary_ind[i,0]-1,boundary_ind[i,1]-1,boundary_ind[i,2]])\n",
    "    conc_boundary = np.asarray(conc_boundary)\n",
    "    conc_boundary = conc_boundary.flatten('F')[:,None]\n",
    "    \n",
    "    # Stacking them in multidimensional tensors for training (inp_train)\n",
    "    inp_train = np.vstack([initial_cond,boundary_ind])\n",
    "    conc_train = np.vstack([conc_initial_cond, conc_boundary])\n",
    "\n",
    "    # Generating the x, y and t collocation points for f, with each having a N_f size\n",
    "    # We pointwise add and multiply to spread the LHS over the 3D domain\n",
    "    inp_f_train = lb + (ub-lb)*lhs(3, N_f)\n",
    "\n",
    "    # Generating a uniform random sample from ints between 0, and the size of inp_train, of size N_u (initial data size) and without replacement (unique)\n",
    "    idx = np.random.choice(inp_train.shape[0], N_u, replace=False)\n",
    "    # Getting the corresponding inp_train (which is now scarce boundary and initial coordinates)\n",
    "    inp_train = inp_train[idx,:]\n",
    "    # Getting the corresponding conc_train\n",
    "    conc_train = conc_train[idx,:]\n",
    "\n",
    "    return x, y, t, xv, tv, yv, conc, inp_star, conc_star, inp_train, conc_train, inp_f_train, ub, lb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logger class implemented for error and loss analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, frequency=10):\n",
    "        print(\"TensorFlow version: {}\".format(tf.__version__))\n",
    "        print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
    "        print(\"GPU-accerelated: {}\".format(tf.test.is_gpu_available()))\n",
    "\n",
    "        self.start_time = time.time()\n",
    "        self.frequency = frequency\n",
    "\n",
    "    def __get_elapsed(self):\n",
    "        return datetime.fromtimestamp(time.time() - self.start_time).strftime(\"%M:%S\")\n",
    "\n",
    "    def __get_error_u(self):\n",
    "        return self.error_fn()\n",
    "\n",
    "    def set_error_fn(self, error_fn):\n",
    "        self.error_fn = error_fn\n",
    "  \n",
    "    def log_train_start(self, model):\n",
    "        print(\"\\nTraining started\")\n",
    "        print(\"================\")\n",
    "        self.model = model\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def log_train_epoch(self, epoch, loss, custom=\"\", is_iter=False):\n",
    "        if epoch % self.frequency == 0:\n",
    "            print(f\"{'nt_epoch' if is_iter else 'tf_epoch'} = {epoch:6d}  elapsed = {self.__get_elapsed()}  loss = {loss:.4e}  error = {self.__get_error_u():.4e}  \" + custom)\n",
    "\n",
    "    def log_train_opt(self, name):\n",
    "        # print(f\"tf_epoch =      0  elapsed = 00:00  loss = 2.7391e-01  error = 9.0843e-01\")\n",
    "        print(f\"—— Starting {name} optimization ——\")\n",
    "\n",
    "    def log_train_end(self, epoch, custom=\"\"):\n",
    "        print(\"==================\")\n",
    "        print(f\"Training finished (epoch {epoch}): duration = {self.__get_elapsed()}  error = {self.__get_error_u():.4e}  \" + custom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Functional dependencies like L-BFGS etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Time tracking functions\n",
    "global_time_list = []\n",
    "global_last_time = 0\n",
    "def reset_time():\n",
    "    global global_time_list, global_last_time\n",
    "    global_time_list = []\n",
    "    global_last_time = time.perf_counter()\n",
    "    \n",
    "def record_time():\n",
    "    global global_last_time, global_time_list\n",
    "    new_time = time.perf_counter()\n",
    "    global_time_list.append(new_time - global_last_time)\n",
    "    global_last_time = time.perf_counter()\n",
    "    #print(\"step: %.2f\"%(global_time_list[-1]*1000))\n",
    "\n",
    "def last_time():\n",
    "    \"\"\"Returns last interval records in millis.\"\"\"\n",
    "    global global_last_time, global_time_list\n",
    "    if global_time_list:\n",
    "        return 1000 * global_time_list[-1]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def dot(a, b):\n",
    "    \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
    "    return tf.reduce_sum(a*b)\n",
    "\n",
    "def verbose_func(s):\n",
    "    print(s)\n",
    "\n",
    "final_loss = None\n",
    "times = []\n",
    "def lbfgs(opfunc, x, config, state, do_verbose, log_fn):\n",
    "    \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\"\"\"\n",
    "\n",
    "    if config.maxIter == 0:\n",
    "        return\n",
    "\n",
    "    global final_loss, times\n",
    "\n",
    "    maxIter = config.maxIter\n",
    "    maxEval = config.maxEval or maxIter*1.25\n",
    "    tolFun = config.tolFun or 1e-5\n",
    "    tolX = config.tolX or 1e-19\n",
    "    nCorrection = config.nCorrection or 100\n",
    "    lineSearch = config.lineSearch\n",
    "    lineSearchOpts = config.lineSearchOptions\n",
    "    learningRate = config.learningRate or 1\n",
    "    isverbose = config.verbose or False\n",
    "\n",
    "    # verbose function\n",
    "    if isverbose:\n",
    "        verbose = verbose_func\n",
    "    else:\n",
    "        verbose = lambda x: None\n",
    "\n",
    "    # evaluate initial f(x) and df/dx\n",
    "    f, g = opfunc(x)\n",
    "\n",
    "    f_hist = [f]\n",
    "    currentFuncEval = 1\n",
    "    state.funcEval = state.funcEval + 1\n",
    "    p = g.shape[0]\n",
    "\n",
    "    # check optimality of initial point\n",
    "    tmp1 = tf.abs(g)\n",
    "    if tf.reduce_sum(tmp1) <= tolFun:\n",
    "        verbose(\"optimality condition below tolFun\")\n",
    "        return x, f_hist\n",
    "\n",
    "    # optimize for a max of maxIter iterations\n",
    "    nIter = 0\n",
    "    times = []\n",
    "    while nIter < maxIter:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # keep track of nb of iterations\n",
    "        nIter = nIter + 1\n",
    "        state.nIter = state.nIter + 1\n",
    "\n",
    "    ############################################################\n",
    "    ## compute gradient descent direction\n",
    "    ############################################################\n",
    "        if state.nIter == 1:\n",
    "            d = -g\n",
    "            old_dirs = []\n",
    "            old_stps = []\n",
    "            Hdiag = 1\n",
    "        else:\n",
    "            # do lbfgs update (update memory)\n",
    "            y = g - g_old\n",
    "            s = d*t\n",
    "            ys = dot(y, s)\n",
    "      \n",
    "            if ys > 1e-10:\n",
    "                # updating memory\n",
    "                if len(old_dirs) == nCorrection:\n",
    "                    # shift history by one (limited-memory)\n",
    "                    del old_dirs[0]\n",
    "                    del old_stps[0]\n",
    "\n",
    "                # store new direction/step\n",
    "                old_dirs.append(s)\n",
    "                old_stps.append(y)\n",
    "\n",
    "                # update scale of initial Hessian approximation\n",
    "                Hdiag = ys/dot(y, y)\n",
    "\n",
    "            # compute the approximate (L-BFGS) inverse Hessian \n",
    "            # multiplied by the gradient\n",
    "            k = len(old_dirs)\n",
    "\n",
    "            # need to be accessed element-by-element, so don't re-type tensor:\n",
    "            ro = [0]*nCorrection\n",
    "            for i in range(k):\n",
    "                ro[i] = 1/dot(old_stps[i], old_dirs[i])\n",
    "        \n",
    "\n",
    "            # iteration in L-BFGS loop collapsed to use just one buffer\n",
    "            # need to be accessed element-by-element, so don't re-type tensor:\n",
    "            al = [0]*nCorrection\n",
    "\n",
    "            q = -g\n",
    "            for i in range(k-1, -1, -1):\n",
    "                al[i] = dot(old_dirs[i], q) * ro[i]\n",
    "                q = q - al[i]*old_stps[i]\n",
    "\n",
    "            # multiply by initial Hessian\n",
    "            r = q*Hdiag\n",
    "            for i in range(k):\n",
    "                be_i = dot(old_stps[i], r) * ro[i]\n",
    "                r += (al[i]-be_i)*old_dirs[i]\n",
    "        \n",
    "            d = r\n",
    "        # final direction is in r/d (same object)\n",
    "\n",
    "        g_old = g\n",
    "        f_old = f\n",
    "    \n",
    "        ############################################################\n",
    "        ## compute step length\n",
    "        ############################################################\n",
    "        # directional derivative\n",
    "        gtd = dot(g, d)\n",
    "\n",
    "        # check that progress can be made along that direction\n",
    "        if gtd > -tolX:\n",
    "            verbose(\"Can not make progress along direction.\")\n",
    "            break\n",
    "\n",
    "        # reset initial guess for step size\n",
    "        if state.nIter == 1:\n",
    "            tmp1 = tf.abs(g)\n",
    "            t = min(1, 1/tf.reduce_sum(tmp1))\n",
    "        else:\n",
    "            t = learningRate\n",
    "\n",
    "\n",
    "        # optional line search: user function\n",
    "        lsFuncEval = 0\n",
    "        if lineSearch and isinstance(lineSearch) == types.FunctionType:\n",
    "            # perform line search, using user function\n",
    "            f,g,x,t,lsFuncEval = lineSearch(opfunc,x,t,d,f,g,gtd,lineSearchOpts)\n",
    "            f_hist.append(f)\n",
    "        else:\n",
    "            # no line search, simply move with fixed-step\n",
    "            x += t*d\n",
    "      \n",
    "            if nIter != maxIter:\n",
    "                # re-evaluate function only if not in last iteration\n",
    "                # the reason we do this: in a stochastic setting,\n",
    "                # no use to re-evaluate that function here\n",
    "                f, g = opfunc(x)\n",
    "                lsFuncEval = 1\n",
    "                f_hist.append(f)\n",
    "\n",
    "\n",
    "        # update func eval\n",
    "        currentFuncEval = currentFuncEval + lsFuncEval\n",
    "        state.funcEval = state.funcEval + lsFuncEval\n",
    "\n",
    "        ############################################################\n",
    "        ## check conditions\n",
    "        ############################################################\n",
    "        if nIter == maxIter:\n",
    "            break\n",
    "\n",
    "        if currentFuncEval >= maxEval:\n",
    "            # max nb of function evals\n",
    "            verbose('max nb of function evals')\n",
    "            break\n",
    "\n",
    "        tmp1 = tf.abs(g)\n",
    "        if tf.reduce_sum(tmp1) <=tolFun:\n",
    "            # check optimality\n",
    "            verbose('optimality condition below tolFun')\n",
    "            break\n",
    "    \n",
    "        tmp1 = tf.abs(d*t)\n",
    "        if tf.reduce_sum(tmp1) <= tolX:\n",
    "            # step size below tolX\n",
    "            verbose('step size below tolX')\n",
    "            break\n",
    "\n",
    "        if tf.abs(f-f_old) < tolX:\n",
    "            # function value changing less than tolX\n",
    "            verbose('function value changing less than tolX'+str(tf.abs(f-f_old)))\n",
    "            break\n",
    "\n",
    "        if do_verbose:\n",
    "            log_fn(nIter, f.numpy(), True)\n",
    "            #print(\"Step %3d loss %6.5f msec %6.3f\"%(nIter, f.numpy(), last_time()))\n",
    "            record_time()\n",
    "            times.append(last_time())\n",
    "\n",
    "        if nIter == maxIter - 1:\n",
    "            final_loss = f.numpy()\n",
    "\n",
    "    # save state\n",
    "    state.old_dirs = old_dirs\n",
    "    state.old_stps = old_stps\n",
    "    state.Hdiag = Hdiag\n",
    "    state.g_old = g_old\n",
    "    state.f_old = f_old\n",
    "    state.t = t\n",
    "    state.d = d\n",
    "\n",
    "    return x, f_hist, currentFuncEval\n",
    "\n",
    "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
    "class dummy(object):\n",
    "    pass\n",
    "\n",
    "class Struct(dummy):\n",
    "    def __getattribute__(self, key):\n",
    "        if key == '__dict__':\n",
    "            return super(dummy, self).__getattribute__('__dict__')\n",
    "        return self.__dict__.get(key, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN Architecture defined for 2-D Cahn-Hilliard equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulation\n",
    "\n",
    "### 1. Order parameter\n",
    "\n",
    "An order parameter  c  is defined as the concentration of B atom. The unit of  c  is defined as atomic fraction in this code.\n",
    "\n",
    "### 2. Total free energy\n",
    "\n",
    "The total Gibbs free energy of the system is defined by\n",
    "\n",
    "$ G = \\int_{V} \\left( g_{chem}(u) + g_{grad}(\\nabla u) \\right) dV $\n",
    " \n",
    "where  $g_{chem}$  and  $g_{grad}$ are the chemical free energy and the gradient energy densities, respectively. The chemical free energy density is formulated based on the regular solution approximation as:\n",
    "\n",
    "$ g_{chem} = RT\\left[ u\\ln u + (1-u)\\ln(1-u)\\right] + Lu(1-u) $\n",
    " \n",
    "where  $L$  is the atomic interaction parameter. The gradient energy density is expressed by:\n",
    "\n",
    "$ g_{grad} = \\frac{a_{c}}{2} \\left| \\nabla u \\right|^{2} $\n",
    " \n",
    "where  $a_{c}$  is the gradient energy coefficient. In this model,  $a_{c}$  is not related to any physical values.\n",
    "\n",
    "### 3. Time evolution equation\n",
    "\n",
    "The time evolution of the order parameter  $u$  is given by assuming that the total free energy of the system  $G$  decreases monotonically with time. For the conserved order parameter, the time evolution equation is derived from the Cahn-Hilliard equation given as:\n",
    "\n",
    "$ \\frac{\\partial u}{\\partial t} = \\nabla \\cdot \\left( M_{c} \\nabla \\frac{\\delta G}{\\delta u} \\right) =  \\nabla \\cdot \\left( M_{c} \\nabla \\mu \\right) $\n",
    " \n",
    "where  $\\mu$  is the diffusion potential of B atom. According to the total Gibbs free energy,  $\\mu$  is expressed by:\n",
    "\n",
    "$ \\mu = \\frac{\\delta G}{\\delta u} = RT\\left[ \\ln u - \\ln (1-u) \\right] + L(1-2u) - a_{c} \\nabla^{2}u $\n",
    " \n",
    "Here, remind that the functional derivative of $G$  is given by the Euler-Lagrange equation:\n",
    "\n",
    "$ \\frac{\\delta G}{\\delta u}=\\frac{\\partial g}{\\partial u}-\\nabla\\cdot\\frac{\\partial g}{\\partial (\\nabla u)} $\n",
    " \n",
    "$M$  is the diffusion mobility of B atom which is assumed to be given by:\n",
    "\n",
    "$ M_{c} = \\left[ \\frac{D_{A}}{RT}u + \\frac{D_{B}}{RT}(1-u)\\right]u(1-u) = \\frac{D_{A}}{RT}\\left[ u + \\frac{D_{B}}{D_{A}}(1-u)\\right]u(1-u) $\n",
    " \n",
    "Here,  $D_{A}$  and  $D_{B}$  are the diffusion coefficients of A and B atoms, respectively.\n",
    "\n",
    "Since the diffusion mobility  $M_{c}$  depends on  $u$ , in two-dimensional space, the time evolution equation can be written as:\n",
    "\n",
    "$ \\frac{\\partial u}{\\partial t} =  \\nabla \\cdot \\left( M_{c} \\nabla \\mu \\right) = M_{c} \\left(\\frac{\\partial^{2}\\mu}{\\partial x^{2}} +  \\frac{\\partial^{2}\\mu}{\\partial y^{2}}\\right) + \\frac{\\partial M_{c}}{\\partial u} \\left( \\frac{\\partial u}{\\partial x}\\frac{\\partial \\mu}{\\partial x} +  \\frac{\\partial u}{\\partial y}\\frac{\\partial \\mu}{\\partial y} \\right) $\n",
    " \n",
    "where the derivative of  $M_{c}$   with respect to  $ u $  is given by:\n",
    "\n",
    "$ \\frac{\\partial M_{c}}{\\partial u} = \\frac{D_{A}}{RT}\\left[ \\left(1-\\frac{D_{B}}{D_{A}}\\right)u(1-u) + \\left(u+ \\frac{D_{B}}{D_{A}}(1-u)\\right)(1-2u) \\right] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN(object):\n",
    "    def __init__(self, layers, optimizer, logger, inp_f, ub, lb, R, temp, L_a, a_c, D_a, D_b):\n",
    "        # Descriptive Keras model [3, 20, …, 20, 1]\n",
    "        self.u_model = tf.keras.Sequential()\n",
    "        self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
    "        self.u_model.add(tf.keras.layers.Lambda(lambda X: 2*(X - lb)/(ub - lb) - 1.0))\n",
    "        for width in layers[1:]:\n",
    "            if width != 1:\n",
    "                self.u_model.add(tf.keras.layers.Dense(width, activation=tf.nn.tanh,kernel_initializer='glorot_normal'))\n",
    "            else:\n",
    "                self.u_model.add(tf.keras.layers.Dense(width, activation=tf.nn.sigmoid,kernel_initializer='glorot_normal'))\n",
    "\n",
    "        # Computing the sizes of weights/biases for future decomposition\n",
    "        self.sizes_w = []\n",
    "        self.sizes_b = []\n",
    "        for i, width in enumerate(layers):\n",
    "            if i != 1:\n",
    "                self.sizes_w.append(int(width * layers[1]))\n",
    "                self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
    "\n",
    "        self.R = R\n",
    "        self.temp = temp\n",
    "        self.D_a = D_a\n",
    "        self.D_b = D_b\n",
    "        self.a_c = a_c\n",
    "        self.L_a = L_a\n",
    "        self.optimizer = optimizer\n",
    "        self.logger = logger\n",
    "\n",
    "        self.dtype = tf.float32\n",
    "\n",
    "        # Separating the collocation coordinates\n",
    "        self.x_f = tf.convert_to_tensor(inp_f[:, 0:1], dtype=self.dtype)\n",
    "        self.y_f = tf.convert_to_tensor(inp_f[:, 1:2], dtype=self.dtype)\n",
    "        self.t_f = tf.convert_to_tensor(inp_f[:, 2:3], dtype=self.dtype)\n",
    "    \n",
    "        # Defining custom loss\n",
    "    def __loss(self, u, u_pred):\n",
    "        f_pred = self.f_model()\n",
    "        return tf.reduce_mean(tf.square(u - u_pred)) + \\\n",
    "            tf.reduce_mean(tf.square(f_pred))\n",
    "\n",
    "    def __grad(self, X, u):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = self.__loss(u, self.u_model(X))\n",
    "        return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
    "\n",
    "    def __wrap_training_variables(self):\n",
    "        var = self.u_model.trainable_variables\n",
    "        return var\n",
    "\n",
    "    # The actual PINN\n",
    "    def f_model(self):\n",
    "        R = self.R\n",
    "        T = self.temp\n",
    "        D_a = self.D_a\n",
    "        D_b = self.D_b\n",
    "        a_c = self.a_c\n",
    "        L = self.L_a\n",
    "        # Using the new GradientTape paradigm of TF2.0,\n",
    "        # which keeps track of operations to get the gradient at runtime\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Watching the three inputs we’ll need later, x, y and t\n",
    "            tape.watch(self.x_f)\n",
    "            tape.watch(self.y_f)\n",
    "            tape.watch(self.t_f)\n",
    "            tape.watch(self.t_f)\n",
    "            # Packing together the inputs\n",
    "            X_f = tf.stack([self.x_f[:,0], self.y_f[:,0],self.t_f[:,0]], axis=1)\n",
    "            # Getting the prediction\n",
    "            u = self.u_model(X_f)\n",
    "            np.where(u!=0,u,0.001)\n",
    "            # Deriving INSIDE the tape (since we’ll need the x derivative of this later, u_xx)\n",
    "            u_x = tape.gradient(u, self.x_f)\n",
    "            u_y = tape.gradient(u, self.y_f)\n",
    "            u_xx = tape.gradient(u_x, self.x_f)\n",
    "            u_yy = tape.gradient(u_y, self.y_f)\n",
    "            mu = (R*T)*(np.log(u/(1-u))) + (L*(1-2*u)) - (a_c*(u_xx+u_yy))\n",
    "            mu_x = tape.gradient(mu, self.x_f)\n",
    "            mu_y = tape.gradient(mu, self.y_f)\n",
    "            M_c = (D_a/R/T)*(u + (D_b/D_a)*(1-u))*(u*(1-u))\n",
    "            \n",
    "            #tape.watch(mu)\n",
    "        # Getting the other derivatives\n",
    "        Mc_u = tape.gradient(M_c, u)\n",
    "        #Mc_u = (D_a/R/T)*((1-(D_b/D_a))*u*(1-u) + (u + (D_b/D_a)*(1-u))(1-2*u))\n",
    "        u_t = tape.gradient(u, self.t_f)\n",
    "        mu_xx = tape.gradient(mu_x, self.x_f)\n",
    "        mu_yy = tape.gradient(mu_y, self.y_f)\n",
    "\n",
    "        # Letting the tape go\n",
    "        del tape\n",
    "        \n",
    "        f_err = u_t - M_c*(mu_xx+mu_yy) + Mc_u*((u_x*mu_x)+(u_y*mu_y))\n",
    "        \n",
    "        # Buidling the PINNs\n",
    "        return f_err\n",
    "\n",
    "    def get_weights(self):\n",
    "        w = []\n",
    "        for layer in self.u_model.layers[1:]:\n",
    "            weights_biases = layer.get_weights()\n",
    "            weights = weights_biases[0].flatten()\n",
    "            biases = weights_biases[1]\n",
    "            w.extend(weights)\n",
    "            w.extend(biases)\n",
    "        return tf.convert_to_tensor(w, dtype=self.dtype)\n",
    "\n",
    "    def set_weights(self, w):\n",
    "        for i, layer in enumerate(self.u_model.layers[1:]):\n",
    "            start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
    "            end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
    "            weights = w[start_weights:end_weights]\n",
    "            w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
    "            weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
    "            biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
    "            weights_biases = [weights, biases]\n",
    "            layer.set_weights(weights_biases)\n",
    "\n",
    "    def summary(self):\n",
    "        return self.u_model.summary()\n",
    "\n",
    "    # The training function\n",
    "    def fit(self, X_u, u, tf_epochs=5000, nt_config=Struct()):\n",
    "        self.logger.log_train_start(self)\n",
    "\n",
    "        # Creating the tensors\n",
    "        X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
    "        u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
    "\n",
    "        self.logger.log_train_opt(\"Adam\")\n",
    "        for epoch in range(tf_epochs):\n",
    "            # Optimization step\n",
    "            loss_value, grads = self.__grad(X_u, u)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.__wrap_training_variables()))\n",
    "            self.logger.log_train_epoch(epoch, loss_value)\n",
    "    \n",
    "        self.logger.log_train_opt(\"LBFGS\")\n",
    "        def loss_and_flat_grad(w):\n",
    "            with tf.GradientTape() as tape:\n",
    "                self.set_weights(w)\n",
    "                loss_value = self.__loss(u, self.u_model(X_u))\n",
    "            grad = tape.gradient(loss_value, self.u_model.trainable_variables)\n",
    "            grad_flat = []\n",
    "            for g in grad:\n",
    "                grad_flat.append(tf.reshape(g, [-1]))\n",
    "            grad_flat =  tf.concat(grad_flat, 0)\n",
    "            return loss_value, grad_flat\n",
    "        # tfp.optimizer.lbfgs_minimize(\n",
    "        #   loss_and_flat_grad,\n",
    "        #   initial_position=self.get_weights(),\n",
    "        #   num_correction_pairs=nt_config.nCorrection,\n",
    "        #   max_iterations=nt_config.maxIter,\n",
    "        #   f_relative_tolerance=nt_config.tolFun,\n",
    "        #   tolerance=nt_config.tolFun,\n",
    "        #   parallel_iterations=6)\n",
    "        lbfgs(loss_and_flat_grad,self.get_weights(),nt_config, Struct(), True, lambda epoch, loss, is_iter:self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
    "\n",
    "        self.logger.log_train_end(tf_epochs + nt_config.maxIter)\n",
    "\n",
    "    def predict(self, X_star):\n",
    "        u_star = self.u_model(X_star)\n",
    "        f_star = self.f_model()\n",
    "        return u_star, f_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intializing hyperparameters and Cahn-Hilliard parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data size on the solution u\n",
    "N_u = 25000\n",
    "# Collocation points size, where we’ll check for f = 0\n",
    "N_f = 10000\n",
    "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
    "layers = [3, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
    "tf_epochs = 1000\n",
    "tf_optimizer = tf.keras.optimizers.Adam(\n",
    "  learning_rate=0.1,\n",
    "  beta_1=0.99,\n",
    "  epsilon=1e-1)\n",
    "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
    "nt_epochs = 2000\n",
    "nt_config = Struct()\n",
    "nt_config.learningRate = 0.8\n",
    "nt_config.maxIter = nt_epochs\n",
    "nt_config.nCorrection = 50\n",
    "nt_config.tolFun = 1.0 * np.finfo(float).eps\n",
    "\n",
    "R = 8.314 # gas constant\n",
    "temp = 673 # temperature [K]\n",
    "L_a = 20000.-9.*temp # Atom intaraction constant [J/mol]\n",
    "a_c = 3.0e-14 # gradient coefficient [Jm2/mol]\n",
    "D_a = 1.0e-04*np.exp(-300000.0/R/temp) # diffusion coefficient of A atom [m2/s]\n",
    "D_b = 2.0e-05*np.exp(-300000.0/R/temp) # diffusion coefficient of B atom [m2/s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running prep_data() to formulate dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, t, xv, tv, yv, conc, inp_star, conc_star, inp_train, conc_train, inp_f_train, ub, lb = prep_data(N_u, N_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.0.0\n",
      "Eager execution: True\n",
      "GPU-accerelated: True\n",
      "\n",
      "Training started\n",
      "================\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda (Lambda)              (None, 3)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 3,041\n",
      "Trainable params: 3,041\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "—— Starting Adam optimization ——\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "tf_epoch =      0  elapsed = 00:01  loss = 2.1040e-02  error = 2.4233e-01  \n",
      "tf_epoch =     10  elapsed = 00:12  loss = 1.7502e-02  error = 1.8230e-01  \n",
      "tf_epoch =     20  elapsed = 00:23  loss = 1.5649e-02  error = 1.2293e-01  \n",
      "tf_epoch =     30  elapsed = 00:33  loss = 1.5751e-02  error = 9.9134e-02  \n",
      "tf_epoch =     40  elapsed = 00:44  loss = 1.5883e-02  error = 9.6269e-02  \n",
      "tf_epoch =     50  elapsed = 00:55  loss = 1.5518e-02  error = 9.3532e-02  \n",
      "tf_epoch =     60  elapsed = 01:06  loss = 1.5155e-02  error = 8.9317e-02  \n",
      "tf_epoch =     70  elapsed = 01:17  loss = 1.5198e-02  error = 8.7443e-02  \n",
      "tf_epoch =     80  elapsed = 01:28  loss = 1.5516e-02  error = 8.7935e-02  \n",
      "tf_epoch =     90  elapsed = 01:39  loss = 1.5730e-02  error = 8.7981e-02  \n",
      "tf_epoch =    100  elapsed = 01:50  loss = 1.5640e-02  error = 8.6159e-02  \n",
      "tf_epoch =    110  elapsed = 02:01  loss = 1.5347e-02  error = 8.3607e-02  \n",
      "tf_epoch =    120  elapsed = 02:13  loss = 1.5077e-02  error = 8.2297e-02  \n",
      "tf_epoch =    130  elapsed = 02:24  loss = 1.4961e-02  error = 8.2833e-02  \n",
      "tf_epoch =    140  elapsed = 02:35  loss = 1.4972e-02  error = 8.3981e-02  \n",
      "tf_epoch =    150  elapsed = 02:47  loss = 1.5007e-02  error = 8.4035e-02  \n",
      "tf_epoch =    160  elapsed = 02:58  loss = 1.4989e-02  error = 8.2161e-02  \n",
      "tf_epoch =    170  elapsed = 03:10  loss = 1.4914e-02  error = 7.8759e-02  \n",
      "tf_epoch =    180  elapsed = 03:21  loss = 1.4830e-02  error = 7.5065e-02  \n",
      "tf_epoch =    190  elapsed = 03:33  loss = 1.4785e-02  error = 7.2382e-02  \n",
      "tf_epoch =    200  elapsed = 03:44  loss = 1.4790e-02  error = 7.1363e-02  \n",
      "tf_epoch =    210  elapsed = 03:55  loss = 1.4820e-02  error = 7.1813e-02  \n",
      "tf_epoch =    220  elapsed = 04:06  loss = 1.4846e-02  error = 7.3079e-02  \n",
      "tf_epoch =    230  elapsed = 04:16  loss = 1.4853e-02  error = 7.4548e-02  \n",
      "tf_epoch =    240  elapsed = 04:27  loss = 1.4842e-02  error = 7.5835e-02  \n",
      "tf_epoch =    250  elapsed = 04:38  loss = 1.4823e-02  error = 7.6733e-02  \n",
      "tf_epoch =    260  elapsed = 04:49  loss = 1.4804e-02  error = 7.7150e-02  \n",
      "tf_epoch =    270  elapsed = 05:00  loss = 1.4787e-02  error = 7.7095e-02  \n",
      "tf_epoch =    280  elapsed = 05:11  loss = 1.4773e-02  error = 7.6686e-02  \n",
      "tf_epoch =    290  elapsed = 05:21  loss = 1.4765e-02  error = 7.6084e-02  \n",
      "tf_epoch =    300  elapsed = 05:32  loss = 1.4763e-02  error = 7.5408e-02  \n",
      "tf_epoch =    310  elapsed = 05:43  loss = 1.4763e-02  error = 7.4682e-02  \n",
      "tf_epoch =    320  elapsed = 05:55  loss = 1.4758e-02  error = 7.3880e-02  \n",
      "tf_epoch =    330  elapsed = 06:06  loss = 1.4743e-02  error = 7.3022e-02  \n",
      "tf_epoch =    340  elapsed = 06:17  loss = 1.4721e-02  error = 7.2232e-02  \n",
      "tf_epoch =    350  elapsed = 06:27  loss = 1.4702e-02  error = 7.1675e-02  \n",
      "tf_epoch =    360  elapsed = 06:38  loss = 1.4694e-02  error = 7.1428e-02  \n",
      "tf_epoch =    370  elapsed = 06:50  loss = 1.4696e-02  error = 7.1390e-02  \n",
      "tf_epoch =    380  elapsed = 07:01  loss = 1.4701e-02  error = 7.1356e-02  \n",
      "tf_epoch =    390  elapsed = 07:11  loss = 1.4700e-02  error = 7.1184e-02  \n",
      "tf_epoch =    400  elapsed = 07:22  loss = 1.4691e-02  error = 7.0917e-02  \n",
      "tf_epoch =    410  elapsed = 07:32  loss = 1.4679e-02  error = 7.0737e-02  \n",
      "tf_epoch =    420  elapsed = 07:43  loss = 1.4670e-02  error = 7.0807e-02  \n",
      "tf_epoch =    430  elapsed = 07:54  loss = 1.4668e-02  error = 7.1135e-02  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_epoch =    440  elapsed = 08:05  loss = 1.4670e-02  error = 7.1590e-02  \n",
      "tf_epoch =    450  elapsed = 08:15  loss = 1.4671e-02  error = 7.2013e-02  \n",
      "tf_epoch =    460  elapsed = 08:26  loss = 1.4670e-02  error = 7.2307e-02  \n",
      "tf_epoch =    470  elapsed = 08:37  loss = 1.4666e-02  error = 7.2444e-02  \n",
      "tf_epoch =    480  elapsed = 08:47  loss = 1.4662e-02  error = 7.2427e-02  \n",
      "tf_epoch =    490  elapsed = 08:59  loss = 1.4658e-02  error = 7.2266e-02  \n",
      "tf_epoch =    500  elapsed = 09:10  loss = 1.4655e-02  error = 7.1990e-02  \n",
      "tf_epoch =    510  elapsed = 09:20  loss = 1.4653e-02  error = 7.1655e-02  \n",
      "tf_epoch =    520  elapsed = 09:31  loss = 1.4652e-02  error = 7.1335e-02  \n",
      "tf_epoch =    530  elapsed = 09:42  loss = 1.4651e-02  error = 7.1095e-02  \n",
      "tf_epoch =    540  elapsed = 09:53  loss = 1.4649e-02  error = 7.0981e-02  \n",
      "tf_epoch =    550  elapsed = 10:04  loss = 1.4645e-02  error = 7.1026e-02  \n",
      "tf_epoch =    560  elapsed = 10:14  loss = 1.4640e-02  error = 7.1235e-02  \n",
      "tf_epoch =    570  elapsed = 10:25  loss = 1.4637e-02  error = 7.1565e-02  \n",
      "tf_epoch =    580  elapsed = 10:36  loss = 1.4635e-02  error = 7.1916e-02  \n",
      "tf_epoch =    590  elapsed = 10:47  loss = 1.4634e-02  error = 7.2175e-02  \n",
      "tf_epoch =    600  elapsed = 10:58  loss = 1.4632e-02  error = 7.2268e-02  \n",
      "tf_epoch =    610  elapsed = 11:08  loss = 1.4628e-02  error = 7.2190e-02  \n",
      "tf_epoch =    620  elapsed = 11:19  loss = 1.4625e-02  error = 7.1984e-02  \n",
      "tf_epoch =    630  elapsed = 11:30  loss = 1.4623e-02  error = 7.1724e-02  \n",
      "tf_epoch =    640  elapsed = 11:41  loss = 1.4621e-02  error = 7.1485e-02  \n",
      "tf_epoch =    650  elapsed = 11:52  loss = 1.4618e-02  error = 7.1336e-02  \n",
      "tf_epoch =    660  elapsed = 12:02  loss = 1.4616e-02  error = 7.1304e-02  \n",
      "tf_epoch =    670  elapsed = 12:13  loss = 1.4615e-02  error = 7.1369e-02  \n",
      "tf_epoch =    680  elapsed = 12:24  loss = 1.4613e-02  error = 7.1471e-02  \n",
      "tf_epoch =    690  elapsed = 12:35  loss = 1.4610e-02  error = 7.1559e-02  \n",
      "tf_epoch =    700  elapsed = 12:45  loss = 1.4608e-02  error = 7.1621e-02  \n",
      "tf_epoch =    710  elapsed = 12:56  loss = 1.4606e-02  error = 7.1673e-02  \n",
      "tf_epoch =    720  elapsed = 13:07  loss = 1.4604e-02  error = 7.1730e-02  \n",
      "tf_epoch =    730  elapsed = 13:18  loss = 1.4603e-02  error = 7.1792e-02  \n",
      "tf_epoch =    740  elapsed = 13:28  loss = 1.4601e-02  error = 7.1845e-02  \n",
      "tf_epoch =    750  elapsed = 13:39  loss = 1.4599e-02  error = 7.1871e-02  \n",
      "tf_epoch =    760  elapsed = 13:50  loss = 1.4597e-02  error = 7.1856e-02  \n",
      "tf_epoch =    770  elapsed = 14:01  loss = 1.4595e-02  error = 7.1793e-02  \n",
      "tf_epoch =    780  elapsed = 14:11  loss = 1.4593e-02  error = 7.1688e-02  \n",
      "tf_epoch =    790  elapsed = 14:22  loss = 1.4592e-02  error = 7.1565e-02  \n",
      "tf_epoch =    800  elapsed = 14:33  loss = 1.4590e-02  error = 7.1466e-02  \n",
      "tf_epoch =    810  elapsed = 14:44  loss = 1.4588e-02  error = 7.1427e-02  \n",
      "tf_epoch =    820  elapsed = 14:55  loss = 1.4586e-02  error = 7.1463e-02  \n",
      "tf_epoch =    830  elapsed = 15:05  loss = 1.4584e-02  error = 7.1559e-02  \n",
      "tf_epoch =    840  elapsed = 15:16  loss = 1.4582e-02  error = 7.1676e-02  \n",
      "tf_epoch =    850  elapsed = 15:28  loss = 1.4581e-02  error = 7.1767e-02  \n",
      "tf_epoch =    860  elapsed = 15:38  loss = 1.4579e-02  error = 7.1793e-02  \n",
      "tf_epoch =    870  elapsed = 15:49  loss = 1.4578e-02  error = 7.1747e-02  \n",
      "tf_epoch =    880  elapsed = 16:00  loss = 1.4576e-02  error = 7.1648e-02  \n",
      "tf_epoch =    890  elapsed = 16:11  loss = 1.4574e-02  error = 7.1535e-02  \n",
      "tf_epoch =    900  elapsed = 16:21  loss = 1.4572e-02  error = 7.1444e-02  \n",
      "tf_epoch =    910  elapsed = 16:32  loss = 1.4571e-02  error = 7.1398e-02  \n",
      "tf_epoch =    920  elapsed = 16:42  loss = 1.4569e-02  error = 7.1397e-02  \n",
      "tf_epoch =    930  elapsed = 16:53  loss = 1.4568e-02  error = 7.1427e-02  \n",
      "tf_epoch =    940  elapsed = 17:04  loss = 1.4566e-02  error = 7.1462e-02  \n",
      "tf_epoch =    950  elapsed = 17:14  loss = 1.4564e-02  error = 7.1483e-02  \n",
      "tf_epoch =    960  elapsed = 17:25  loss = 1.4563e-02  error = 7.1482e-02  \n",
      "tf_epoch =    970  elapsed = 17:35  loss = 1.4561e-02  error = 7.1465e-02  \n",
      "tf_epoch =    980  elapsed = 17:46  loss = 1.4559e-02  error = 7.1441e-02  \n",
      "tf_epoch =    990  elapsed = 17:58  loss = 1.4558e-02  error = 7.1419e-02  \n",
      "—— Starting LBFGS optimization ——\n",
      "nt_epoch =     10  elapsed = 18:20  loss = 1.4439e-02  error = 7.1369e-02  \n",
      "nt_epoch =     20  elapsed = 18:32  loss = 1.4367e-02  error = 7.7577e-02  \n",
      "nt_epoch =     30  elapsed = 18:44  loss = 1.4303e-02  error = 7.3419e-02  \n",
      "nt_epoch =     40  elapsed = 18:55  loss = 1.4242e-02  error = 7.4072e-02  \n",
      "nt_epoch =     50  elapsed = 19:07  loss = 1.4099e-02  error = 8.1137e-02  \n",
      "nt_epoch =     60  elapsed = 19:18  loss = 1.3834e-02  error = 7.6204e-02  \n",
      "nt_epoch =     70  elapsed = 19:30  loss = 1.3653e-02  error = 8.3226e-02  \n",
      "nt_epoch =     80  elapsed = 19:43  loss = 1.3431e-02  error = 8.4088e-02  \n",
      "nt_epoch =     90  elapsed = 19:55  loss = 1.3198e-02  error = 8.0691e-02  \n",
      "nt_epoch =    100  elapsed = 20:06  loss = 1.2912e-02  error = 7.3016e-02  \n",
      "nt_epoch =    110  elapsed = 20:17  loss = 1.2699e-02  error = 7.4595e-02  \n",
      "nt_epoch =    120  elapsed = 20:29  loss = 1.2430e-02  error = 7.7440e-02  \n",
      "nt_epoch =    130  elapsed = 20:40  loss = 1.2304e-02  error = 7.2832e-02  \n",
      "nt_epoch =    140  elapsed = 20:51  loss = 1.2204e-02  error = 7.3203e-02  \n",
      "nt_epoch =    150  elapsed = 21:02  loss = 1.2088e-02  error = 7.0941e-02  \n",
      "nt_epoch =    160  elapsed = 21:13  loss = 1.1998e-02  error = 7.1362e-02  \n",
      "nt_epoch =    170  elapsed = 21:24  loss = 1.1873e-02  error = 7.1822e-02  \n",
      "nt_epoch =    180  elapsed = 21:35  loss = 1.1770e-02  error = 7.1424e-02  \n",
      "nt_epoch =    190  elapsed = 21:46  loss = 1.1641e-02  error = 7.1005e-02  \n",
      "nt_epoch =    200  elapsed = 21:57  loss = 1.1556e-02  error = 7.0528e-02  \n",
      "nt_epoch =    210  elapsed = 22:07  loss = 1.1460e-02  error = 7.1285e-02  \n",
      "nt_epoch =    220  elapsed = 22:18  loss = 1.1363e-02  error = 7.4249e-02  \n",
      "nt_epoch =    230  elapsed = 22:29  loss = 1.1273e-02  error = 7.5856e-02  \n",
      "nt_epoch =    240  elapsed = 22:40  loss = 1.1208e-02  error = 7.6398e-02  \n",
      "nt_epoch =    250  elapsed = 22:51  loss = 1.1139e-02  error = 7.6699e-02  \n",
      "nt_epoch =    260  elapsed = 23:02  loss = 1.1060e-02  error = 7.6513e-02  \n",
      "nt_epoch =    270  elapsed = 23:12  loss = 1.0995e-02  error = 7.6321e-02  \n",
      "nt_epoch =    280  elapsed = 23:24  loss = 1.0914e-02  error = 7.5091e-02  \n",
      "nt_epoch =    290  elapsed = 23:35  loss = 1.0842e-02  error = 7.3293e-02  \n",
      "nt_epoch =    300  elapsed = 23:46  loss = 1.0731e-02  error = 7.1661e-02  \n",
      "nt_epoch =    310  elapsed = 23:58  loss = 1.0583e-02  error = 7.2675e-02  \n",
      "nt_epoch =    320  elapsed = 24:09  loss = 1.0455e-02  error = 7.0767e-02  \n",
      "nt_epoch =    330  elapsed = 24:21  loss = 1.0314e-02  error = 7.0739e-02  \n",
      "nt_epoch =    340  elapsed = 24:32  loss = 1.0131e-02  error = 6.9886e-02  \n",
      "nt_epoch =    350  elapsed = 24:45  loss = 9.9502e-03  error = 7.3806e-02  \n",
      "nt_epoch =    360  elapsed = 24:56  loss = 9.8030e-03  error = 7.4532e-02  \n",
      "nt_epoch =    370  elapsed = 25:08  loss = 9.6750e-03  error = 7.2761e-02  \n",
      "nt_epoch =    380  elapsed = 25:20  loss = 9.5522e-03  error = 7.2664e-02  \n",
      "nt_epoch =    390  elapsed = 25:32  loss = 9.4284e-03  error = 7.2654e-02  \n",
      "nt_epoch =    400  elapsed = 25:43  loss = 9.2952e-03  error = 7.1524e-02  \n",
      "nt_epoch =    410  elapsed = 25:55  loss = 9.1494e-03  error = 7.2311e-02  \n",
      "nt_epoch =    420  elapsed = 26:06  loss = 9.0032e-03  error = 7.4651e-02  \n",
      "nt_epoch =    430  elapsed = 26:18  loss = 8.8809e-03  error = 7.8207e-02  \n",
      "nt_epoch =    440  elapsed = 26:30  loss = 8.6802e-03  error = 7.9015e-02  \n",
      "nt_epoch =    450  elapsed = 26:42  loss = 8.4900e-03  error = 7.7072e-02  \n",
      "nt_epoch =    460  elapsed = 26:53  loss = 8.3658e-03  error = 7.6136e-02  \n",
      "nt_epoch =    470  elapsed = 27:04  loss = 8.2313e-03  error = 7.9498e-02  \n",
      "nt_epoch =    480  elapsed = 27:15  loss = 8.0827e-03  error = 7.9084e-02  \n",
      "nt_epoch =    490  elapsed = 27:26  loss = 7.8387e-03  error = 8.4073e-02  \n",
      "nt_epoch =    500  elapsed = 27:38  loss = 7.6921e-03  error = 8.5096e-02  \n",
      "nt_epoch =    510  elapsed = 27:49  loss = 7.5306e-03  error = 8.7038e-02  \n",
      "nt_epoch =    520  elapsed = 28:01  loss = 7.4236e-03  error = 8.9260e-02  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt_epoch =    530  elapsed = 28:12  loss = 7.3280e-03  error = 9.1639e-02  \n",
      "nt_epoch =    540  elapsed = 28:25  loss = 7.1811e-03  error = 9.4063e-02  \n",
      "nt_epoch =    550  elapsed = 28:37  loss = 7.0860e-03  error = 9.1112e-02  \n",
      "nt_epoch =    560  elapsed = 28:50  loss = 7.0086e-03  error = 8.9338e-02  \n",
      "nt_epoch =    570  elapsed = 29:01  loss = 6.9018e-03  error = 9.0255e-02  \n",
      "nt_epoch =    580  elapsed = 29:12  loss = 6.8289e-03  error = 9.0042e-02  \n",
      "nt_epoch =    590  elapsed = 29:23  loss = 6.7557e-03  error = 9.0152e-02  \n",
      "nt_epoch =    600  elapsed = 29:35  loss = 6.6791e-03  error = 8.9018e-02  \n",
      "nt_epoch =    610  elapsed = 29:47  loss = 6.6237e-03  error = 8.9785e-02  \n",
      "nt_epoch =    620  elapsed = 29:59  loss = 6.5601e-03  error = 9.2378e-02  \n",
      "nt_epoch =    630  elapsed = 30:11  loss = 6.4826e-03  error = 9.1748e-02  \n",
      "nt_epoch =    640  elapsed = 30:22  loss = 6.3998e-03  error = 9.0980e-02  \n",
      "nt_epoch =    650  elapsed = 30:34  loss = 6.3069e-03  error = 9.2996e-02  \n",
      "nt_epoch =    660  elapsed = 30:45  loss = 6.2258e-03  error = 9.3363e-02  \n",
      "nt_epoch =    670  elapsed = 30:57  loss = 6.1424e-03  error = 9.4190e-02  \n",
      "nt_epoch =    680  elapsed = 31:08  loss = 6.0780e-03  error = 9.2588e-02  \n",
      "nt_epoch =    690  elapsed = 31:20  loss = 6.0159e-03  error = 9.0880e-02  \n",
      "nt_epoch =    700  elapsed = 31:31  loss = 5.9648e-03  error = 9.0696e-02  \n",
      "nt_epoch =    710  elapsed = 31:43  loss = 5.9326e-03  error = 9.1274e-02  \n",
      "nt_epoch =    720  elapsed = 31:55  loss = 5.8987e-03  error = 9.1087e-02  \n",
      "nt_epoch =    730  elapsed = 32:06  loss = 5.8722e-03  error = 9.0522e-02  \n",
      "nt_epoch =    740  elapsed = 32:18  loss = 5.8368e-03  error = 9.1272e-02  \n",
      "nt_epoch =    750  elapsed = 32:29  loss = 5.8116e-03  error = 9.1038e-02  \n",
      "nt_epoch =    760  elapsed = 32:41  loss = 5.7839e-03  error = 9.0983e-02  \n",
      "nt_epoch =    770  elapsed = 32:52  loss = 5.7634e-03  error = 9.0082e-02  \n",
      "nt_epoch =    780  elapsed = 33:03  loss = 5.7408e-03  error = 8.9882e-02  \n",
      "nt_epoch =    790  elapsed = 33:14  loss = 5.7177e-03  error = 8.9411e-02  \n",
      "nt_epoch =    800  elapsed = 33:25  loss = 5.6914e-03  error = 8.9564e-02  \n",
      "nt_epoch =    810  elapsed = 33:36  loss = 5.6574e-03  error = 8.9585e-02  \n",
      "nt_epoch =    820  elapsed = 33:48  loss = 5.6231e-03  error = 8.7978e-02  \n",
      "nt_epoch =    830  elapsed = 33:59  loss = 5.5980e-03  error = 8.8371e-02  \n",
      "nt_epoch =    840  elapsed = 34:10  loss = 5.5691e-03  error = 8.8706e-02  \n",
      "nt_epoch =    850  elapsed = 34:21  loss = 5.5424e-03  error = 8.9514e-02  \n",
      "nt_epoch =    860  elapsed = 34:31  loss = 5.5231e-03  error = 8.8364e-02  \n",
      "nt_epoch =    870  elapsed = 34:42  loss = 5.4934e-03  error = 8.7168e-02  \n",
      "nt_epoch =    880  elapsed = 34:53  loss = 5.4652e-03  error = 8.6740e-02  \n",
      "nt_epoch =    890  elapsed = 35:04  loss = 5.4225e-03  error = 9.3854e-02  \n",
      "nt_epoch =    900  elapsed = 35:14  loss = 5.3792e-03  error = 9.4804e-02  \n",
      "nt_epoch =    910  elapsed = 35:25  loss = 5.3129e-03  error = 9.2130e-02  \n",
      "nt_epoch =    920  elapsed = 35:36  loss = 5.2667e-03  error = 9.3279e-02  \n",
      "nt_epoch =    930  elapsed = 35:47  loss = 5.2322e-03  error = 9.1004e-02  \n",
      "nt_epoch =    940  elapsed = 35:58  loss = 5.1875e-03  error = 8.7738e-02  \n",
      "nt_epoch =    950  elapsed = 36:08  loss = 5.1420e-03  error = 8.7510e-02  \n",
      "nt_epoch =    960  elapsed = 36:19  loss = 5.0916e-03  error = 8.8853e-02  \n",
      "nt_epoch =    970  elapsed = 36:30  loss = 5.0305e-03  error = 9.2441e-02  \n",
      "nt_epoch =    980  elapsed = 36:41  loss = 4.9727e-03  error = 9.4619e-02  \n",
      "nt_epoch =    990  elapsed = 36:51  loss = 4.9162e-03  error = 9.8812e-02  \n",
      "nt_epoch =   1000  elapsed = 37:02  loss = 4.8629e-03  error = 1.0554e-01  \n",
      "nt_epoch =   1010  elapsed = 37:13  loss = 4.7590e-03  error = 1.1007e-01  \n",
      "nt_epoch =   1020  elapsed = 37:24  loss = 4.6759e-03  error = 1.1577e-01  \n",
      "nt_epoch =   1030  elapsed = 37:35  loss = 4.5692e-03  error = 1.2255e-01  \n",
      "nt_epoch =   1040  elapsed = 37:45  loss = 4.4896e-03  error = 1.2942e-01  \n",
      "nt_epoch =   1050  elapsed = 37:56  loss = 4.4308e-03  error = 1.3010e-01  \n",
      "nt_epoch =   1060  elapsed = 38:07  loss = 4.3675e-03  error = 1.2685e-01  \n",
      "nt_epoch =   1070  elapsed = 38:18  loss = 4.3197e-03  error = 1.2392e-01  \n",
      "nt_epoch =   1080  elapsed = 38:29  loss = 4.2799e-03  error = 1.2639e-01  \n",
      "nt_epoch =   1090  elapsed = 38:40  loss = 4.2407e-03  error = 1.2972e-01  \n",
      "nt_epoch =   1100  elapsed = 38:50  loss = 4.2063e-03  error = 1.3071e-01  \n",
      "nt_epoch =   1110  elapsed = 39:01  loss = 4.1789e-03  error = 1.3553e-01  \n",
      "nt_epoch =   1120  elapsed = 39:12  loss = 4.1495e-03  error = 1.3805e-01  \n",
      "nt_epoch =   1130  elapsed = 39:22  loss = 4.1182e-03  error = 1.4084e-01  \n",
      "nt_epoch =   1140  elapsed = 39:33  loss = 4.0736e-03  error = 1.4048e-01  \n",
      "nt_epoch =   1150  elapsed = 39:44  loss = 4.0110e-03  error = 1.3937e-01  \n",
      "nt_epoch =   1160  elapsed = 39:55  loss = 3.9673e-03  error = 1.3939e-01  \n",
      "nt_epoch =   1170  elapsed = 40:07  loss = 3.9226e-03  error = 1.4510e-01  \n",
      "nt_epoch =   1180  elapsed = 40:19  loss = 3.8933e-03  error = 1.4532e-01  \n",
      "nt_epoch =   1190  elapsed = 40:30  loss = 3.8579e-03  error = 1.4411e-01  \n",
      "nt_epoch =   1200  elapsed = 40:41  loss = 3.8215e-03  error = 1.4377e-01  \n",
      "nt_epoch =   1210  elapsed = 40:52  loss = 3.7926e-03  error = 1.4439e-01  \n",
      "nt_epoch =   1220  elapsed = 41:02  loss = 3.7542e-03  error = 1.3909e-01  \n",
      "nt_epoch =   1230  elapsed = 41:13  loss = 3.7204e-03  error = 1.3809e-01  \n",
      "nt_epoch =   1240  elapsed = 41:24  loss = 3.6923e-03  error = 1.3488e-01  \n",
      "nt_epoch =   1250  elapsed = 41:35  loss = 3.6487e-03  error = 1.3248e-01  \n",
      "nt_epoch =   1260  elapsed = 41:46  loss = 3.6201e-03  error = 1.3191e-01  \n",
      "nt_epoch =   1270  elapsed = 41:57  loss = 3.5922e-03  error = 1.3250e-01  \n",
      "nt_epoch =   1280  elapsed = 42:08  loss = 3.5683e-03  error = 1.3288e-01  \n",
      "nt_epoch =   1290  elapsed = 42:19  loss = 3.5429e-03  error = 1.3322e-01  \n",
      "nt_epoch =   1300  elapsed = 42:30  loss = 3.5130e-03  error = 1.3170e-01  \n",
      "nt_epoch =   1310  elapsed = 42:41  loss = 3.4890e-03  error = 1.3029e-01  \n",
      "nt_epoch =   1320  elapsed = 42:52  loss = 3.4648e-03  error = 1.3330e-01  \n",
      "nt_epoch =   1330  elapsed = 43:03  loss = 3.4443e-03  error = 1.3232e-01  \n",
      "nt_epoch =   1340  elapsed = 43:15  loss = 3.4251e-03  error = 1.3053e-01  \n",
      "nt_epoch =   1350  elapsed = 43:25  loss = 3.4082e-03  error = 1.3049e-01  \n",
      "nt_epoch =   1360  elapsed = 43:36  loss = 3.3901e-03  error = 1.2982e-01  \n",
      "nt_epoch =   1370  elapsed = 43:47  loss = 3.3716e-03  error = 1.2732e-01  \n",
      "nt_epoch =   1380  elapsed = 43:59  loss = 3.3545e-03  error = 1.2693e-01  \n",
      "nt_epoch =   1390  elapsed = 44:10  loss = 3.3300e-03  error = 1.2835e-01  \n",
      "nt_epoch =   1400  elapsed = 44:22  loss = 3.3096e-03  error = 1.2904e-01  \n",
      "nt_epoch =   1410  elapsed = 44:33  loss = 3.2913e-03  error = 1.2787e-01  \n",
      "nt_epoch =   1420  elapsed = 44:46  loss = 3.2808e-03  error = 1.2753e-01  \n",
      "nt_epoch =   1430  elapsed = 44:57  loss = 3.2684e-03  error = 1.2756e-01  \n",
      "nt_epoch =   1440  elapsed = 45:08  loss = 3.2519e-03  error = 1.2516e-01  \n",
      "nt_epoch =   1450  elapsed = 45:19  loss = 3.2428e-03  error = 1.2447e-01  \n",
      "nt_epoch =   1460  elapsed = 45:30  loss = 3.2252e-03  error = 1.2453e-01  \n",
      "nt_epoch =   1470  elapsed = 45:41  loss = 3.2084e-03  error = 1.2579e-01  \n",
      "nt_epoch =   1480  elapsed = 45:52  loss = 3.1942e-03  error = 1.2435e-01  \n",
      "nt_epoch =   1490  elapsed = 46:04  loss = 3.1779e-03  error = 1.2261e-01  \n",
      "nt_epoch =   1500  elapsed = 46:15  loss = 3.1799e-03  error = 1.1882e-01  \n",
      "nt_epoch =   1510  elapsed = 46:26  loss = 3.1482e-03  error = 1.1890e-01  \n",
      "nt_epoch =   1520  elapsed = 46:38  loss = 3.1307e-03  error = 1.1665e-01  \n",
      "nt_epoch =   1530  elapsed = 46:49  loss = 3.1164e-03  error = 1.1623e-01  \n",
      "nt_epoch =   1540  elapsed = 47:00  loss = 3.0991e-03  error = 1.1557e-01  \n",
      "nt_epoch =   1550  elapsed = 47:11  loss = 3.0811e-03  error = 1.1420e-01  \n",
      "nt_epoch =   1560  elapsed = 47:23  loss = 3.0624e-03  error = 1.1316e-01  \n",
      "nt_epoch =   1570  elapsed = 47:34  loss = 3.0401e-03  error = 1.1170e-01  \n",
      "nt_epoch =   1580  elapsed = 47:45  loss = 3.0191e-03  error = 1.0804e-01  \n",
      "nt_epoch =   1590  elapsed = 47:57  loss = 3.0047e-03  error = 1.0630e-01  \n",
      "nt_epoch =   1600  elapsed = 48:08  loss = 2.9842e-03  error = 1.0477e-01  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt_epoch =   1610  elapsed = 48:19  loss = 2.9667e-03  error = 1.0453e-01  \n",
      "nt_epoch =   1620  elapsed = 48:30  loss = 2.9490e-03  error = 1.0256e-01  \n",
      "nt_epoch =   1630  elapsed = 48:41  loss = 2.9294e-03  error = 1.0217e-01  \n",
      "nt_epoch =   1640  elapsed = 48:52  loss = 2.9089e-03  error = 1.0120e-01  \n",
      "nt_epoch =   1650  elapsed = 49:04  loss = 2.8846e-03  error = 1.0089e-01  \n",
      "nt_epoch =   1660  elapsed = 49:16  loss = 2.8690e-03  error = 1.0014e-01  \n",
      "nt_epoch =   1670  elapsed = 49:28  loss = 2.8560e-03  error = 9.9121e-02  \n",
      "nt_epoch =   1680  elapsed = 49:39  loss = 2.8406e-03  error = 9.6660e-02  \n",
      "nt_epoch =   1690  elapsed = 49:51  loss = 2.8269e-03  error = 9.4850e-02  \n",
      "nt_epoch =   1700  elapsed = 50:02  loss = 2.8119e-03  error = 9.3626e-02  \n",
      "nt_epoch =   1710  elapsed = 50:13  loss = 2.7977e-03  error = 9.2826e-02  \n",
      "nt_epoch =   1720  elapsed = 50:24  loss = 2.7861e-03  error = 9.2934e-02  \n",
      "nt_epoch =   1730  elapsed = 50:35  loss = 2.7740e-03  error = 9.2989e-02  \n",
      "nt_epoch =   1740  elapsed = 50:46  loss = 2.7569e-03  error = 9.2878e-02  \n",
      "nt_epoch =   1750  elapsed = 50:57  loss = 2.7440e-03  error = 9.2693e-02  \n",
      "nt_epoch =   1760  elapsed = 51:08  loss = 2.7316e-03  error = 9.3266e-02  \n",
      "nt_epoch =   1770  elapsed = 51:19  loss = 2.7201e-03  error = 9.3316e-02  \n",
      "nt_epoch =   1780  elapsed = 51:30  loss = 2.7080e-03  error = 9.2247e-02  \n",
      "nt_epoch =   1790  elapsed = 51:41  loss = 2.6937e-03  error = 9.0972e-02  \n",
      "nt_epoch =   1800  elapsed = 51:53  loss = 2.6789e-03  error = 9.1097e-02  \n",
      "nt_epoch =   1810  elapsed = 52:04  loss = 2.6598e-03  error = 9.2086e-02  \n",
      "nt_epoch =   1820  elapsed = 52:15  loss = 2.6421e-03  error = 9.2591e-02  \n",
      "nt_epoch =   1830  elapsed = 52:26  loss = 2.6282e-03  error = 9.3763e-02  \n",
      "nt_epoch =   1840  elapsed = 52:37  loss = 2.6165e-03  error = 9.3942e-02  \n",
      "nt_epoch =   1850  elapsed = 52:48  loss = 2.6022e-03  error = 9.2235e-02  \n",
      "nt_epoch =   1860  elapsed = 53:00  loss = 2.5862e-03  error = 9.2299e-02  \n",
      "nt_epoch =   1870  elapsed = 53:11  loss = 2.5724e-03  error = 9.2255e-02  \n",
      "nt_epoch =   1880  elapsed = 53:22  loss = 2.5624e-03  error = 9.1973e-02  \n",
      "nt_epoch =   1890  elapsed = 53:33  loss = 2.5511e-03  error = 9.0533e-02  \n",
      "nt_epoch =   1900  elapsed = 53:43  loss = 2.5384e-03  error = 8.9330e-02  \n",
      "nt_epoch =   1910  elapsed = 53:54  loss = 2.5205e-03  error = 8.9683e-02  \n",
      "nt_epoch =   1920  elapsed = 54:05  loss = 2.5042e-03  error = 9.0499e-02  \n",
      "nt_epoch =   1930  elapsed = 54:16  loss = 2.4917e-03  error = 9.0401e-02  \n",
      "nt_epoch =   1940  elapsed = 54:27  loss = 2.4728e-03  error = 9.1341e-02  \n",
      "nt_epoch =   1950  elapsed = 54:38  loss = 2.4529e-03  error = 9.2147e-02  \n",
      "nt_epoch =   1960  elapsed = 54:49  loss = 2.4367e-03  error = 9.1940e-02  \n",
      "nt_epoch =   1970  elapsed = 55:00  loss = 2.4224e-03  error = 9.2331e-02  \n",
      "nt_epoch =   1980  elapsed = 55:11  loss = 2.4128e-03  error = 9.2447e-02  \n",
      "nt_epoch =   1990  elapsed = 55:22  loss = 2.3954e-03  error = 9.3935e-02  \n",
      "==================\n",
      "Training finished (epoch 3000): duration = 55:32  error = 9.3551e-02  \n"
     ]
    }
   ],
   "source": [
    "# Creating the model and training\n",
    "logger = Logger(frequency=10)\n",
    "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, inp_f_train, ub, lb,  R, temp, L_a, a_c, D_a, D_b)\n",
    "def error():\n",
    "    u_pred, _ = pinn.predict(inp_star[0:100000,:])\n",
    "    return np.linalg.norm(conc_star[0:100000,:] - u_pred, 2) / np.linalg.norm(conc_star[0:100000,:], 2)\n",
    "logger.set_error_fn(error)\n",
    "pinn.fit(inp_train, conc_train, tf_epochs, nt_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred, f_pred = pinn.predict(inp_star[32768000:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32784384, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_star.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32784384-(128*128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred, f_pred = pinn.predict(inp_star[0:128*128,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = np.asarray(u_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = u_pred.reshape((128,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57350284, 0.5685475 , 0.56245184, ..., 0.62742037, 0.7732414 ,\n",
       "        0.8024825 ],\n",
       "       [0.5471482 , 0.54328114, 0.53828645, ..., 0.7334618 , 0.8011007 ,\n",
       "        0.7270435 ],\n",
       "       [0.51698947, 0.51417625, 0.5103012 , ..., 0.78835833, 0.75742024,\n",
       "        0.5624922 ],\n",
       "       ...,\n",
       "       [0.60846424, 0.7206157 , 0.79283804, ..., 0.5352092 , 0.5336466 ,\n",
       "        0.5321508 ],\n",
       "       [0.7444018 , 0.79958695, 0.78786135, ..., 0.5380051 , 0.5363639 ,\n",
       "        0.534769  ],\n",
       "       [0.8009851 , 0.7652694 , 0.593369  , ..., 0.5408544 , 0.53918284,\n",
       "        0.53753453]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(u_pred,cmap='bwr');\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
